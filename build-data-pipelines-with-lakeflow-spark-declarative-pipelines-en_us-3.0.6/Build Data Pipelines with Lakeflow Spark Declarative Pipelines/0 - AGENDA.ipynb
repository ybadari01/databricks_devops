{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07657f70-07e0-49c6-b7a3-c7effa0bb3f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3afbe61a-dcae-4a6b-bf5f-64d8b77e34a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Build Data Pipelines with Lakeflow Spark Declarative Pipelines (SDP)\n",
    "\n",
    "This course introduces users to the essential concepts and skills needed to build data pipelines using Lakeflow Spark Declarative Pipelines (SDP) in Databricks for incremental batch or streaming ingestion and processing through multiple streaming tables and materialized views. Designed for data engineers new to Spark Declarative Pipelines, the course provides a comprehensive overview of core components such as incremental data processing, streaming tables, materialized views, and temporary views, highlighting their specific purposes and differences.\n",
    "\n",
    "Topics covered include:\n",
    "\n",
    "- Developing and debugging ETL pipelines with the multi-file editor in Spark Declarative Pipelines using SQL (with Python code examples provided)\n",
    "\n",
    "- How Spark Declarative Pipelines track data dependencies in a pipeline through the pipeline graph\n",
    "\n",
    "- Configuring pipeline compute resources, data assets, trigger modes, and other advanced options\n",
    "\n",
    "Next, the course introduces data quality expectations in Spark Declarative Pipelines, guiding users through the process of integrating expectations into pipelines to validate and enforce data integrity. Learners will then explore how to put a pipeline into production, including scheduling options, and enabling pipeline event logging to monitor pipeline performance and health.\n",
    "\n",
    "Finally, the course covers how to implement Change Data Capture (CDC) using the AUTO CDC INTO syntax within Spark Declarative Pipelines to manage slowly changing dimensions (SCD Type 1 and Type 2), preparing users to integrate CDC into their own pipelines.\n",
    "\n",
    "---\n",
    "### Course Agenda\n",
    "The following modules are part of the **Data Engineer Learning** Path by Databricks Academy.\n",
    "| # | Notebook Name |\n",
    "| --- | --- |\n",
    "| 1 | [REQUIRED - Course Setup and Creating a Pipeline]($./1 - REQUIRED - Course Setup and Creating a Pipeline) |\n",
    "| 2 | [Developing a Simple Pipeline]($./2 - Developing a Simple Pipeline) |\n",
    "| 3 | [Adding Data Quality Expectations]($./3 - Adding Data Quality Expectations) |\n",
    "| 4L | [Create a Pipeline]($./4 Lab - Create a Pipeline) |\n",
    "| 5 | [Deploying a Pipeline to Production]($./5 - Deploying a Pipeline to Production) |\n",
    "| 6 | [Change Data Capture with AUTO CDC with SCD TYPE 1]($./6 - Change Data Capture with AUTO CDC with SCD TYPE 1) |\n",
    "| 7L | [AUTO CDC INTO with SCD Type 1]($./7 BONUS Lab - AUTO CDC INTO with SCD Type 1) |\n",
    "\n",
    "--- \n",
    "\n",
    "### Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run demo and lab notebooks, you need to use the following Databricks runtime: **`17.3.x-scala2.13`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "699146ef-8e1e-4ec0-a68a-39ac3269338c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "0 - AGENDA",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
