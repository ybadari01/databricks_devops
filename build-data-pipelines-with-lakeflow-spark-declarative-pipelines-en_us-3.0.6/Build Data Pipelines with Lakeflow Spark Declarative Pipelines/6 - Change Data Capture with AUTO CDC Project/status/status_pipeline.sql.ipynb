{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "236647b1-b846-4998-9f00-bc413ca7e39b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca5222c7-aecd-4fa7-ace8-6b5b3e308924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Status Pipeline\n",
    "\n",
    "In this DLT pipeline, the **status** pipeline is implemented within a notebook. DLT supports using `.py`, `.sql`, or notebook files as pipeline sources.\n",
    "\n",
    "**NOTE:** In an notebook you must use either Python or SQL. \n",
    "\n",
    "The final pipeline performs the following tasks:\n",
    "\n",
    "1. Creates the **status_bronze_demo5** streaming table by ingesting raw JSON files from `/Volumes/dbacademy/ops/your-lab-user/status/`.\n",
    "\n",
    "2. Creates the **status_silver_demo5** streaming table from the **status_bronze_demo5** table.\n",
    "\n",
    "3. Creates the materialized view **full_order_status_gold_demo5** to capture each order's status by joining the following tables:\n",
    "   - **status_silver_demo5**\n",
    "   - **orders_silver_demo5**\n",
    "\n",
    "4. Creates the following materialized views:\n",
    "   - **cancelled_orders_gold_demo5** – Displays all cancelled orders and how many days passed before cancellation.\n",
    "   - **delivered_orders_gold_demo5** – Displays all delivered orders and how many days it took to deliver each order.\n",
    "![Pipeline](../../Includes/images/demo6_pipeline_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "672527af-9679-4c5a-83d5-9c537288e3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. JSON -> Bronze\n",
    "The code below ingests JSON files located in your `/Volumes/dbacademy/ops/your-lab-user/status/` volume, using the `source` DLT configuration parameter to point to the base path `/Volumes/dbacademy/ops/your-lab-user/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ae78ae3-f41f-47cf-a58a-3a8351d4ddc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE 1_bronze_db.status_bronze_demo6\n",
    "  COMMENT \"Ingest raw JSON order status files from cloud storage\"\n",
    "  TBLPROPERTIES (\n",
    "    \"quality\" = \"bronze\",\n",
    "    \"pipelines.reset.allowed\" = false     -- prevent full table refreshes on the bronze table\n",
    "  )\n",
    "AS \n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() processing_time, \n",
    "  _metadata.file_name AS source_file\n",
    "FROM STREAM read_files(\n",
    "  \"${source}/status\", \n",
    "  format => \"json\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a57c960-04d1-410e-a6b4-366d934e2f95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Bronze -> Silver\n",
    "The code below performs a simple transformation on the date field and selects only the necessary columns for the silver streaming table **status_silver_demo5**.  \n",
    "\n",
    "We're also adding a comment and table properties to document the table for production use, along with DLT expectations to enforce data quality on the streaming table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28584da0-6e0c-405f-b806-62bec53cd4a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH STREAMING TABLE 2_silver_db.status_silver_demo6\n",
    "  (\n",
    "    -- Drop rows if order_status_timestamp is not valid\n",
    "    CONSTRAINT valid_timestamp EXPECT (order_status_timestamp > \"2021-12-25\") ON VIOLATION DROP ROW,\n",
    "    -- Warn if order_status is not in the following\n",
    "    CONSTRAINT valid_order_status EXPECT (order_status IN ('on the way','canceled','return canceled','delivered','return processed','placed','preparing'))\n",
    "  )\n",
    "  COMMENT \"Order with each status and timestamp\"\n",
    "  TBLPROPERTIES (\"quality\" = \"silver\")\n",
    "AS \n",
    "SELECT\n",
    "  order_id,\n",
    "  order_status,\n",
    "  timestamp(status_timestamp) AS order_status_timestamp\n",
    "FROM STREAM 1_bronze_db.status_bronze_demo6;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb2d21bc-8b20-4a4f-a672-020896a4f477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Use a Materialized View to Join Two Tables\n",
    "One way to join two streaming tables in DLT is by creating a materialized view that performs the join.  This approach takes all rows from each streaming table and executes a full inner join operation.\n",
    "\n",
    "**NOTES:**\n",
    "\n",
    "- **Materialized views include built-in optimizations where applicable:**\n",
    "  - [Incremental refresh for materialized views](https://docs.databricks.com/aws/en/optimizations/incremental-refresh)\n",
    "  - [Delta Live Tables Announces New Capabilities and Performance Optimizations](https://www.databricks.com/blog/2022/06/29/delta-live-tables-announces-new-capabilities-and-performance-optimizations.html)\n",
    "  - [Cost-effective, incremental ETL with serverless compute for Delta Live Tables pipelines](https://www.databricks.com/blog/cost-effective-incremental-etl-serverless-compute-delta-live-tables-pipelines)\n",
    "\n",
    "- **Stateful joins (Stream to Stream):** For stateful joins in DLT (i.e., joining incrementally as data is ingested), refer to the [Optimize stateful processing in DLT with watermarks](https://docs.databricks.com/aws/en/dlt/stateful-processing) documentation. **Stateful joins are an advanced topic and outside the scope of this course.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51499097-5174-45ca-9e6e-004dabef744c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REFRESH MATERIALIZED VIEW 3_gold_db.full_order_info_gold_demo6\n",
    "  COMMENT \"Joining the orders and order status silver tables to view all orders with each individual status per order\"\n",
    "  TBLPROPERTIES (\"quality\" = \"gold\")\n",
    "AS \n",
    "SELECT\n",
    "  orders.order_id,\n",
    "  orders.order_timestamp,\n",
    "  status.order_status,\n",
    "  status.order_status_timestamp\n",
    "-- Notice that the STREAM keyword was not used when referencing the streaming tables\n",
    "FROM 2_silver_db.status_silver_demo6 status    \n",
    "  INNER JOIN 2_silver_db.orders_silver_demo6 orders \n",
    "  ON orders.order_id = status.order_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81f53ee6-9649-4010-93ec-6611b8e0f6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Create Materialized Views for Cancelled and Delivered Orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c3a053e-a670-46d7-8246-1c3c87fd72a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The code below will create two tables using the joined data from above:\n",
    "\n",
    "- **3_gold_db.cancelled_orders_gold_demo5**\n",
    "    - A materialized view containing all **cancelled** orders\n",
    "    - number of days it took to cancel each order.\n",
    "\n",
    "- **3_gold_db.delivered_orders_gold_demo5**\n",
    "    - A materialized view containing all **delivered** orders\n",
    "    - number of days it took to deliver each order.\n",
    "\n",
    "    [datediff function](https://docs.databricks.com/aws/en/sql/language-manual/functions/datediff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c045260-e0e6-4c6e-b5d8-02b32cf37f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- CANCELLED ORDERS MV\n",
    "CREATE OR REFRESH MATERIALIZED VIEW 3_gold_db.cancelled_orders_gold_demo6\n",
    "  COMMENT \"All cancelled orders\"\n",
    "  TBLPROPERTIES (\"quality\" = \"gold\")\n",
    "AS \n",
    "SELECT\n",
    "  order_id,\n",
    "  order_timestamp,\n",
    "  order_status,\n",
    "  order_status_timestamp,\n",
    "  datediff(DAY,order_timestamp, order_status_timestamp) AS days_to_cancel -- calculate days to cancel\n",
    "FROM 3_gold_db.full_order_info_gold_demo6\n",
    "WHERE order_status = 'canceled';\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-- DELIVERED ORDERS MV\n",
    "CREATE OR REFRESH MATERIALIZED VIEW 3_gold_db.delivered_orders_gold_demo6\n",
    "  COMMENT \"All delivered orders\"\n",
    "  TBLPROPERTIES (\"quality\" = \"gold\")\n",
    "AS \n",
    "SELECT\n",
    "  order_id,\n",
    "  order_timestamp,\n",
    "  order_status,\n",
    "  order_status_timestamp,\n",
    "  datediff(DAY,order_timestamp, order_status_timestamp) AS days_to_delivery -- calculate days to deliver\n",
    "FROM 3_gold_db.full_order_info_gold_demo6\n",
    "WHERE order_status = 'delivered';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0029e768-518e-4e65-83ab-a90b9c3b9211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Create the Production Pipeline\n",
    "Follow the steps below to modify the pipeline settings and run the production pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ab0fcf2-31fb-4982-8a23-168ee7804486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Complete the following steps to modify your pipeline configuration for production:\n",
    "\n",
    "   a. Select the **Settings** icon ![Pipeline Settings](../../Includes/images/pipeline_settings_icon.png) in the left navigation pane.\n",
    "\n",
    "   b. In the **Pipeline settings** section, you can modify the **Pipeline name** and **Run as** settings.\n",
    "\n",
    "      - Click the pencil icon ![pencil_settings_icon.png](../../Includes/images/pencil_settings_icon.png) next to **Run as**.\n",
    "\n",
    "      - You can optionally change the executor of the pipeline to a service principal.  \n",
    "        A service principal is an identity you create in Databricks for use with automated tools, jobs, and applications.  \n",
    "\n",
    "        - For more information, see the [What is a service principal?](https://docs.databricks.com/aws/en/admin/users-groups/service-principals#what-is-a-service-principal) documentation.\n",
    "\n",
    "      - For this course select **Cancel** and leave **Run as** set to your username.\n",
    "\n",
    "   c. In the **Code assets** section, confirm that:\n",
    "\n",
    "      - **Root folder** points to this DLT project (**5 - Deploying a DLT Pipeline to Production**).\n",
    "\n",
    "      - **Source code** references the **orders** and **status** folders within this project.\n",
    "\n",
    "   d. In the **Default location for data assets** section, confirm the following:\n",
    "\n",
    "      - **Default catalog** is your **labuser** catalog.\n",
    "\n",
    "      - **Default schema** is the **default** schema.\n",
    "\n",
    "   e. In the **Compute** section, confirm that **Serverless** compute is selected.\n",
    "\n",
    "   f. In the **Configuration** section, ensure that the `source` key is set to your data source volume path:  \n",
    "      `/Volumes/dbacademy/ops/your-labuser-name`\n",
    "\n",
    "   g. In the **Tags** section you can add takes to help determine usage per department/chargeback.. We will leave them as is. \n",
    "\n",
    "   h. In the **Advanced settings** section:\n",
    "\n",
    "      - Expand **Advanced settings**.\n",
    "\n",
    "      - Click **Edit advanced settings**.\n",
    "\n",
    "      - In **Pipeline mode**, ensure **Triggered** is selected so the pipeline runs on a schedule.  \n",
    "        - Alternatively, you can choose **Continuous** mode to keep the pipeline running at all times.  \n",
    "        - For more details, see [Triggered vs. continuous pipeline mode](https://docs.databricks.com/aws/en/dlt/pipeline-mode).\n",
    "\n",
    "      - In **PIpeline user mode** select **Production**.\n",
    "\n",
    "      - For **Channel**, you can leave it as **Preview** for training purposes:\n",
    "        - **Current** – Uses the latest stable Databricks Runtime version, recommended for production.\n",
    "        - **Preview** – Uses a more recent, potentially less stable Runtime version, ideal for testing upcoming features.\n",
    "        - View the [DLT release notes and the release upgrade process](https://docs.databricks.com/aws/en/release-notes/dlt/) documentation for more information.\n",
    "\n",
    "      - In the **Event logs** section:\n",
    "        - Select **Publish event log to metastore**.\n",
    "        - Set **Event log name** to `event_log_demo_5`.\n",
    "        - Set **Event log catalog** to your **labuser** catalog.\n",
    "        - Set **Event log schema** to the **default** schema.\n",
    "\n",
    "        **NOTE:** If the event log is not saved to the correct location, the event log exploration steps will not work properly in the main notebook.\n",
    "\n",
    "   i. Click **Save** to save your DLT pipeline settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43e797ec-7a3d-42bc-ae5b-e27afc7e749f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Once your pipeline is production-ready, you'll want to schedule it to run either on a time interval or continuously.\n",
    "\n",
    "   For this demonstration, we’ll:\n",
    "   - Schedule the pipeline to run every day at 8:00 PM.\n",
    "   - Optionally configure notifications to alert you upon job **Start**, **Success**, and **Failure**.  \n",
    "     *(If you don’t want email notifications, you can skip this step.)*\n",
    "\n",
    "   Complete the following steps to schedule the pipeline:\n",
    "\n",
    "   a. Select the **Schedule** button.\n",
    "\n",
    "   b. For the job name, leave it as **5 - Deploying a DLT Pipeline to Production Project - labuser-name**.\n",
    "\n",
    "   c. Below **Job name**, select **Advanced**.\n",
    "\n",
    "   d. In the **Schedule** section, configure the following:\n",
    "   - Set the **Day**.\n",
    "   - Set the time to **20:00** (8:00 PM).\n",
    "   - Leave the **Timezone** as default.\n",
    "   - Select **More options**, and under **Notifications**, add your email to receive alerts for:\n",
    "     - **Start**\n",
    "     - **Success**\n",
    "     - **Failure**\n",
    "\n",
    "   e. Click **Create** to save and schedule the job.\n",
    "\n",
    "  **NOTE:** You could also set the pipeline to run a few minutes after your current time to see it start through the scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "203f789e-d82e-46a7-845c-ac013bf5b704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. After completing the pipeline settings and scheduling the pipeline, let's manually trigger the pipeline by selecting the **Run pipeline** button.\n",
    "\n",
    "    While the pipeline is running, you can explore what the final pipeline will look like using the image below:\n",
    "\n",
    "    ![DLT Pipeline Demo 6](../../Includes/images/demo6_pipeline_image.png) \n",
    "\n",
    "**NOTE:** Currently we have one JSON file in both **status** and **orders**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db55de8c-a2f6-46bd-a020-d37567192851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. After the pipeline has completed it's first run, complete the following:\n",
    "\n",
    "   a. Examine the **Pipeline graph** and confirm:\n",
    "      - 174 rows were read into the **orders_bronze** and **orders_silver** streaming tables\n",
    "      - 536 rows were read into the **status_bronze** and **status_silver** streaming tables\n",
    "      - 536 rows are in the **full_order_info_gold** materialized view\n",
    "      - 7 rows are in the **orders_by_date_gold** materialized view\n",
    "      - 8 rows are in the **cancelled_orders_gold** materialized view\n",
    "      - 94 rows are in the **delivered_orders_gold** materialized view\n",
    "\n",
    "   b. Go back to the main notebook **5 - Deploying a DLT Pipeline to Production**\n",
    "\n",
    "   c. Complete the steps in step **D. Land Data Data to Your Data Source Volume**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f79ad2eb-d537-4f55-9820-98691d77cb10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. After you have landed **4** new files into the data source volume, run the pipeline to process the newly landed JSON files.\n",
    "\n",
    "   Notice the following:\n",
    "\n",
    "   a. The **status** bronze to silver flow ingests 410 new rows.\n",
    "\n",
    "   b. The **orders** bronze to silver flow ingests 98 new rows.\n",
    "\n",
    "   c. The **full_order_info_gold** materialized view join contains a total of 946 rows (the previous 536 rows + the new 410 rows).\n",
    "\n",
    "   d. The **cancelled_orders_gold** materialized view contains 21 rows.\n",
    "\n",
    "   e. The **delivered_orders_gold** materialized view contains 176 rows.\n",
    "\n",
    "   f. The **orders_by_date_gold** materialized view contains 11 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afad3af6-97dd-4962-86f4-14f652e7f530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. In the window at the bottom, select the **Expectations** link for the **status_silver_demo5** table. It should contain the value **2**. Notice that in this run, 7.6% (31 rows) for the **valid_order_status** expectation returned a warning.\n",
    "\n",
    "This is something we would want to investigate and address in future stages of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e50bd3-ce67-4079-970a-aa598af1877c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Go back to the main notebook **5 - Deploying a DLT Pipeline to Production** and complete the steps in step **E. Monitor Your Pipeline with the Event Log**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f2112a-9289-4a5c-a026-6c9a3b03dd5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {},
   "notebookName": "status_pipeline.sql",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "SQL",
   "language": "sql",
   "name": "sql"
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
