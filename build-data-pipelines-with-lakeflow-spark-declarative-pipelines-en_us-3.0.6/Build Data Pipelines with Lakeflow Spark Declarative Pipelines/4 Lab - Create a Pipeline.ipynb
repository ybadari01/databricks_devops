{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff52f46-b901-4cf4-bf80-da4afa8f13e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f49b2a2a-4da9-436a-bcd3-070e00318d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4 Lab - Create a Pipeline  \n",
    "### Estimated Duration: ~15-20 minutes\n",
    "\n",
    "In this lab, you'll migrate a traditional ETL workflow to a pipeline for incremental data processing. You'll practice building streaming tables and materialized views using Lakeflow Spark Declarative Pipelines syntax.\n",
    "\n",
    "#### Your Tasks:\n",
    "- Create a new Pipeline  \n",
    "- Convert traditional SQL ETL to declarative syntax for incremental processing \n",
    "- Configure pipeline settings  \n",
    "- Define data quality expectations  \n",
    "- Validate and run the pipeline\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Create a pipeline and execute it successfully using the new Lakeflow Pipeline Editor.\n",
    "- Modify and configure pipeline settings to align with specific data processing requirements.\n",
    "- Integrate data quality expectations into a pipeline and evaluate their effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "764ca219-e820-4284-9413-e3f4f2e5c9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE (your cluster starts with **labuser**)\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd1b3cb-39f9-4618-9acd-2af3b8ccda16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically create and reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f39704d7-93ce-442f-b3b3-007fe3794e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "169a90a0-9b77-4a11-8929-b5765d79e9ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. SCENARIO\n",
    "\n",
    "Your data engineering team has identified an opportunity to modernize an existing ETL pipeline that was originally developed in a Databricks notebook. While the current pipeline gets the job done, it lacks the scalability, observability, efficiency and automated data quality features required as your data volume and complexity grow.\n",
    "\n",
    "To address this, you've been asked to migrate the existing pipeline to a Lakeflow Spark Declarative Pipeline. Spark Declarative Pipelines will enable your team to define data transformations more declaratively, apply data quality rules, and benefit from built-in optimization, lineage tracking and monitoring.\n",
    "\n",
    "Your goal is to refactor the original notebook based logic (shown in the cells below) into a Spark Declarative Pipeline.\n",
    "\n",
    "### REQUIREMENTS:\n",
    "  - Migrate the ETL code below to a Spark Declarative Pipeline.\n",
    "  - Add the required data quality expectations to the bronze table and silver table:\n",
    "  - Create materialized views for the most up to date aggregated information.\n",
    "\n",
    "Follow the steps below to complete your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eeef04d6-fcfd-4380-ac60-1f1cf97adbd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B1. Explore the Raw Data\n",
    "\n",
    "1. Complete the following steps to view where our lab's streaming raw source files are coming from:\n",
    "\n",
    "   a. Select the **Catalog** icon ![Catalog Icon](./Includes/images/catalog_icon.png) in the left navigation bar.  \n",
    "\n",
    "   b. Expand your **labuser** catalog.  \n",
    "\n",
    "   c. Expand the **default** schema.  \n",
    "\n",
    "   d. Expand **Volumes**.  \n",
    "\n",
    "   e. Expand the **lab_files** volume.  \n",
    "\n",
    "   f. You should see a single CSV file named **employees_1.csv**. If not, refresh the catalogs.  \n",
    "\n",
    "   g. The files in the **lab_files** volume will be the data source files you will be ingesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "386a5c75-dc45-49b8-8d21-897960932e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cell below to view the raw CSV file in your **lab_files** volume. Notice the following:\n",
    "\n",
    "   - It’s a simple CSV file separated by commas.  \n",
    "   - It contains headers.  \n",
    "   - It has 7 rows in total (6 data records and 1 header row).  \n",
    "   - The first record (row 2) is a test record and should not be included in the pipeline and will be dropped by a data quality expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa4c0b83-c000-45e7-95bf-13dc6b454175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "        SELECT *\n",
    "        FROM csv.`/Volumes/{DA.catalog_name}/default/lab_files/`\n",
    "        ''').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "530011cd-d3b3-480d-a874-4e568f076bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Current ETL Code\n",
    "\n",
    "Run each cell below to view the results of the current ETL pipeline. This will give you an idea of the expected output. Don’t worry too much about the data transformations within the SQL queries.\n",
    "\n",
    "The focus of this lab is on using **declarative SQL** and creating a **Spark Declarative Pipeline**. You will not need to modify the transformation logic, only the `CREATE` statements and `FROM` clauses to ensure data is read and processed incrementally in your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb4fa779-a323-4649-9149-81bcc91428a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### B2.1 - CSV to Bronze\n",
    "\n",
    "Explore the code and run the cell. Observe the results. Notice that:\n",
    "\n",
    "- The CSV file in the volume is read in as a table named **employees_bronze_lab4** in the **labuser.lab_1_bronze_db** schema.  \n",
    "- The table contains 6 rows with the correct column names.\n",
    "\n",
    "Think about what you will need to change when migrating this to a Spake Declarative Pipeline. Hints are added as comments in the code below.\n",
    "\n",
    "**NOTE:** In your Spark Declarative Pipeline we will want to add data quality expectations to document any bad data coming into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "576bbfb9-5807-43e6-8e15-aefc6258101a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Specify to use your labuser catalog from the course DA object\n",
    "USE CATALOG IDENTIFIER(DA.catalog_name);\n",
    "\n",
    "\n",
    "CREATE OR REPLACE TABLE lab_1_bronze_db.employees_bronze_lab4  -- You will have to modify this to create a streaming table in the pipeline\n",
    "AS\n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() AS ingestion_time,\n",
    "  _metadata.file_name as raw_file_name\n",
    "FROM read_files(                                           -- You will have to modify FROM clause to incrementally read in data\n",
    "  '/Volumes/' || DA.catalog_name || '/default/lab_files',  -- You will have to modify this path in the pipeline to your specific raw data source\n",
    "  format => 'CSV',\n",
    "  header => 'true'\n",
    ");\n",
    "\n",
    "\n",
    "-- Display table\n",
    "SELECT *\n",
    "FROM lab_1_bronze_db.employees_bronze_lab4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3bb479c-9a75-47c3-aca4-bd1aff0679fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### B2.2 - Bronze to Silver\n",
    "\n",
    "Run the cell below to create the table **labuser.lab_2_silver_db.employees_silver_lab4** and explore the results. Notice that a few simple data transformations were applied to the bronze table, and metadata columns were removed.\n",
    "\n",
    "Think about what you will need to change when migrating this to a Spark Declarative Pipeline. Hints are added as comments in the code below.\n",
    "\n",
    "**NOTE:** For simplicity, we are leaving the **test** row in place, and you will remove it using data quality expectations. Typically, we could have just filtered out the null value(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd63ad1e-dcfa-43cd-b3de-a9fc0f8e1c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE lab_2_silver_db.employees_silver_lab4 -- You will have to modify this to create a streaming table in the pipeline\n",
    "AS\n",
    "SELECT\n",
    "  EmployeeID,\n",
    "  FirstName,\n",
    "  upper(Country) AS Country,\n",
    "  Department,\n",
    "  Salary,\n",
    "  HireDate,\n",
    "  date_format(HireDate, 'MMMM') AS HireMonthName,\n",
    "  year(HireDate) AS HireYear, \n",
    "  Operation\n",
    "FROM lab_1_bronze_db.employees_bronze_lab4;                    -- You will have to modify FROM clause to incrementally read in data\n",
    "\n",
    "\n",
    "-- Display table\n",
    "SELECT *\n",
    "FROM lab_2_silver_db.employees_silver_lab4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4f1a706-ab2e-4b1b-8f74-bed88080f137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### B2.3 - Silver to Gold\n",
    "The code below creates two traditional views to aggregate the silver tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35d920e7-6d53-4e97-afae-dd72971a6260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell to create a view that calculates the total number of employees and total salary by country.\n",
    "\n",
    "    Think about what you will need to change when migrating this to a Spark Declarative Pipeline. A hint is added as a comment in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa439208-da04-4346-80b4-aee63760188d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW lab_3_gold_db.employees_by_country_gold_lab4 -- You will have to modify this to create a materialized view in the pipeline\n",
    "AS\n",
    "SELECT \n",
    "  Country,\n",
    "  count(*) AS TotalEmployees,\n",
    "  sum(Salary) AS TotalSalary\n",
    "FROM lab_2_silver_db.employees_silver_lab4\n",
    "GROUP BY Country;\n",
    "\n",
    "\n",
    "-- Display view\n",
    "SELECT *\n",
    "FROM lab_3_gold_db.employees_by_country_gold_lab4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1413946c-ee8b-4205-8f6f-eeb800919214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cell to create a view that calculates the salary by department.\n",
    "\n",
    "    Think about what you will need to change when migrating this to a Spark Declarative Pipeline. A hint is added as a comment in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8079cbb4-253a-4ee0-9afe-2f552430e407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW lab_3_gold_db.salary_by_department_gold_lab4  -- You will have to modify this to create a materialized view in the pipeline\n",
    "AS\n",
    "SELECT\n",
    "  Department,\n",
    "  sum(Salary) AS TotalSalary\n",
    "FROM lab_2_silver_db.employees_silver_lab4\n",
    "GROUP BY Department;\n",
    "\n",
    "\n",
    "-- Display view\n",
    "SELECT *\n",
    "FROM lab_3_gold_db.salary_by_department_gold_lab4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a23edbf1-edf1-4084-a4f0-2c2a7ea96b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### B2.4 - Delete the Tables\n",
    "\n",
    "Run the cell below to delete all the tables you created above. You will recreate them as streaming tables and materialized views in the Spark Declarative Pipeline.\n",
    "\n",
    "**NOTE:** If you have created the streaming tables and materialized views with Spark Declarative Pipelines and want to drop them to redo this lab, the following code will not work with the lab's default **No isolation shared cluster**. You will have to run the cells in this notebook using Serverless or manually delete the pipeline and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8da395a5-2c60-47cd-89cf-026ca9da361b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS lab_1_bronze_db.employees_bronze_lab4;\n",
    "DROP TABLE IF EXISTS lab_2_silver_db.employees_silver_lab4;\n",
    "DROP VIEW IF EXISTS lab_3_gold_db.employees_by_country_gold_lab4;\n",
    "DROP VIEW IF EXISTS lab_3_gold_db.salary_by_department_gold_lab4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a3fcf81-d6ac-4cb3-9e93-5f9901ed667c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to view and copy the path to your **lab_files** volume. You will need this path when building your pipeline to reference your data source files.\n",
    "\n",
    "**NOTE:** You can also navigate to the volume and copy the path using the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf727a83-5392-48e4-a256-a75522d3d4d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'/Volumes/{DA.catalog_name}/default/lab_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02dbfe3a-5382-41cc-a2ef-4d7cab91a804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. TO DO: Create the Lakeflow Spark Declarative Pipeline (Steps)\n",
    "\n",
    "After you have explored the traditional ETL code to create the tables and views, it's time to modify that syntax to declarative SQL for your new pipeline.\n",
    "\n",
    "You will have to complete the following:\n",
    "\n",
    "**NOTE:** The solution files can be found in the **4 - Lab Solution Project**. All code is in the one **ingest.sql** file:\n",
    "\n",
    "1. Create a Spark Declarative Pipeline and name it **Lab4 - firstname pipeline project**.\n",
    "\n",
    "    - Select your **labuser** catalog  \n",
    "\n",
    "    - Select the **default** schema  \n",
    "\n",
    "    - Select the **Start with sample code in SQL** language  \n",
    "\n",
    "    - **NOTE:** The Spark Declarative Pipeline will contain sample files and notebooks. You can exclude the sample files from the pipeline before you run the pipeline.\n",
    "\n",
    "\n",
    "2. Migrate the ETL code (shown below for each step as markdown) into one or more files and folders to organize your pipeline (you can also put everything in a single file if you want).\n",
    "<br></br>\n",
    "##### 2a. Modify the code (shown below) to create the **bronze** streaming table by completing the following:\n",
    "\n",
    "```SQL\n",
    "CREATE OR REPLACE TABLE lab_1_bronze_db.employees_bronze_lab4  -- You will have to modify this to create a streaming table in the pipeline\n",
    "AS\n",
    "SELECT \n",
    "    *,\n",
    "    current_timestamp() AS ingestion_time,\n",
    "    _metadata.file_name as raw_file_name\n",
    "FROM read_files(                                           -- You will have to modify FROM clause to incrementally read in data\n",
    "    '/Volumes/' || DA.catalog_name || '/default/lab_files',  -- You will have to modify this path in the pipeline to your specific raw data source\n",
    "    format => 'CSV',\n",
    "    header => 'true'\n",
    ");\n",
    "```\n",
    "\n",
    "- Modify the `CREATE OR REPLACE TABLE` statement to create a streaming table.  \n",
    "\n",
    "- Add the keyword `STREAM` in the `FROM` clause to incrementally ingest data from the delta table.\n",
    "\n",
    "- Modify the path in the `FROM` clause to point to your **labuser.default.lab_files** volume path (example: `/Volumes/labuser1234/default/lab_files`). You can statically add the path in the `read_files` function, or use a configuration parameter.\n",
    "- **NOTE:** You can't use the `DA` object in your path. Remember to add a static path or configuration parameter.\n",
    "\n",
    "<br></br>\n",
    "##### 2b. Modify the code (shown below) to create the **silver** streaming table by completing the following in your pipeline project:\n",
    "\n",
    "```\n",
    "CREATE OR REPLACE TABLE lab_2_silver_db.employees_silver_lab4 -- You will have to modify this to create a streaming table in the pipeline\n",
    "AS\n",
    "SELECT\n",
    "    EmployeeID,\n",
    "    FirstName,\n",
    "    upper(Country) AS Country,\n",
    "    Department,\n",
    "    Salary,\n",
    "    HireDate,\n",
    "    date_format(HireDate, 'MMMM') AS HireMonthName,\n",
    "    year(HireDate) AS HireYear, \n",
    "    Operation\n",
    "FROM lab_1_bronze_db.employees_bronze_lab4;                    -- You will have to modify FROM clause to incrementally read in data\n",
    "```\n",
    "\n",
    "- Modify the `CREATE OR REPLACE TABLE` statement to create a streaming table.  \n",
    "\n",
    "- Add the keyword `STREAM` in the `FROM` clause to incrementally ingest data.\n",
    "\n",
    "- Add the following data quality expectations:      \n",
    "```\n",
    "CONSTRAINT check_country EXPECT (Country IN ('US','GR')),\n",
    "CONSTRAINT check_salary EXPECT (Salary > 0),\n",
    "CONSTRAINT check_null_id EXPECT (EmployeeID IS NOT NULL) ON VIOLATION DROP ROW\n",
    "\n",
    "```\n",
    "<br></br>\n",
    "##### 2c. Replace the `CREATE OR REPLACE VIEW` statement in the two views to create materialized views instead of traditional views in your pipeline project.\n",
    "\n",
    "```\n",
    "CREATE OR REPLACE VIEW lab_3_gold_db.employees_by_country_gold_lab4 -- You will have to modify this to create a materialized view in the pipeline\n",
    "AS\n",
    "SELECT \n",
    "    Country,\n",
    "    count(*) AS TotalEmployees,\n",
    "    sum(Salary) AS TotalSalary\n",
    "FROM lab_2_silver_db.employees_silver_lab4\n",
    "GROUP BY Country;\n",
    "\n",
    "\n",
    "CREATE OR REPLACE VIEW lab_3_gold_db.salary_by_department_gold_lab4  -- You will have to modify this to create a materialized view in the pipeline\n",
    "AS\n",
    "SELECT\n",
    "    Department,\n",
    "    sum(Salary) AS TotalSalary\n",
    "FROM lab_2_silver_db.employees_silver_lab4\n",
    "GROUP BY Department;\n",
    "\n",
    "```\n",
    "<br></br>\n",
    "3. Pipeline configuration requirements:\n",
    "\n",
    "- Your Spark Declarative Pipeline should use **Serverless** compute.  \n",
    "\n",
    "- Your pipeline should use your **labuser** catalog by default.  \n",
    "\n",
    "- Your pipeline should use your **default** schema by default.\n",
    "\n",
    "- Make sure your pipeline is including your .sql file only.\n",
    "\n",
    "- (OPTIONAL) If using a configuration variable to point to your path make sure it is setup and applied in the `read_files` function.\n",
    "\n",
    "<br></br>\n",
    "4. When complete, run the pipeline. Troubleshoot any errors.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "##### Final Spark Declarative Pipeline Image  \n",
    "Below is what your final pipeline should look like after the first run with a single CSV file.\n",
    "\n",
    "![Final Lab4 Pipeline](./Includes/images/lab4_solutiongraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76bf5a6a-efaf-4bb2-a0f3-5acf2ad24899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### LAB SOLUTION (optional)\n",
    "If you need the solution, you can view the lab solution code in the **4 - Lab Solution Project** folder. You can also execute the code below to automatically create the Spark Declarative Pipeline with the necessary configuration settings for your specific lab.\n",
    "\n",
    "**NOTE:** After you run the cell, wait 30 seconds for the pipeline to finish creating. Then open one of the files in the **4 - Lab Solution Project** folder to open the new Spark Declarative Pipeline editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb1d4908-003d-4d69-b01c-a0f4e600f410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_declarative_pipeline(pipeline_name=f'4 - Lab Solution Project - {DA.catalog_name}', \n",
    "                            root_path_folder_name='4 - Lab Solution Project',\n",
    "                            catalog_name = DA.catalog_name,\n",
    "                            schema_name = 'default',\n",
    "                            source_folder_names=['solution'],\n",
    "                            configuration = {'source':f'/Volumes/{DA.catalog_name}/default/lab_files'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2141df7f-a933-4114-817e-88725a123955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Explore the Streaming Tables and Materialized Views\n",
    "\n",
    "After you have created and run your Spark Declarative Pipeline, complete the following tasks to explore your new streaming tables and materialized views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eeb1945-dcd9-4ee4-a719-9e16333488dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. In the catalog explorer on the left, expand your **labuser** catalog and expand the following schemas:\n",
    "   - **lab_1_bronze_db**\n",
    "   - **lab_2_silver_db**\n",
    "   - **lab_3_gold_db**\n",
    "\n",
    "    You should see the two streaming tables and materialized views within your schemas (if you don't use the solution, you won't have the **_solution** at the end of the streaming tables and materialized views):\n",
    "\n",
    "   <img src=\"./Includes/images/lab4_solution_schemas.png\" alt=\"Objects in Schemas\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ff753eb-9613-4799-9383-c6ea1006c56b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cell below to view the data in your **labuser.lab_1_bronze_db.employees_bronze_lab4** streaming table. Notice that the first row contains a `null` **EmployeeID**.\n",
    "\n",
    "**NOTE:** If you ran the solution pipeline, the streaming table is named **employees_bronze_lab4_solution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "255e0f8a-ff3b-4708-9eba-30baaa6fbacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM lab_1_bronze_db.employees_bronze_lab4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e53dc07e-a960-4a57-8591-fb5714b450bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the cell below to view the data in your **labuser.lab_2_silver_db.employees_silver_lab4** streaming table. Notice that the silver table removed the **EmployeeID** value that contained a `null` using a data quality expectation.\n",
    "\n",
    "**NOTE:** If you ran the solution pipeline, the streaming table is named **employees_silver_lab4_solution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60dd3941-3170-4d29-969f-a6d1e799f178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM lab_2_silver_db.employees_silver_lab4;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb79500c-678d-44e1-ba9b-1a0c45182595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Run the cell below to view the data in your **labuser.lab_3_gold_db.employees_by_country_gold_lab4** materialized view. \n",
    "\n",
    "    **Final Results**\n",
    "    | Country | TotalCount | TotalSalary |\n",
    "    |---------|------------|-------------|\n",
    "    | GR      | 2          | 108000      |\n",
    "    | US      | 3          | 201000      |\n",
    "\n",
    "**NOTE:** If you ran the solution pipeline, the materialized view is named **employees_by_country_gold_lab4_solution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "656e2fb5-8dbe-4bc1-ad16-6e4f8e3968f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM lab_3_gold_db.employees_by_country_gold_lab4\n",
    "ORDER BY TotalSalary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6082caaf-58f8-4057-91b6-9e7f5122e1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Run the cell below to view the data in your **labuser.lab_3_gold_db.salary_by_department_gold_lab4** materialized view. \n",
    "\n",
    "    **Final Results**\n",
    "    | Department  | TotalSalary |\n",
    "    |-------------|-------------|\n",
    "    | Sales          | 141000      |\n",
    "    | IT          | 168000      |\n",
    "\n",
    "**NOTE:** If you ran the solution pipeline, the materialized view is named **salary_by_department_gold_lab4_solution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e968964-ec23-4a7c-90da-1814a652fe29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM lab_3_gold_db.salary_by_department_gold_lab4\n",
    "ORDER BY TotalSalary;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54f22d35-8add-48f3-a571-c30113bdc6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. CHALLENGE SCENARIO (OPTIONAL IN LIVE TEACH)\n",
    "### Duration: ~10 minutes\n",
    "\n",
    "**NOTE:** *If you finish early in a live class, feel free to complete the challenge below. The challenge is optional and most likely won't be completed during the live class. Only continue if your Spark Declarative Pipeline was set up correctly in the previous section by comparing your pipeline to the solution image.*\n",
    "\n",
    "**SCENARIO:** In the challenge, you will land a new CSV file in your **lab_files** cloud storage volume and rerun the pipeline to watch the Spark Declarative Pipeline only ingest the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b634d15e-bce8-423e-80f7-fc0db773dd17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to copy another file to your **labuser.default.lab_user** volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ff327ce-5761-4f2e-8b45-ea26c41a7e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LabSetup.copy_file(copy_file = 'employees_2.csv', \n",
    "                   to_target_volume = f'/Volumes/{DA.catalog_name}/default/lab_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "830c4228-a2f0-4543-804c-4852702708c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. In the left navigation area, navigate to your **labuser.default.lab_files** volume and expand the volume. Confirm it contains two CSV files: \n",
    "    - **employees_1.csv** \n",
    "    - **employees_2.csv**\n",
    "\n",
    "**NOTE:** You might have to refresh your catalogs if the file is not shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d26841-2d9e-46fa-818c-4e9210e5408d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the cell below to preview only the new CSV file and view the results. Notice that the new CSV file contains employee information:\n",
    "\n",
    "    - Contains 4 rows.  \n",
    "    - The **Operation** column specifies an action for each employee (e.g., update the record, delete the record, or add a new employee).\n",
    "\n",
    "**NOTE:** Don’t worry about the **Operation** column yet. We’ll cover how to capture these specific changes in your data (Change Data Capture) in a later demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad330612-92a9-46b6-85dd-189944e7eb77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM read_files(                                           \n",
    "  '/Volumes/' || DA.catalog_name || '/default/lab_files/employees_2.csv',  \n",
    "  format => 'CSV',\n",
    "  header => 'true'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f303d23b-085f-443c-9871-98cfa4b67cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Now that you have explored the new CSV file in cloud storage, go back to your Spark Declarative Pipeline and select **Run pipeline**. Notice that the pipeline only read in the new file in cloud storage.\n",
    "\n",
    "\n",
    "##### Final Spark Declarative Pipeline Image\n",
    "Below is what your final pipeline should look like after the first run with a single CSV file.\n",
    "\n",
    "![Final Challenge Lab4 DLT Pipeline](./Includes/images/lab4_solutionchallenge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a71f4fb-d347-48c4-859d-11b635acecb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Explore the history of your streaming tables using the Catalog Explorer. Notice that there are two appends to both the **bronze** and **silver** tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fad38335-d2ec-4054-a5ce-eeceeb8d0667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "4 Lab - Create a Pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
