{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1346893d-5902-421c-bc7d-a2f77687d190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../Includes/_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "064682a4-3e23-4a3b-87c6-ff38ef03e9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA = DBAcademyHelper()\n",
    "DA.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "378cbb2a-8e6b-4c75-acfc-6a4c7b8cf9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- CREATE DA VARIABLE USING SQL FOR USER INFORMATION FROM THE META TABLE\n",
    "\n",
    "-- Create a temp view storing information from the obs table.\n",
    "CREATE OR REPLACE TEMP VIEW user_info AS\n",
    "SELECT map_from_arrays(collect_list(replace(key,'.','_')), collect_list(value))\n",
    "FROM dbacademy.ops.meta;\n",
    "\n",
    "-- Create SQL dictionary var (map)\n",
    "DECLARE OR REPLACE DA MAP<STRING,STRING>;\n",
    "\n",
    "-- Set the temp view in the DA variable\n",
    "SET VAR DA = (SELECT * FROM user_info);\n",
    "\n",
    "DROP VIEW IF EXISTS user_info;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f07ed6d-d042-4d2e-b898-d5cf6ede6137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_volume(in_catalog: str, in_schema: str, volume_name: str):\n",
    "    '''\n",
    "    Create a volume in the specified catalog.schema.\n",
    "    '''\n",
    "    print(f'Creating volume: {in_catalog}.{in_schema}.{volume_name} if not exists.\\n')\n",
    "    r = spark.sql(f'CREATE VOLUME IF NOT EXISTS {in_catalog}.{in_schema}.{volume_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52b20010-9558-4cce-a597-223de7ea0f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def create_catalogs(self, catalog_suffix: list):\n",
    "def create_schemas(in_catalog: str, schema_names: list):\n",
    "    '''\n",
    "    Create schemas for the course in the specified catalog. Use DA.catalog_name in vocareum.\n",
    "\n",
    "    If the schemas do not exist in the environment it will creates the schemas based the user's schema_name list.\n",
    "\n",
    "    Parameters:\n",
    "    - schema_names (list): A list of strings representing schema names to creates.\n",
    "\n",
    "    Returns:\n",
    "        Log information:\n",
    "            - If schemas(s) do not exist, prints information on the schemas it created.\n",
    "            - If schemas(s) exist, prints information that schemas exist.\n",
    "\n",
    "    Example:\n",
    "    -------\n",
    "    - create_schemas(in_catalog = DA.catalog_name, schema_names = ['1_bronze', '2_silver', '3_gold'])\n",
    "    '''\n",
    "\n",
    "    ## Current schemas in catalog\n",
    "    list_of_curr_schemas = spark.sql(f'SHOW SCHEMAS IN {in_catalog}').toPandas().databaseName.to_list()\n",
    "\n",
    "    # Create schema in catalog if not exists\n",
    "    for schema in schema_names:\n",
    "        if schema not in list_of_curr_schemas:\n",
    "            print(f'Creating schema: {in_catalog}.{schema}.')\n",
    "            spark.sql(f'CREATE SCHEMA IF NOT EXISTS {in_catalog}.{schema}')\n",
    "        else:\n",
    "            print(f'Schema {in_catalog}.{schema} already exists. No action taken.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed7e2a4b-b174-493b-a21d-1431e50bc972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_if_schemas_are_created(in_catalog: str, check_schemas: list[str]):\n",
    "    '''\n",
    "    Search for the specified catalogs by looking at the last word separated by the _. Return error if those don't exist.\n",
    "\n",
    "    Parameters:\n",
    "    - in_catalog: Catalog to check for schemas\n",
    "    - check_schemas: List of schemas to check for in the catalog above\n",
    "        \n",
    "    Returns:\n",
    "    - Raises an error if the catalogs don't exists. Otherwise prints a successful note.\n",
    "\n",
    "    Example:\n",
    "    - check_if_schemas_are_created(in_catalog='mycatalog', check_schemas=['bronze', 'silver', 'gold'])\n",
    "    '''\n",
    "\n",
    "    ## Current schemas in catalog\n",
    "    list_of_curr_schemas = set(spark.sql(f'SHOW SCHEMAS IN {in_catalog}').toPandas().databaseName.to_list())\n",
    "\n",
    "    # Convert check_catalogs list to a set\n",
    "    check_schemas_set = set(check_schemas)\n",
    "    \n",
    "    # Check if all items are in the predefined items set\n",
    "    missing_items = check_schemas_set - list_of_curr_schemas\n",
    "    \n",
    "    if missing_items:\n",
    "        # If there are any missing items, raise an error\n",
    "        raise ValueError(f\"The necessary schemas in your labuser schema ({check_schemas_set}) do not exist in your workspace. Please run the 0 - REQUIRED - Course Setup and Exploration notebook to setup your environment for this course.\")\n",
    "    \n",
    "    # If all items are found, return True\n",
    "    print(f'Schemas are available, lab check passed: {sorted(check_schemas_set)}.')\n",
    "\n",
    "\n",
    "# check_if_schemas_are_created(in_catalog=DA.catalog_name, check_schemas=[DA.schema_bronze, DA.schema_silver, DA.schema_gold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "418da5fe-5fc7-45a3-b1bf-83fde8d64a98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delete_source_files(source_files: str):\n",
    "    \"\"\"\n",
    "    Deletes all files in the specified source volume.\n",
    "\n",
    "    This function iterates through all the files in the given volume,\n",
    "    deletes them, and prints the name of each file being deleted.\n",
    "\n",
    "    Parameters:\n",
    "    - source_files : str\n",
    "        The path to the volume containing the files to delete. \n",
    "        Use the {DA.paths.working_dir} to dynamically navigate to the user's volume location in dbacademy/ops/vocareumlab@name:\n",
    "            Example: DA.paths.working_dir = /Volumes/dbacademy/ops/vocareumlab@name\n",
    "\n",
    "    Returns:\n",
    "    - None. This function does not return any value. It performs file deletion and prints all files that it deletes. If no files are found it prints in the output.\n",
    "\n",
    "    Example:\n",
    "    - delete_source_files(f'{DA.paths.working_dir}/pii/stream_source/user_reg')\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "\n",
    "    print(f'\\nSearching for files in {source_files} volume to delete prior to creating files...')\n",
    "    if os.path.exists(source_files):\n",
    "        list_of_files = sorted(os.listdir(source_files))\n",
    "    else:\n",
    "        list_of_files = None\n",
    "\n",
    "    if not list_of_files:  # Checks if the list is empty.\n",
    "        print(f\"No files found in {source_files}.\\n\")\n",
    "    else:\n",
    "        for file in list_of_files:\n",
    "            file_to_delete = source_files + file\n",
    "            print(f'Deleting file: {file_to_delete}')\n",
    "            dbutils.fs.rm(file_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c7ec679-7f07-4f1f-9f9d-b749eac4d67b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def copy_files(copy_from: str, copy_to: str, n: int, sleep=2):\n",
    "    '''\n",
    "    Copy files from one location to another destination's volume.\n",
    "\n",
    "    This method performs the following tasks:\n",
    "      1. Lists files in the source directory and sorts them. Sorted to keep them in the same order when copying for consistency.\n",
    "      2. Verifies that the source directory has at least `n` files.\n",
    "      3. Copies files from the source to the destination, skipping files already present at the destination.\n",
    "      4. Pauses for `sleep` seconds after copying each file.\n",
    "      5. Stops after copying `n` files or if all files are processed.\n",
    "      6. Will print information on the files copied.\n",
    "    \n",
    "    Parameters\n",
    "    - copy_from (str): The source directory where files are to be copied from.\n",
    "    - copy_to (str): The destination directory where files will be copied to.\n",
    "    - n (int): The number of files to copy from the source. If n is larger than total files, an error is returned.\n",
    "    - sleep (int, optional): The number of seconds to pause after copying each file. Default is 2 seconds.\n",
    "\n",
    "    Returns:\n",
    "    - None: Prints information to the log on what files it's loading. If the file exists, it skips that file.\n",
    "\n",
    "    Example:\n",
    "    - copy_files(copy_from='/Volumes/gym_data/v01/user-reg', \n",
    "           copy_to=f'{DA.paths.working_dir}/pii/stream_source/user_reg',\n",
    "           n=1)\n",
    "    '''\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    print(f\"\\n----------------Loading files to user's volume: '{copy_to}'----------------\")\n",
    "\n",
    "    ## List all files in the copy_from volume and sort the list\n",
    "    list_of_files_to_copy = sorted(os.listdir(copy_from))\n",
    "    total_files_in_copy_location = len(list_of_files_to_copy)\n",
    "\n",
    "    ## Get a list of files in the source\n",
    "    list_of_files_in_source = os.listdir(copy_to)\n",
    "\n",
    "    assert total_files_in_copy_location >= n, f\"The source location contains only {total_files_in_copy_location} files, but you specified {n}  files to copy. Please specify a number less than or equal to the total number of files available.\"\n",
    "\n",
    "    ## Looping counter\n",
    "    counter = 1\n",
    "\n",
    "    ## Load files if not found in the co\n",
    "    for file in list_of_files_to_copy:\n",
    "\n",
    "      ## If the file is found in the source, skip it with a note. Otherwise, copy file.\n",
    "      if file in list_of_files_in_source:\n",
    "        print(f'File number {counter} - {file} is already in the source volume \"{copy_to}\". Skipping file.')\n",
    "      else:\n",
    "        file_to_copy = f'{copy_from}/{file}'\n",
    "        copy_file_to = f'{copy_to}/{file}'\n",
    "        print(f'File number {counter} - Copying file {file_to_copy} --> {copy_file_to}.')\n",
    "        dbutils.fs.cp(file_to_copy, copy_file_to , recurse = True)\n",
    "        \n",
    "        ## Sleep after load\n",
    "        time.sleep(sleep) \n",
    "\n",
    "      ## Stop after n number of loops based on argument.\n",
    "      if counter == n:\n",
    "        break\n",
    "      else:\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee260704-ac73-4225-915d-69491a80765b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def copy_file_for_multiple_sources(copy_n_files = 2, \n",
    "                                   sleep_set = 3,\n",
    "                                   copy_from_source=str,\n",
    "                                   copy_to_target=str\n",
    "                                   ):\n",
    "\n",
    "    for n in range(copy_n_files):\n",
    "        n = n + 1\n",
    "        copy_files(copy_from = f'{copy_from_source}/orders/stream_json', copy_to = f'{copy_to_target}/orders', n = n, sleep=sleep_set)\n",
    "        copy_files(copy_from = f'{copy_from_source}/customers/stream_json', copy_to = f'{copy_to_target}/customers', n = n, sleep=sleep_set)\n",
    "        copy_files(copy_from = f'{copy_from_source}/status/stream_json', copy_to = f'{copy_to_target}/status', n = n, sleep=sleep_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7448453e-9f2d-4bfa-8118-2218a31c4734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def create_directory_in_user_volume(user_default_volume_path: str, create_folders: list):\n",
    "    '''\n",
    "    Creates multiple (or single) directories in the specified volume path.\n",
    "\n",
    "    Parameters:\n",
    "    - user_default_volume_path (str): The base directory path where the folders will be created. \n",
    "                                      You can use the default DA.paths.working_dir as the user's volume path.\n",
    "    - create_folders (list): A list of strings representing folder names to be created within the base directory.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return any values but prints log information about the created directories.\n",
    "\n",
    "    Example: \n",
    "    - create_directory_in_user_volume(user_default_volume_path=DA.paths.working_dir, create_folders=['customers', 'orders', 'status'])\n",
    "    '''\n",
    "    \n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    for folder in create_folders:\n",
    "\n",
    "        create_folder = f'{user_default_volume_path}/{folder}'\n",
    "\n",
    "        if not os.path.exists(create_folder):\n",
    "        # If it doesn't exist, create the directory\n",
    "            dbutils.fs.mkdirs(create_folder)\n",
    "            print(f'Creating folder: {create_folder}')\n",
    "\n",
    "        else:\n",
    "            print(f\"Directory {create_folder} already exists. No action taken.\")\n",
    "        \n",
    "    print('----------------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5696a0b2-6a0a-43ff-a2c5-037203e1b734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_config_values(config_values):\n",
    "    \"\"\"\n",
    "    Displays list of key-value pairs as rows of HTML text and textboxes\n",
    "    \n",
    "    Parameters:\n",
    "    - config_values: list of (key, value) tuples\n",
    "        \n",
    "    Returns:\n",
    "    - HTML output displaying the config values\n",
    "\n",
    "    Example:\n",
    "    - DA.display_config_values([('catalog',DA.catalog_name),('schema',DA.schema_name)])\n",
    "    \"\"\"\n",
    "    html = \"\"\"<table style=\"width:100%\">\"\"\"\n",
    "    for name, value in config_values:\n",
    "        html += f\"\"\"\n",
    "        <tr>\n",
    "            <td style=\"white-space:nowrap; width:1em\">{name}:</td>\n",
    "            <td><input type=\"text\" value=\"{value}\" style=\"width: 100%\"></td></tr>\"\"\"\n",
    "    html += \"</table>\"\n",
    "    displayHTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc86d27b-6a23-4b01-bb15-0059236e87e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_tables(in_catalog: str, in_schema: list, dry_run: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Drops all tables and views in the specified schema within a given catalog.\n",
    "\n",
    "    Args:\n",
    "        in_catalog (str): The catalog name (e.g., 'dbacademy_peter').\n",
    "        in_schema (str): The schema name (e.g., 'default').\n",
    "        dry_run (bool): If True, only prints tables that would be dropped without actually dropping them.\n",
    "\n",
    "    Returns:\n",
    "        list: Fully qualified names of the tables that were dropped (or would be dropped in dry-run mode).\n",
    "\n",
    "    Example:\n",
    "    >>> drop_tables(in_catalog='dbacademy_peter', in_schema='1_bronze_db')\n",
    "    \"\"\"\n",
    "    # Check if catalog exists\n",
    "    catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if in_catalog not in catalogs:\n",
    "        raise ValueError(f\"Catalog '{in_catalog}' does not exist.\")\n",
    "\n",
    "    ## Delete tables and views in schema\n",
    "    for schema in in_schema:\n",
    "        \n",
    "        # Check if schema exists in the catalog\n",
    "        full_schema = f\"{in_catalog}.{schema}\"\n",
    "        if not spark.catalog.databaseExists(full_schema):\n",
    "            raise ValueError(f\"Schema '{schema}' does not exist in catalog '{in_catalog}'.\")\n",
    "        \n",
    "        print(f\"\\n{'Previewing' if dry_run else 'Dropping'} all tables in {in_catalog}.{schema}:\")\n",
    "\n",
    "        # Get all tables in the schema\n",
    "        tables = spark.sql(f\"SHOW TABLES IN {in_catalog}.{schema}\").collect()\n",
    "\n",
    "        if not tables:\n",
    "            print(f\"No tables found in schema {in_catalog}.{schema}. Nothing to drop.\")\n",
    "        else:\n",
    "            table_names = [f\"{in_catalog}.{schema}.{t.tableName}\" for t in tables]\n",
    "\n",
    "            for table_full_name in table_names:\n",
    "                if dry_run:\n",
    "                    print(f\"Would drop: {table_full_name}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        spark.sql(f\"DROP TABLE IF EXISTS {table_full_name}\")\n",
    "                        print(f\"Dropped TABLE: {table_full_name}\")\n",
    "                    except:\n",
    "                        spark.sql(f\"DROP VIEW IF EXISTS {table_full_name}\")\n",
    "                        print(f\"Dropped VIEW: {table_full_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fd4a13a-e08e-46b1-a5ea-80fd262d4a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import json\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "\n",
    "def create_declarative_pipeline(pipeline_name: str, \n",
    "                        root_path_folder_name: str,\n",
    "                        source_folder_names: list = [],\n",
    "                        catalog_name: str = 'dbacademy',\n",
    "                        schema_name: str = 'default',\n",
    "                        serverless: bool = True,\n",
    "                        configuration: dict = {},\n",
    "                        continuous: bool = False,\n",
    "                        photon: bool = True,\n",
    "                        channel: str = 'PREVIEW',\n",
    "                        development: bool = True,\n",
    "                        pipeline_type = 'WORKSPACE'\n",
    "                        ):\n",
    "  \n",
    "    '''\n",
    "  Creates the specified DLT pipeline.\n",
    "\n",
    "  Parameters:\n",
    "  ----------\n",
    "  pipeline_name : str\n",
    "      The name of the DLT pipeline to be created.\n",
    "  root_path_folder_name : str\n",
    "      The root folder name where the pipeline will be located. This folder must be in the location where this function is called.\n",
    "  source_folder_names : list, optional\n",
    "      A list of source folder names. Must defined at least one folder within the root folder location above.\n",
    "  catalog_name : str, optional\n",
    "      The catalog name for the DLT pipeline. Default is 'dbacademy'.\n",
    "  schema_name : str, optional\n",
    "      The schema name for the DLT pipeline. Default is 'default'.\n",
    "  serverless : bool, optional\n",
    "      If True, the pipeline will be serverless. Default is True.\n",
    "  configuration : dict, optional\n",
    "      A dictionary of configuration settings for the pipeline. Default is an empty dictionary.\n",
    "  continuous : bool, optional\n",
    "      If True, the pipeline will be run in continuous mode. Default is False.\n",
    "  photon : bool, optional\n",
    "      If True, the pipeline will use Photon for processing. Default is True.\n",
    "  channel : str, optional\n",
    "      The channel for the pipeline, such as 'PREVIEW'. Default is 'PREVIEW'.\n",
    "  development : bool, optional\n",
    "      If True, the pipeline will be set up for development. Default is True.\n",
    "  pipeline_type : str, optional\n",
    "      The type of the pipeline (e.g., 'WORKSPACE'). Default is 'WORKSPACE'.\n",
    "\n",
    "  Returns:\n",
    "  -------\n",
    "  None\n",
    "      This function does not return anything. It creates the DLT pipeline based on the provided parameters.\n",
    "\n",
    "  Example:\n",
    "  --------\n",
    "  create_dlt_pipeline(pipeline_name='my_pipeline_name', \n",
    "                      root_path_folder_name='6 - Putting a DLT Pipeline in Production Project',\n",
    "                      source_folder_names=['orders', 'status'])\n",
    "  '''\n",
    "  \n",
    "    w = WorkspaceClient()\n",
    "    for pipeline in w.pipelines.list_pipelines():\n",
    "        if pipeline.name == pipeline_name:\n",
    "            raise ValueError(f\"Lakeflow Declarative Pipeline name '{pipeline_name}' already exists. Please delete the pipeline using the UI and rerun the cell to recreate the pipeline.\")\n",
    "\n",
    "    ## Create empty dictionary\n",
    "    create_dlt_pipeline_call = {}\n",
    "\n",
    "    ## Pipeline type\n",
    "    create_dlt_pipeline_call['pipeline_type'] = pipeline_type\n",
    "\n",
    "    ## Modify dictionary for specific DLT configurations\n",
    "    create_dlt_pipeline_call['name'] = pipeline_name\n",
    "\n",
    "    ## Set paths to root and source folders\n",
    "    main_course_folder_path = os.getcwd()\n",
    "\n",
    "    main_path_to_dlt_project_folder = os.path.join('/', main_course_folder_path, root_path_folder_name)\n",
    "    create_dlt_pipeline_call['root_path'] = main_path_to_dlt_project_folder\n",
    "\n",
    "    ## Add path of root folder to source folder names\n",
    "    add_path_to_folder_names = [os.path.join(main_path_to_dlt_project_folder, folder_name, '**') for folder_name in source_folder_names]\n",
    "    source_folders_path = [{'glob':{'include':folder_name}} for folder_name in add_path_to_folder_names]\n",
    "    create_dlt_pipeline_call['libraries'] = source_folders_path\n",
    "\n",
    "    ## Set default catalog and schema\n",
    "    create_dlt_pipeline_call['catalog'] = catalog_name\n",
    "    create_dlt_pipeline_call['schema'] = schema_name\n",
    "\n",
    "    ## Set serverless compute\n",
    "    create_dlt_pipeline_call['serverless'] = serverless\n",
    "\n",
    "    ## Set configuration parameters\n",
    "    create_dlt_pipeline_call['configuration'] = configuration\n",
    "\n",
    "    ## Set if continouous or not\n",
    "    create_dlt_pipeline_call['continuous'] = continuous \n",
    "\n",
    "    ## Set to use Photon\n",
    "    create_dlt_pipeline_call['photon'] = photon\n",
    "\n",
    "    ## Set DLT channel\n",
    "    create_dlt_pipeline_call['channel'] = channel\n",
    "\n",
    "    ## Set if development mode\n",
    "    create_dlt_pipeline_call['development'] = development\n",
    "\n",
    "    ## Creat DLT pipeline\n",
    "\n",
    "    print(f\"Creating the Lakeflow Declarative Pipeline '{pipeline_name}'...\")\n",
    "    print(f\"Root folder path: {main_path_to_dlt_project_folder}\")\n",
    "    print(f\"Source folder path(s): {source_folders_path}\")\n",
    "\n",
    "    w.api_client.do('POST', '/api/2.0/pipelines', body=create_dlt_pipeline_call)\n",
    "    print(f\"\\nLakeflow Declarative Pipeline Creation '{pipeline_name}' Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cf9c111-0e4b-4415-ba32-dcf22ce8d1b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def setup_complete():\n",
    "  '''\n",
    "  Prints a note in the output that the setup was complete.\n",
    "  '''\n",
    "  print('\\n\\n\\n------------------------------------------------------------------------------')\n",
    "  print('SETUP COMPLETE!')\n",
    "  print('------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee50d7c5-9af6-4901-85e4-7374ecf1914f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# @DBAcademyHelper.add_method\n",
    "# def create_DA_keys(self): \n",
    "#     '''\n",
    "#     Create the DA references to the bronze, silver and gold catalogs for the user.\n",
    "#     '''\n",
    "#     print('Set DA dynamic references to the dev, stage and prod catalogs.\\n')\n",
    "#     setattr(DA, f'catalog_bronze', f'{self.catalog_name}_bronze_1')\n",
    "#     setattr(DA, f'catalog_silver', f'{self.catalog_name}_silver_2')\n",
    "#     setattr(DA, f'catalog_gold', f'{self.catalog_name}_gold_3')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-Common",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
