{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aa79529-58ef-4f0a-beb5-c743461c27a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5011f8f8-ccec-4378-b5d9-cd3b181c2944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1 - REQUIRED - Course Setup and Creating a Pipeline\n",
    "\n",
    "In this demo, we'll set up the course environment, explore its components, build a traditional ETL pipeline using JSON files as the data source, and then learn how to create a sample Lakeflow Spark Declarative Pipeline (SDP).\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "- Efficiently navigate the Workspace to locate course catalogs, schemas, and source files.\n",
    "- Create a Lakeflow Spark Declarative Pipeline using the Workspace and the Pipeline UI.\n",
    "\n",
    "\n",
    "### IMPORTANT - PLEASE READ!\n",
    "- **REQUIRED** - This notebook is required for all users to run. If you do not run this notebook, you will be missing the necessary files and schemas required for the rest of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c88c86cb-6d35-497a-83ee-b90a5e4d5f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE (your cluster starts with **labuser**)\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "230706dd-6602-4593-aec5-cdb1f56dbb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically create and reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ca2565e-cc8c-490d-b2d4-6ef8e1fe3d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-1-setup-REQUIRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39a4e9a0-1192-40e0-8a58-19687bab1502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Explore the Lab Environment\n",
    "\n",
    "Explore the raw data source files, catalogs, and schema in the course lab environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08c91520-326f-448e-b186-8aecc0af1aef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Complete these steps to explore your user catalog and schemas you will be using in this course:\n",
    "\n",
    "   - a. Select the **Catalog** icon ![Catalog Icon](./Includes/images/catalog_icon.png) in the left navigation bar.\n",
    "\n",
    "   - b. You should see your unique catalog, named something like **labuser1234_56789**. You will use this catalog throughout the course.\n",
    "\n",
    "   - c. Expand your **labuser** catalog. It should contain the following schemas:\n",
    "     - **1_bronze_db**\n",
    "     - **2_silver_db**\n",
    "     - **3_gold_db**\n",
    "     - **default**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "106c4a6b-7591-44b7-b7b9-78df4f3b7e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete the following steps to view where our streaming raw source files are coming from:\n",
    "\n",
    "   a. Select the **Catalog** icon ![Catalog Icon](./Includes/images/catalog_icon.png) in the left navigation bar.\n",
    "\n",
    "   b. Expand the **dbacademy** catalog.\n",
    "\n",
    "   c. Expand the **ops** schema and then **Volumes**.\n",
    "\n",
    "   d. Expand your **labuser@vocareum** volume. You should notice that your volume contains three folders:\n",
    "   - **customers**\n",
    "   - **orders**\n",
    "   - **status**\n",
    "\n",
    "   e. Expand each folder and notice that each cloud storage location contains a single JSON file to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "360334a1-325f-48cc-9ff1-988b0f214d38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. To easily reference this volume path (`/Volumes/dbacademy/ops/your-labuser-name`) throughout the course, you can use the:\n",
    "   - The python `DA.paths.working_dir` variable\n",
    "   - The SQL `DA.paths_working_dir` variable\n",
    "\n",
    "   Run the cells below and confirm that the path points to your volume.\n",
    "\n",
    "   **Example:** `/Volumes/dbacademy/ops/labuser1234_5678@vocareum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed15dd20-ce80-4b66-8696-608fcd91d833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## With Python\n",
    "print(DA.paths.working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e9f5cdf-8850-481f-bbab-727bd0a79c82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- With SQL\n",
    "values(DA.paths_working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20c78e35-5802-4d84-8994-bf572feeda91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Build a Traditional ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9799ea45-7549-4eb5-89e5-2c92ca9f0ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Query the raw JSON file(s) in your `/Volumes/dbacademy/ops/your-labuser-name/orders` volume to preview the data. \n",
    "\n",
    "      Notice that the JSON file is displayed ingested into tabular form using the `read_files` function. Take note of the following:\n",
    "\n",
    "    a. The **orders** JSON file contains order data for a company.\n",
    "\n",
    "    b. The one JSON file in your **/orders** volume (**00.json**) contains 174 rows. Remember that number for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "adfd414a-d632-4788-bcd7-95fce465d239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'''\n",
    "          SELECT * \n",
    "          FROM json.`{DA.paths.working_dir}/orders`\n",
    "          ''').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d1e1c8-395c-4f89-8c96-f71effbff28b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Traditionally, you would build an ETL pipeline by reading all of the files within the cloud storage location each time the pipeline runs. As data scales, this method becomes inefficient, more expensive, and time-consuming.\n",
    "\n",
    "   For example, you would write code like below.\n",
    "\n",
    "   **NOTES:** \n",
    "   - The tables and views will be written to your **labuser.default** schema (database).\n",
    "   - Knowledge of the Databricks `read_files` function is prerequisite for this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90b39b2a-a93d-4531-81d8-3433e6a141ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- JSON -> Bronze\n",
    "-- Read ALL files from your working directory each time the query is executed\n",
    "CREATE OR REPLACE TABLE default.orders_bronze\n",
    "AS \n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() AS processing_time,\n",
    "  _metadata.file_name AS source_file\n",
    "FROM read_files(\n",
    "    DA.paths_working_dir || \"/orders\", \n",
    "    format =>\"json\");\n",
    "\n",
    "\n",
    "-- Bronze -> Silver\n",
    "-- Read the entire bronze table each time the query is executed\n",
    "CREATE OR REPLACE TABLE default.orders_silver\n",
    "AS \n",
    "SELECT \n",
    "  order_id,\n",
    "  timestamp(order_timestamp) AS order_timestamp, \n",
    "  customer_id,\n",
    "  notifications\n",
    "FROM default.orders_bronze;   \n",
    "\n",
    "\n",
    "-- Silver -> Gold\n",
    "-- Aggregate the silver each time the query is executed.\n",
    "CREATE OR REPLACE VIEW default.orders_by_date_vw     \n",
    "AS \n",
    "SELECT \n",
    "  date(order_timestamp) AS order_date, \n",
    "  count(*) AS total_daily_orders\n",
    "FROM default.orders_silver                               \n",
    "GROUP BY date(order_timestamp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d421ed6-8cb9-4be9-a2d0-c463db6a94a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the code in the cells to view the **orders_bronze** and **orders_silver** tables, and the **orders_by_date_vw** view. Explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19a8a893-75ce-4ab0-9b55-d8533585360b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM default.orders_bronze\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd80ccf-057b-4bd6-b0f6-6545dd5933d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM default.orders_silver\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1063d50c-5b90-4197-95fc-26f0281b12c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM default.orders_by_date_vw\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f9708c9-9bba-4ea2-9cd0-f49bad488f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Considerations\n",
    "\n",
    "- As JSON files are added to the volume in cloud storage, your **bronze table** code will read **all** of the files each time it executes, rather than reading only new rows of raw data. As the data grows, this can become inefficient and costly.\n",
    "\n",
    "- The **silver table** code will always read all the rows from the bronze table to prepare the silver table. As the data grows, this can also become inefficient and costly.\n",
    "\n",
    "- The traditional view, **orders_by_date_vw**, executes each time it is called. As the data grows, this can become inefficient.\n",
    "\n",
    "- To check data quality as new rows are added, additional code is needed to identify any values that do not meet the required conditions.\n",
    "\n",
    "- Monitoring the pipeline for each run is a challenge.\n",
    "\n",
    "- There is no simple user interface to explore, monitor, or fix issues everytime the code runs.\n",
    "\n",
    "### We can automatically process data incrementally, manage infrastructure, monitor, observe, optimize, and view this ETL pipeline by converting this to use **Spark Declarative Pipelines**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07a4cb0f-0cfd-4680-8e19-6be8501a46c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Get Started Creating a Lakeflow Spark Declarative Pipeline Using the New Lakeflow Pipelines Editor\n",
    "\n",
    "In this section, we'll show you how to start creating a Spark Declarative Pipeline using the new Lakeflow Pipelines Editor. We won't run or modify the pipeline just yet!\n",
    "\n",
    "There are a few different ways to create your pipeline. Let's explore these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65ae3517-6e17-435d-a16d-aa33b4091d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. First, complete the following steps to enable the new **Lakeflow Pipelines Editor**:\n",
    "\n",
    "   **NOTE:** This is being updated and how to enable it might change slightly moving forward.\n",
    "\n",
    "   a. In the top-right corner, select your user icon ![User Lab Icon](./Includes/images/user_lab_circle_icon.png).\n",
    "\n",
    "   b. Right-click on **Settings** and select **Open in New Tab**.\n",
    "\n",
    "   c. Select **Developer**.\n",
    "\n",
    "   d. Scroll to the bottom and enable **Lakeflow Pipelines Editor** if it's not enabled and Click **Enable tabs for notebooks and files**.\n",
    "\n",
    "   ![Lakeflow Pipeline Editor](./Includes/images/lakeflow-pipeline-editor.png)\n",
    "\n",
    "   e. Refresh your browser page to enable the option you turned on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82070d26-2c40-481e-8073-458ff866f753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D1. Create a Spark Declarative Pipeline Using the File Explorer\n",
    "1. Complete the following steps to create a Spark Declarative Pipeline using the left navigation pane:\n",
    "\n",
    "   a. In the left navigation bar, select the **Folder** ![Folder Icon](./Includes/images/folder_icon.png) icon to open the Workspace navigation.\n",
    "\n",
    "   b. Navigate to the **Build Data Pipelines with Lakeflow Spark Declarative Pipelines** folder (you are most likely already there).\n",
    "\n",
    "   c. (**PLEASE READ**) To complete this demonstration, it'll be easier to open this same notebook in another tab to follow along with these instructions. Right click on the notebook **1 - REQUIRED - Course Setup and Creating a Pipeline** and select **Open in a New Tab**.\n",
    "\n",
    "   d. In the other tab select the three ellipsis icon ![Ellipsis Icon](./Includes/images/ellipsis_icon.png) in the folder navigation bar.\n",
    "\n",
    "   e. Select **Create** -> **ETL Pipeline**:\n",
    "      - If you have not enabled the new **Lakeflow Pipelines Editor** a pop-up might appear asking you to enable the new editor. Select **Enable** here or complete the previous step.\n",
    "\n",
    "      </br>\n",
    "\n",
    "      - Then use the following information:\n",
    "\n",
    "         - **Name**: `yourfirstname-my-pipeline-project`\n",
    "\n",
    "         - **Default catalog**: Select your **labuser** catalog\n",
    "\n",
    "         - **Default schema**: Select your **default** schema (database)\n",
    "\n",
    "         - Select **Start with sample code in SQL**\n",
    "\n",
    "         The project will open up in the pipeline editor and look like the following:\n",
    "\n",
    "      ![Pipeline Editor](./Includes/images/new_pipeline_editor_sample.png)\n",
    "\n",
    "   f. This will open your Spark Declarative Pipeline within the **Lakeflow Pipelines Editor**. By default, the project creates multiple folders and sample files for you as a starter. You can use this sample folder structure or create your own. Notice the following in the pipeline editor:\n",
    "\n",
    "      - The Spark Declarative Pipeline is located within the **Pipeline** tab.\n",
    "\n",
    "      - Here, you start with a sample project and folder structure.\n",
    "\n",
    "      - To navigate back to all your files and folders, select **All Files**.\n",
    "\n",
    "      - We will explore the pipeline editor and running a pipeline in the next demonstration.\n",
    "\n",
    "   g. Close the link with the sample pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47fdfc6c-f76e-429f-a8ad-51a49cb18f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. Create a Spark Declarative Pipeline Using the Pipeline UI\n",
    "1. You can also create a Spark Declarative Pipeline using the far-left main navigation bar by completing the following steps:\n",
    "\n",
    "   a. On the far-left navigation bar, right-click **Jobs and Pipelines** and select **Open Link in New Tab**.\n",
    "\n",
    "   b. Find the blue **Create** button and select it.\n",
    "\n",
    "   c. Select **ETL pipeline**.\n",
    "\n",
    "   d. The same **Create pipeline** pop-up appears as before. \n",
    "\n",
    "   e. Here select **Add existing assets**. \n",
    "\n",
    "   f. The **Add existing assets** button enables you to select a folder with pipeline assets. This option will enable you to associate this new pipeline with code files already available in your Workspace, including Git folders.\n",
    "\n",
    "   <img src=\"./Includes/images/existing_assets.png\" alt=\"Existing Assets\" width=\"400\">\n",
    "\n",
    "\n",
    "   g. You can close out of the pop up window and close the pipeline tab. You do not need to select a folder yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0e1dd8c-8f65-41bd-8cc5-98631b1793e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "1 - REQUIRED - Course Setup and Creating a Pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
