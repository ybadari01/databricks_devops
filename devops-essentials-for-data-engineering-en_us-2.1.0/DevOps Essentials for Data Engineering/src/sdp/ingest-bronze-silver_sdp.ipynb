{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e58e287f-92a3-4851-a6b6-b3c8327226c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simple Ingest-Bronze-Silver SDP Example\n",
    "This Spark Declarative Pipeline(SDP) provides a basic example, demonstrating how to set expectations on tables. It serves as a starting point, and you can easily add more tables or expectations based on your specific needs. For simplicity, this example will focus on the essentials.\n",
    "\n",
    "Please check out the following resources for more information.\n",
    "\n",
    "- [Manage data quality with pipeline expectations](https://docs.databricks.com/aws/en/ldp/expectations#manage-data-quality-with-pipeline-expectations)\n",
    "\n",
    "- [Expectation recommendations and advanced patterns](https://docs.databricks.com/aws/en/ldp/expectation-patterns#expectation-recommendations-and-advanced-patterns)\n",
    "\n",
    "- [Applying software development & DevOps best practices to Delta Live Table pipelines](https://www.databricks.com/blog/applying-software-development-devops-best-practices-delta-live-table-pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c57eaf83-7662-430a-97c0-c2f82cf7d3d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "## Add previous folder to python path to import our helpers package\n",
    "import sys\n",
    "sys.path.append('../.')\n",
    "from helpers import project_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0753bd4-b027-4971-a6b2-82e500c6dfa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Obtain Configuration Variables\n",
    "This raw source data path and catalog will dynamically be set using the configuration variable set in the Spark Declarative Pipeline for each environment: **development**, **stage**, or **production**.\n",
    "\n",
    "- **development** – Reads the dev CSV file from **your_unique_catalog_1_dev.default.health.dev_health.csv**.\n",
    "\n",
    "- **stage** – Reads the stage CSV file from **your_unique_catalog_2_stage.default.health.stage_health.csv**.\n",
    "\n",
    "- **production** – Reads CSV files in the production volume **your_unique_catalog_2_stage.default.health/*.csv**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60956f61-33d5-45c7-8c6d-ac8b00b263aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Store the target configuration environment in the variable targert\n",
    "target = spark.conf.get(\"target\")\n",
    "\n",
    "## Store the target raw data configuration in the variable raw_data_path\n",
    "raw_data_path = spark.conf.get(\"raw_data_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "584665e1-ae4d-448a-bf9c-78d483cc231f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Ingest CSV Files -> health_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97cb5727-039f-4f8c-afa6-7f46b0d81a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## The health_bronze table is created using the value based on the target variable.\n",
    "## development - import the DEV CSV\n",
    "## stage - import the STAGE CSV\n",
    "## production - import the daily CSV files from our production source volume\n",
    "\n",
    "\n",
    "## Simple expectations for the bronze table\n",
    "valid_rows = {\n",
    "        \"not_null_pii\": \"PII IS NOT NULL\", \n",
    "        \"valid_date\": \"date IS NOT NULL\"\n",
    "    }\n",
    "\n",
    "@dp.table(\n",
    "    comment = \"This table will be used to ingest the raw CSV files and add metadata columns to the bronze table.\",\n",
    "    table_properties = {\"quality\": \"bronze\"}\n",
    ")\n",
    "\n",
    "## Fail process if expectation is not met\n",
    "@dp.expect_all_or_fail(valid_rows)\n",
    "\n",
    "def health_bronze():\n",
    "    return (\n",
    "        spark\n",
    "        .readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\",\"true\")\n",
    "        .schema(project_functions.get_health_csv_schema())   ## Use the custom schema we created\n",
    "        .load(raw_data_path)   ## <--------------- Path is based on the configuration parameter set (DEV, STAGE, PROD)\n",
    "        .select(\n",
    "            \"*\",\n",
    "            \"_metadata.file_name\",\n",
    "            \"_metadata.file_modification_time\",\n",
    "            F.current_timestamp().alias(\"processing_time\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b7d486f-c72b-4cdd-8a0e-1c17f0947831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Silver Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95538c8c-9f26-4c95-9e1e-382c411a30f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dp.table(\n",
    "    comment = \"This table will create, drop and categorize columns from the bronze table.\",\n",
    "    table_properties = {\"quality\": \"bronze\"}\n",
    ")\n",
    "def health_silver():\n",
    "    return (\n",
    "        dp\n",
    "        .read_stream(\"health_bronze\")\n",
    "        .withColumn(\"HighCholest_Group\", project_functions.high_cholest_map(\"HighCholest\"))  # UDF - highcholest_map \n",
    "        .withColumn(\"Age_Group\", project_functions.group_ages_map(\"Age\"))                   # UDF - group_ages_map Age\n",
    "        .drop(\"file_name\", \"file_modification_time\", \"processing_time\")         # Drop unnecessary metadata columns\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest-bronze-silver_sdp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
