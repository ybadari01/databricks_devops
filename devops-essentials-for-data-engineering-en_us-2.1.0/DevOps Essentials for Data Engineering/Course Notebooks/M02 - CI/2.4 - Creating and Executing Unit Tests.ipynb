{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0143ef37-1576-41c1-9676-4c17b1ee8524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a79436fc-e4cd-492e-8a99-86fa13adff17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.4 - Creating and Executing Unit Tests\n",
    "\n",
    "A unit test is a type of software testing that focuses on verifying the smallest parts of an application, typically individual functions or methods in isolation. The goal is to ensure that each unit of code works as expected, producing the correct output for a given input. Unit tests are typically automated and run frequently during development to catch bugs early and maintain code quality.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Write simple unit tests within a notebook to verify the functionality of the code.\n",
    "- Store unit tests in an external .py file, evaluating the advantages of externalizing tests for maintainability.\n",
    "- Use the `pytest` package to automatically discover and execute test functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ee93bc4-0cf2-4ba3-8562-565e34a40124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c3859ad-385f-4d93-98f9-8101eaff281b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Create Simple Unit Tests in a Databricks Notebook\n",
    "\n",
    "Let's perform simple unit tests on the functions we created in the previous demonstration. To create a unit test for a function:\n",
    "\n",
    "  **a. Create the Initial Function**  \n",
    "  Create the initial function you want to use in production. Ensure it has clear inputs and outputs that can be easily validated.\n",
    "\n",
    "**b. Create One or More Test Functions**  \n",
    "Write a unit test function (or multiple functions to test different expectations) that calls the initial function with sample inputs and checks if the actual function output matches the defined expected result using assertions. Name the test function with the keyword `test_` followed by a description of what you are testing.  \n",
    "\n",
    "**c. Execute the Unit Test Functions**  \n",
    "Run and review the test using a testing framework like `pytest`. Review the results to ensure the function behaves as expected, and fix any issues if the test fails.\n",
    "\n",
    "**NOTE:** There are many available frameworks you can use. We will use `pytest` in this course. The framework you choose should be discussed with your team or organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7008a98-fad6-446a-b7f1-6abcbf560953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Complete the following to view the custom functions for this project:\n",
    "\n",
    "   a. Navigate to the main course folder **DevOps Essentials for Data Engineering**.  \n",
    "\n",
    "   b. Select **src**.  \n",
    "\n",
    "   c. Then select **helpers**.  \n",
    "\n",
    "   d. Right-click on **[project_functions.py]($../../src/helpers/project_functions.py)**  and select *Open in new tab*.  \n",
    "\n",
    "   e. Review the **project_functions.py** file. Notice it contains the following functions that we saw in the previous notebook: \n",
    "\n",
    "     - `get_health_csv_schema` - Returns the schema for the health data CSVs.  \n",
    "\n",
    "     - `highcholest_map` - Maps a cholesterol value to a categorical label.  \n",
    "\n",
    "     - `group_ages_map` - Maps an age value to an age group category.  \n",
    "\n",
    "   f. Close the **project_functions.py** tab.\n",
    "\n",
    "   g. Run the cell below to import the custom functions from the **helpers** module.\n",
    "\n",
    "**NOTE:** `sys.path.append()` Adds the root folder path to the system path (this allows you to import modules from this folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83e545b8-8bb2-42ae-bcf9-e0e145186ecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Adds the parent directory of the current working directory to the Python search path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_path = os.getcwd()\n",
    "print(f\"Current path JJ: {current_path}\")\n",
    "# Get the root folder path by navigating two levels up from the current path\n",
    "root_folder_path = os.path.dirname(os.path.dirname(current_path))\n",
    "print(f\"Current path JJ: {root_folder_path}\")\n",
    "# Add the root folder path to the system path (this allows you to import modules from this folder)\n",
    "sys.path.append(root_folder_path)\n",
    "\n",
    "# Print a message confirming the root folder path added to sys.path\n",
    "print(f'Add the following path: {root_folder_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb5eb53-846f-41e9-83df-f2f9296cf5ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Import the custom functions\n",
    "from src.helpers import project_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff9bc366-0ff3-4233-99be-5fd952254331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Create the test function `test_get_health_csv_schema_match` to test the `get_health_csv_schema` function from above. \n",
    "\n",
    "      Within the test function:\n",
    "\n",
    "   - The variable **actual_schema** holds the schema result from the `get_health_csv_schema` function.\n",
    "\n",
    "   - The variable **expected_schema** specifies the schema we expect the function to return if it works as expected.\n",
    "\n",
    "   - The `assertSchemaEqual` statement from `pyspark.testing.utils` compares the **expected_schema** with **actual_schema**. If the values are not equal, an error will be raised.\n",
    "\n",
    "\n",
    "   Run the cell and confirm that no errors are returned. This shows that the `get_health_csv_schema` function is working as expected.\n",
    "\n",
    "   [Pytest Testing Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.testing.html#testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842542d9-b4ff-436a-9ce1-2fc07a037382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType, LongType\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "from pyspark.testing.utils import assertSchemaEqual\n",
    "\n",
    "def test_get_health_csv_schema_match():\n",
    "\n",
    "    # Get schema from our function\n",
    "    actual_schema = project_functions.get_health_csv_schema()\n",
    "    print(f\"Schema from function: {actual_schema}\")\n",
    "    # Define the expected schema that the function should return. If that function is changed during development the unit test will pick up the error and the test will fail.\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"ID\", IntegerType(), True),\n",
    "        StructField(\"PII\", StringType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"HighCholest\", IntegerType(), True),\n",
    "        StructField(\"HighBP\", DoubleType(), True),\n",
    "        StructField(\"BMI\", DoubleType(), True),\n",
    "        StructField(\"Age\", DoubleType(), True),\n",
    "        StructField(\"Education\", DoubleType(), True),\n",
    "        StructField(\"income\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # Assert the actual schema matches the expected schema\n",
    "    assertSchemaEqual(actual_schema, expected_schema)\n",
    "    print('Test passed!')\n",
    "\n",
    "test_get_health_csv_schema_match()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "569ee2f9-73ac-4505-9843-0e25aee5d3bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Create the test function `test_high_cholest_column_valid_map` to test the `highcholest_map` function from above. Within the test function:\n",
    "\n",
    "   - Import the `assertDataFrameEqual` function from the `pyspark.testing.utils` module to compare two PySpark DataFrames and assert that they are equal, checking both the data and schema.\n",
    "\n",
    "   - The variable **actual_df** holds the new column created from the result of the `highcholest_map` function.\n",
    "\n",
    "   - The variable **expected_df** contains the expected dataframe we anticipate from the function.\n",
    "\n",
    "   - The `assertDataFrameEqual` function from `pyspark.testing.utils` compares the dataframes **actual_df** and **expected_df**. If the data or schema are not equal, an error will be raised.\n",
    "\n",
    "   Run the cell and confirm that no errors are returned. This shows that the `highcholest_map` function is working as expected.\n",
    "\n",
    "   [Pytest Testing Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.testing.html#testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0390eb9e-f011-4b87-a634-4f6868cae72d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Import assertDataFrameEqual to compare your DataFrames\n",
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "\n",
    "def test_high_cholest_column_valid_map():\n",
    "\n",
    "    # Create the sample DataFrame to test the function\n",
    "    data = [\n",
    "        (0,),\n",
    "        (1,), \n",
    "        (2,), \n",
    "        (3,), \n",
    "        (4,), \n",
    "        (None,)\n",
    "    ]\n",
    "    sample_df = spark.createDataFrame(data, [\"value\"])  # \"value\" is the column name for the single column in the DataFrame created from 'data'\n",
    "    sample_df.display()\n",
    "    # Apply the function on the sample data\n",
    "    # Apply the high_cholest_map function from project_functions to the \"value\" column,\n",
    "    # creating a new column \"actual\" with the mapped results.\n",
    "    actual_df = sample_df.withColumn(\"actual\", project_functions.high_cholest_map(\"value\"))\n",
    "    actual_df.display()\n",
    "    # Create a static DataFrame with the expected results of the highcholest_map function above\n",
    "    expected_df = spark.createDataFrame(\n",
    "        [\n",
    "            (0, \"Normal\"),\n",
    "            (1, \"Above Average\"),\n",
    "            (2, \"High\"),\n",
    "            (3, \"Unknown\"),\n",
    "            (4, \"Unknown\"),\n",
    "            (None, \"Unknown\")\n",
    "        ],\n",
    "        schema=StructType([\n",
    "            StructField(\"value\", LongType(), True),\n",
    "            StructField(\"actual\", StringType(), True)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    ## Check to make sure the column in the sample dataframe and expected dataframe are the same. If not equal an error will be returned.\n",
    "    assertDataFrameEqual(actual_df, expected_df)\n",
    "    print('Test passed!')\n",
    "\n",
    "test_high_cholest_column_valid_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dc0390f-5c65-4bd3-bc93-457ca667f7c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. In cell below the **expected_df** variable has been modified from `(0, \"Normal\")` to `(0, \"Bad Value Cause Error\")`. This will cause the unit test to fail because the expected results were incorrectly specified and will not match the **actual_df**.\n",
    "\n",
    "    Run the unit test below. Notice that the test function returns an error because the dataframes don't match. \n",
    "\n",
    "**NOTE:** If your unit tests fail you will need to assess where the failure occurred and fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c9c3f2-1779-4332-b935-49d42621fdb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Cell causes an error because the actual df does not match the expected df\n",
    "################################################################################\n",
    "\n",
    "def test_high_cholest_column_invalid_map():\n",
    "\n",
    "    # Create the sample DataFrame to test the function on\n",
    "    data = [\n",
    "        (0,),\n",
    "        (1,), \n",
    "        (2,), \n",
    "        (3,), \n",
    "        (4,), \n",
    "        (None,)\n",
    "    ]\n",
    "    sample_df = spark.createDataFrame(data, [\"value\"])\n",
    "\n",
    "    # Apply the function on the sample data\n",
    "    actual_df = sample_df.withColumn(\"actual\", project_functions.high_cholest_map(\"value\"))\n",
    "\n",
    "    # Create a static DataFrame with the expected results of the highcholest_map function above\n",
    "    expected_df = spark.createDataFrame(\n",
    "        [\n",
    "            (0, \"Bad Value Cause Error\"),     ####### <--- Value has been changed to cause an error\n",
    "            (1, \"Above Average\"),\n",
    "            (2, \"High\"),\n",
    "            (3, \"Unknown\"),\n",
    "            (4, \"Unknown\"),\n",
    "            (None, \"Unknown\")\n",
    "        ],\n",
    "        schema=StructType([\n",
    "            StructField(\"value\", LongType(), True),\n",
    "            StructField(\"actual\", StringType(), True)\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    ## Check to make sure the column in the sample dataframe and expected dataframe are the same. If not equal an error will be returned.\n",
    "    assertDataFrameEqual(actual_df, expected_df)\n",
    "\n",
    "test_high_cholest_column_invalid_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c38ee57-1b5a-4b80-9940-9c6d660427f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Developing good unit tests are important because they help ensure individual components of your code work as expected, catching errors early in the development process. This leads to more reliable, maintainable code and can save time in debugging later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a76cbcb-5d07-4c72-904e-49cab81e4e13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Create a File for the Unit Tests\n",
    "\n",
    "In the previous cells, we implemented our unit tests within the Databricks notebook.\n",
    "\n",
    "Typically, unit tests are placed in a separate location, such as a **tests/unit_tests/tests_file_name.py** file (or in multiple Python files), to keep them separate from the main code and maintain a clean project structure. A testing framework is then used to execute the tests.\n",
    "\n",
    "This approach makes it easier to manage, update, and run tests without affecting the application’s core functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6747820b-b482-4a2f-acb8-c88775874b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Let's start by importing our testing framework package `pytest` on the cluster.\n",
    "\n",
    "    [pytest documentation](https://docs.pytest.org/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c323cf91-f3d1-4285-9b6d-431e36ebb85b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install pytest==8.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c977ad7-7e1e-46e9-ad51-02bd647c2fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Complete the following steps to view how to define the unit test in your Python file for this project. The unit test functions have already been placed in main course folder in the **tests/unit_tests/test_spark_helper_functions.py** file for you.\n",
    "\n",
    "    a. Navigate to the main course folder **DevOps Essentials for Data Engineering**.  \n",
    "\n",
    "    b. Open the **tests/unit_tests** folder.\n",
    "\n",
    "    c. Right-click on the **[tests/unit_tests/test_spark_helper_functions.py]($../../tests/unit_tests/test_spark_helper_functions.py)** file and select *Open in a new tab*. Review the file. \n",
    "\n",
    "    Notice the following:\n",
    "\n",
    "      - On line 13, our custom `project_functions` is being imported from the `src.helpers` module.\n",
    "\n",
    "         **NOTE:** We are defining the Python path setting for `pytest` within the **pytest.ini** file in the main course directory. This ensures that the specified directory is added to the Python module search path when `pytest` is run. This is useful for allowing `pytest` to import code from the project root or sibling directories without needing to manually modify `sys.path`. This makes the functions available for use in the current script.\n",
    "\n",
    "      - `@pytest.fixture` is a pytest fixture named **spark** with a session scope, which means it will be set up once per test session and shared across multiple test functions. It creates a SparkSession using `SparkSession.builder.getOrCreate()` (which either retrieves an existing session or creates a new one if none exists) and then yields the spark session to the test functions that use this fixture, allowing them to access the Spark environment for their tests.\n",
    "\n",
    "      - The remainder of the file creates the unit test functions: \n",
    "         - `test_get_health_csv_schema_match`\n",
    "\n",
    "         - `test_highcholest_column_map`\n",
    "\n",
    "         - `test_age_group_column_map`\n",
    "\n",
    "    d. After reviewing the file, close the tab.\n",
    "\n",
    "**NOTE:** The complexity and thoroughness of your tests depend on your organization's best practices and guidelines. This is a simple demonstration example. This course is simply introducing unit testing and will not go deep into `pytest` or any other testing framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b304477-95d6-46c4-ac6e-78cb681e2575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3.  In the next cell use `pytest` to execute all the tests (3 tests) within the **tests/unit_tests/test_spark_helper_functions.py** file.\n",
    "\n",
    "    To execute pytest within Databricks complete the following:\n",
    "\n",
    "    - `sys.dont_write_bytecode = True` - By default, Python generates .pyc bytecode files in a __pycache__ directory. Setting  it to `True` prevents this, which is useful in read-only environments.\n",
    "\n",
    "    - `pytest.main()` - This is the function that starts pytest and runs the tests.\n",
    "\n",
    "    - `./test_simple/test_simple_functions.py` - The path to the test file you want to run.\n",
    "\n",
    "    - `-v` - This stands for \"verbose\". It tells pytest to provide more detailed output during test execution, including the names of tests and their results.\n",
    "\n",
    "    - `-p no:cacheprovider\"` - This disables pytest's cache provider. pytest uses a caching mechanism to speed up repeated test runs, but in some cases, you may want to disable it. The no:cacheprovider option tells pytest to avoid using the cache, which may be useful if you’re running tests in an environment where caching could cause issues.\n",
    "\n",
    "    When executing pytest, it looks for:\n",
    "\n",
    "    - Test files: By default, files named **test_\\*.py** or **\\*_test.py**. In this example we are specifically testing the functions within the **tests/unit_tests/test_spark_helper_functions.py** file.\n",
    "\n",
    "    - Test functions: Functions starting with `test_*()`.\n",
    "\n",
    "    - For example, if you have a file like **test_example.py** with a function `test_addition()`, pytest will automatically detect and run the tests in that file.\n",
    "\n",
    "    Run the cell below and view the results.\n",
    "\n",
    "\n",
    "**NOTE:** Depending on your organization's best practices, you can run unit tests within Databricks or locally in the IDE of your choice based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd310825-b484-4081-a742-f46615df28f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## import the `pytest` and `sys` modules.\n",
    "import pytest\n",
    "import sys\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "\n",
    "retcode = pytest.main([\"../../tests/unit_tests/test_spark_helper_functions.py\", \"-v\", \"-p\", \"no:cacheprovider\"])\n",
    "\n",
    "# Fail the cell execution if there are any test failures.\n",
    "assert retcode == 0, \"The pytest invocation failed. See the log for details.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77495fc8-8069-4055-8492-5e92e634fc79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. View the output of the cell above. Notice that:\n",
    "\n",
    "    - `pytest` automatically discovered all the unit test functions within the **/tests/test_spark_helper_functions.py** file.\n",
    "\n",
    "    - The **test_spark_helper_functions.py** file contained three test functions: \n",
    "      - `test_get_health_csv_schema_match()`\n",
    "\n",
    "      - `test_highcholest_column_map()`\n",
    "\n",
    "      - `test_age_group_column_map()`\n",
    "\n",
    "    - All three unit tests passed.\n",
    "\n",
    "    For more information\n",
    "\n",
    "    [Pyspark Testing Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.testing.html#testing)\n",
    "\n",
    "    [Testing Pyspark Documentation](https://spark.apache.org/docs/latest/api/python/getting_started/testing_pyspark.html#Testing-PySpark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f37b93c-d099-4688-8875-b8dc85e7b743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "Unit testing is important because it helps ensure that individual parts of your code (like functions or methods) work correctly. It catches bugs early, improves code reliability, and makes it easier to maintain and refactor your code with confidence, knowing that existing functionality is still working as expected. You can perform your unit testing within Databricks or locally using the IDE of your choice.\n",
    "\n",
    "This demonstration was a simple, quick introduction to unit testing with the `pytest` framework. There are a variety of other frameworks available, and you must decide which one works best for your team. Also, it's important to decide on unit test best practices for your team and organization.\n",
    "\n",
    "#### Next Steps\n",
    "You can set up a continuous integration and continuous delivery or deployment (CI/CD) system, such as GitHub Actions, to automatically run your unit tests whenever your code changes. For an example, see the coverage of GitHub Actions in [Software engineering best practices for notebooks](https://docs.databricks.com/en/notebooks/best-practices.html). \n",
    "\n",
    "\n",
    "#### Additional Unit Testing Resources\n",
    "- [pytest](https://docs.pytest.org/en/stable/)\n",
    "- [chispa](https://github.com/MrPowers/chispa)\n",
    "- [nutter](https://github.com/microsoft/nutter)\n",
    "- [unittest](https://docs.python.org/3/library/unittest.html)\n",
    "- [Best Practices for Unit Testing PySpark](https://www.youtube.com/watch?v=TbWcCyP2MgE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d873965-a440-44b7-8615-d0cb97afafa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.4 - Creating and Executing Unit Tests",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
