{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3c1aa3d-c308-4696-bd90-17c815735a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Classroom-Setup-Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6a6d86-697e-4eb7-836c-31b79c9dbbc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Create the DA keys for the user's catalogs\n",
    "DA.create_DA_keys()\n",
    "\n",
    "## Display the course catalog and schema name for the user.\n",
    "DA.display_config_values(\n",
    "  [\n",
    "    ('DEV catalog reference: DA.catalog_dev', DA.catalog_dev),\n",
    "    ('STAGE catalog reference: DA.catalog_stage', DA.catalog_stage),\n",
    "    ('PROD catalog reference: DA.catalog_prod', DA.catalog_prod)\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e81c987-5bfe-4045-9371-223430e8970b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def obtain_pipeline_id_or_create_if_not_exists():\n",
    "    '''\n",
    "    Checks to see if the required Spark Declarative Pipeline is created from the previous demo.\n",
    "\n",
    "    If the job exists it returns the pipeline id.\n",
    "\n",
    "    If the job does not exist it creates the Spark Declarative Pipeline and returns the pipeline id.\n",
    "    '''\n",
    "    from databricks.sdk.service import pipelines\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    w = WorkspaceClient()\n",
    "\n",
    "    try:\n",
    "        # New way to declare the pipeline\n",
    "        pipeline = DeclarativePipelineCreator(\n",
    "                            pipeline_name=f\"sdk_health_etl_{DA.catalog_dev}\", \n",
    "                            catalog_name = DA.catalog_name,\n",
    "                            schema_name = 'default',\n",
    "                            root_path_folder_name='src',\n",
    "                            source_folder_names=[\n",
    "                                'src/sdp/**', \n",
    "                                'tests/integration_test/**'],\n",
    "                            configuration = {\n",
    "                                'target': 'dev',\n",
    "                                'raw_data_path':f'/Volumes/{DA.catalog_name}/default/health'\n",
    "                            })\n",
    "\n",
    "        # print(f'----- Pipeline sdk_health_etl_{DA.catalog_dev} not found for the demonstration -----')\n",
    "\n",
    "        pipeline.create_pipeline()\n",
    "        pipeline.start_pipeline()\n",
    "        return pipeline.pipeline_id\n",
    "    except:\n",
    "        for pipeline in w.pipelines.list_pipelines():\n",
    "            if pipeline.name == f\"sdk_health_etl_{DA.catalog_dev}\":\n",
    "                print('Pipeline found and pipeline id has been stored')\n",
    "                return pipeline.pipeline_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "937a849f-27ed-47a5-90d6-5d4b18be4bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_demo_5_job(my_pipeline_id, job_name):\n",
    "    import os\n",
    "    from databricks.sdk.service import jobs\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "\n",
    "    email_me = [DA.username] # Use DA object to get email \n",
    "    target_catalog = DA.catalog_name\n",
    "\n",
    "    ## Creating an instance of the WorkspaceClient class from the Databricks SDK\n",
    "    w = WorkspaceClient()\n",
    "\n",
    "    ## Error if the job name already exists\n",
    "    for job in w.jobs.list():\n",
    "        if job.settings.name == job_name:\n",
    "            test_job_name = False\n",
    "            assert test_job_name, f'You already have a job with the same name. Please manually delete the job {job_name}'\n",
    "\n",
    "\n",
    "    ## Store the path of the main course folder 2 folders back\n",
    "    current_path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "\n",
    "    ##\n",
    "    ## Create individual tasks\n",
    "    ##\n",
    "    ## Unit tests task\n",
    "    unit_tests_notebook_path = f'{current_path}/Run Unit Tests'\n",
    "    task_unit_tests = jobs.Task(task_key=\"Unit_Tests\",\n",
    "                                description=\"Execute unit tests for project.\",\n",
    "                                notebook_task=jobs.NotebookTask(notebook_path=unit_tests_notebook_path),\n",
    "                                timeout_seconds=0)\n",
    "\n",
    "\n",
    "    ## SDP Execution\n",
    "    task_sdp = jobs.Task(task_key=\"Health_ETL\",\n",
    "                        description=\"Spark Declarative Pipeline\",\n",
    "                        pipeline_task=jobs.PipelineTask(pipeline_id=my_pipeline_id, full_refresh=True),\n",
    "                        depends_on = [\n",
    "                                    jobs.TaskDependency(task_key=\"Unit_Tests\")\n",
    "                                ],\n",
    "                        timeout_seconds=0)\n",
    "\n",
    "\n",
    "\n",
    "    ## Data visualization task\n",
    "    visualization_notebook_path = f'{current_path}/src/Final Visualization'\n",
    "    task_visualization = jobs.Task(task_key=\"Visualization\",\n",
    "                                description=\"Final visualization for project.\",\n",
    "                                notebook_task=jobs.NotebookTask(\n",
    "                                    notebook_path=visualization_notebook_path,\n",
    "                                    base_parameters = {'catalog_name': DA.catalog_name} # Use DA object to get catalog name\n",
    "                                    ),\n",
    "                                depends_on = [\n",
    "                                    jobs.TaskDependency(task_key=\"Health_ETL\")\n",
    "                                ],\n",
    "                                timeout_seconds=0)\n",
    "\n",
    "\n",
    "\n",
    "    ##\n",
    "    ## Create Entire Job Using the Tasks Above\n",
    "    ##\n",
    "    created_job = w.jobs.create(\n",
    "            name=job_name,\n",
    "            description='Final Workflow SDK',\n",
    "            tasks=[\n",
    "                task_unit_tests,\n",
    "                task_visualization,\n",
    "                task_sdp\n",
    "                ],\n",
    "            parameters = [\n",
    "                    jobs.JobParameterDefinition(name='target', default='dev'),\n",
    "                    jobs.JobParameterDefinition(name='catalog_name', default=target_catalog)\n",
    "                ]\n",
    "            )\n",
    "    \n",
    "    print('Creating Workflow using the Databricks SDK for the demonstration.')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-3.1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
