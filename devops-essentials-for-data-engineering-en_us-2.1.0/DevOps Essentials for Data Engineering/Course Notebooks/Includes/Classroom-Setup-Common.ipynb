{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce49ecbf-430d-4332-86fd-93425165ffac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../Includes/_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddcc29db-33fb-4d93-958c-68c5fe87fd20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_DA_keys(self): \n",
    "    '''\n",
    "    Create the DA references to the dev, prod and stage catalogs for the user.\n",
    "    '''\n",
    "    print('Set DA dynamic references to the dev, stage and prod catalogs.\\n')\n",
    "    setattr(DA, f'catalog', f'{self.catalog_name}')\n",
    "    setattr(DA, f'catalog_dev', f'{self.catalog_name}_1_dev')\n",
    "    setattr(DA, f'catalog_stage', f'{self.catalog_name}_2_stage')\n",
    "    setattr(DA, f'catalog_prod', f'{self.catalog_name}_3_prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cdd700c-76d1-4080-95e6-47c98c198b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_volumes(self, in_catalog: str, in_schema: str, vol_names: list):\n",
    "    \"\"\"\n",
    "    Creates the listed volumes in the specified schema within the user's catalog.\n",
    "    \n",
    "    This function will check if the volumes already exists. If they don't exist the volumes will be created and a note returned.\n",
    "    \n",
    "    Args:\n",
    "    -------\n",
    "        - in_catalog (str): The catalog to create the volume in.\n",
    "        - in_schema (str): The schema to create the volumes in.\n",
    "        - vol_names (list): A list of strings representing the volumes to create. If one volume, create a list of a single volume.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "        Note in the log on the action(s) performed.\n",
    "  \n",
    "    Example:\n",
    "    -------\n",
    "        create_volumes(in_catalog=DA.catalog_dev_1, in_schema='default', vol_names=['health'])\n",
    "    \"\"\"\n",
    "    ## Store current volumes in a list\n",
    "    get_current_volumes = spark.sql(f'SHOW VOLUMES IN {in_catalog}.{in_schema}')\n",
    "    current_volumes = (get_current_volumes\n",
    "                       .select('volume_name')\n",
    "                       .toPandas()['volume_name']\n",
    "                       .tolist())\n",
    "\n",
    "    ## Check to see if the volume(s) are created. If not, test each volume and create.\n",
    "    if set(vol_names).issubset(current_volumes):\n",
    "        print(f'Volume check. Volume {vol_names} already exist in {in_catalog}.{in_schema}. No action taken')\n",
    "\n",
    "    for vol in vol_names:\n",
    "        if vol not in current_volumes:\n",
    "            spark.sql(f'CREATE VOLUME IF NOT EXISTS {in_catalog}.{in_schema}.{vol}')\n",
    "            print(f'Created the volume: {in_catalog}.{in_schema}.{vol}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edf5a939-ef65-4a0a-b330-28fe3ea0239f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_spark_data_frame_from_cdc(self, cdc_csv_file_path):\n",
    "    '''\n",
    "    Create the DataFrame used to create the CSV files for the course.\n",
    "    '''\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import rand, when, lit, to_date, monotonically_increasing_id, col, coalesce\n",
    "    from pyspark.sql.types import StringType\n",
    "    import uuid\n",
    "\n",
    "    ##\n",
    "    ## Generate a column with a unique id (using it as a 'fake' PII column)\n",
    "    ##\n",
    "\n",
    "    # Define a UDF to generate deterministic UUID based on the row index\n",
    "    def generate_deterministic_uuid(index):\n",
    "        # Use UUID5 or UUID3 based on a namespace and the row index as the name\n",
    "        return str(uuid.uuid5(uuid.NAMESPACE_DNS, str(index)))  # You can use uuid3 as well\n",
    "\n",
    "    # Register UDF\n",
    "    generate_deterministic_uuid_udf = udf(generate_deterministic_uuid, StringType())\n",
    "\n",
    "    ##\n",
    "    ## Create spark dataframe with required columns and values to save CSV files.\n",
    "    ##\n",
    "    sdf = (spark\n",
    "        .read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(cdc_csv_file_path)\n",
    "        .repartition(1)\n",
    "        .withColumn(\"ID\", monotonically_increasing_id())\n",
    "        .select(\"ID\",\n",
    "                generate_deterministic_uuid_udf(monotonically_increasing_id()).alias('PII'),\n",
    "                when(col(\"ID\") < 15121, \"2025-01-01\")\n",
    "                    .when(col(\"ID\") < 40351, \"2025-01-02\")\n",
    "                    .otherwise(\"2025-01-03\")\n",
    "                    .alias(\"date\"),\n",
    "                when(col(\"HighChol\") < .2, 0)  # when value is less than .2\n",
    "                    .when((col(\"HighChol\").cast('float') >= .2) & (col(\"HighChol\") < 1.03), 1) # when value is between .2 and 1.03 \n",
    "                    .otherwise(2)              # else when value is greater than or equal to 0.9\n",
    "                    .alias(\"HighCholest\"),\n",
    "                \"HighBP\", \n",
    "                \"BMI\",\n",
    "                \"Age\",\n",
    "                \"Education\",\n",
    "                \"income\"        \n",
    "        )\n",
    "    )\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84134c23-d1f0-42d9-8608-e74013dbcbd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def delete_source_files(self, source_files):\n",
    "        \"\"\"\n",
    "        Deletes all files in the specified source volume.\n",
    "\n",
    "        This function iterates through all the files in the given volume,\n",
    "        deletes them, and prints the name of each file being deleted.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        source_files : str, optional\n",
    "            The path to the volume containing the files to delete. \n",
    "            Use the {DA.paths.working_dir} to dynamically navigate to the user's volume location in dbacademy/ops/vocareumlab@name:\n",
    "                Example: DA.paths.working_dir = /Volumes/dbacademy/ops/vocareumlab@name\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        None\n",
    "            This function does not return any value. It performs file deletion as a side effect and prints all files that it deletes.\n",
    "\n",
    "        Example:\n",
    "        --------\n",
    "        delete_source_files(f'{DA.paths.working_dir}/pii/stream_source/user_reg')\n",
    "        \"\"\"\n",
    "        \n",
    "        import os\n",
    "        \n",
    "        print(f'\\nSearching for files in {source_files} volume to delete prior to creating files...')\n",
    "        if os.path.exists(source_files):\n",
    "            list_of_files = sorted(os.listdir(source_files))\n",
    "        else:\n",
    "            list_of_files = None\n",
    "\n",
    "        if not list_of_files:  # Checks if the list is empty.\n",
    "            print(f\"No files found in {source_files}.\\n\")\n",
    "        else:\n",
    "            for file in list_of_files:\n",
    "                file_to_delete = source_files + '/' + file\n",
    "                print(f'Deleting file: {file_to_delete}')\n",
    "                dbutils.fs.rm(file_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc7c3d8a-0ba1-461d-a680-789c56671db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DeclarativePipelineCreator:\n",
    "    \"\"\"\n",
    "    A class to create a Lakeflow Spark Declarative pipeline using the Databricks REST API.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    pipeline_name : str\n",
    "        Name of the pipeline to be created.\n",
    "    root_path_folder_name : str\n",
    "        The folder containing the pipeline code relative to current working directory.\n",
    "    source_folder_names : list\n",
    "        List of subfolders inside the root path containing source notebooks or scripts.\n",
    "    catalog_name : str\n",
    "        The catalog where the pipeline tables will be stored.\n",
    "    schema_name : str\n",
    "        The schema (aka database) under the catalog.\n",
    "    serverless : bool\n",
    "        Whether to use serverless compute.\n",
    "    configuration : dict\n",
    "        Optional key-value configurations passed to the pipeline.\n",
    "    continuous : bool\n",
    "        If True, enables continuous mode (streaming).\n",
    "    photon : bool\n",
    "        Whether to use Photon execution engine.\n",
    "    channel : str\n",
    "        The SDP release channel to use (e.g., \"PREVIEW\", \"CURRENT\").\n",
    "    development : bool\n",
    "        Whether to run the pipeline in development mode.\n",
    "    pipeline_type : str\n",
    "        Type of pipeline (e.g., 'WORKSPACE').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pipeline_name: str,\n",
    "                 root_path_folder_name: str,\n",
    "                 catalog_name: str,\n",
    "                 schema_name: str,\n",
    "                 source_folder_names: list = None,\n",
    "                 serverless: bool = True,\n",
    "                 configuration: dict = None,\n",
    "                 continuous: bool = False,\n",
    "                 photon: bool = True,\n",
    "                 channel: str = 'PREVIEW',\n",
    "                 development: bool = True,\n",
    "                 pipeline_type: str = 'WORKSPACE'):\n",
    "\n",
    "        # Assign all input arguments to instance attributes\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.root_path_folder_name = root_path_folder_name\n",
    "        self.source_folder_names = source_folder_names or []\n",
    "        self.catalog_name = catalog_name\n",
    "        self.schema_name = schema_name\n",
    "        self.serverless = serverless\n",
    "        self.configuration = configuration or {}\n",
    "        self.continuous = continuous\n",
    "        self.photon = photon\n",
    "        self.channel = channel\n",
    "        self.development = development\n",
    "        self.pipeline_type = pipeline_type\n",
    "\n",
    "        # Instantiate the WorkspaceClient to communicate with Databricks REST API\n",
    "        self.workspace = WorkspaceClient()\n",
    "        self.pipeline_body = {}\n",
    "\n",
    "    def _check_pipeline_exists(self):\n",
    "        \"\"\"\n",
    "        Checks if a pipeline with the same name already exists.\n",
    "        Raises:\n",
    "            ValueError if the pipeline already exists.\n",
    "        \"\"\"\n",
    "        for pipeline in self.workspace.pipelines.list_pipelines():\n",
    "            if pipeline.name == self.pipeline_name:\n",
    "                raise ValueError(\n",
    "                    f\"Lakeflow Spark Declarative Pipeline name '{self.pipeline_name}' already exists. \"\n",
    "                    \"Please delete the pipeline using the UI and rerun to recreate.\"\n",
    "                )\n",
    "\n",
    "    def _build_pipeline_body(self):\n",
    "        \"\"\"\n",
    "        Constructs the body of the pipeline creation request based on class attributes.\n",
    "        \"\"\"\n",
    "        # Get current working directory\n",
    "        cwd = os.getcwd()\n",
    "\n",
    "        # Source folder is not in current working directory, so go up two level (Only for this course)\n",
    "        main_course_folder = os.path.dirname(os.path.dirname(cwd))\n",
    "\n",
    "        # Create full path to root folder\n",
    "        root_path_folder = os.path.join('/', main_course_folder, self.root_path_folder_name)\n",
    "\n",
    "        # Convert source folder names into glob pattern paths for the SDP\n",
    "        source_paths = [os.path.join(main_course_folder, folder) for folder in self.source_folder_names]\n",
    "        libraries = [{'glob': {'include': path}} for path in source_paths]\n",
    "\n",
    "        # Build dictionary to be sent in the API request\n",
    "        self.pipeline_body = {\n",
    "            'name': self.pipeline_name,\n",
    "            'pipeline_type': self.pipeline_type,\n",
    "            'root_path': root_path_folder,\n",
    "            'libraries': libraries,\n",
    "            'catalog': self.catalog_name,\n",
    "            'schema': self.schema_name,\n",
    "            'serverless': self.serverless,\n",
    "            'configuration': self.configuration,\n",
    "            'continuous': self.continuous,\n",
    "            'photon': self.photon,\n",
    "            'channel': self.channel,\n",
    "            'development': self.development\n",
    "        }\n",
    "\n",
    "    def create_pipeline(self):\n",
    "        \"\"\"\n",
    "        Creates the pipeline on Databricks using the defined attributes.\n",
    "\n",
    "        Returns:\n",
    "            dict: The response from the Databricks API after creating the pipeline.\n",
    "        \"\"\"\n",
    "        # Check for name conflicts\n",
    "        self._check_pipeline_exists()\n",
    "\n",
    "        # Build the body of the API request and creates self.pipeline_body variable\n",
    "        self._build_pipeline_body()\n",
    "\n",
    "        # Display information to user\n",
    "        print(f\"Creating the Lakeflow Spark Declarative Pipeline '{self.pipeline_name}'...\")\n",
    "        print(f\"Root folder path: {self.pipeline_body['root_path']}\")\n",
    "        print(f\"Source folder path(s): {self.pipeline_body['libraries']}\")\n",
    "\n",
    "        # Make the API call\n",
    "        self.response = self.workspace.api_client.do('POST', '/api/2.0/pipelines', body=self.pipeline_body)\n",
    "\n",
    "        # Notify of completion\n",
    "        print(f\"\\nLakeflow Spark Declarative Pipeline Creation '{self.pipeline_name}' Complete!\")\n",
    "\n",
    "        return self.response\n",
    "\n",
    "    def get_pipeline_id(self):\n",
    "        \"\"\"\n",
    "        Returns the ID of the created pipeline.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'response'):\n",
    "            raise RuntimeError(\"Pipeline has not been created yet. Call create_pipeline() first.\")\n",
    "\n",
    "        return self.response.get(\"pipeline_id\")\n",
    "\n",
    "    def start_pipeline(self):\n",
    "        '''\n",
    "        Starts the pipeline using the attribute set from the generate_pipeline() method.\n",
    "        '''\n",
    "        print('Started the pipeline run. Navigate to Jobs and Pipelines to view the pipeline.')\n",
    "        self.workspace.pipelines.start_update(self.get_pipeline_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75da88f3-9adf-4b01-8bbd-0dede4cd65c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA = DBAcademyHelper()\n",
    "DA.init()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Classroom-Setup-Common",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
