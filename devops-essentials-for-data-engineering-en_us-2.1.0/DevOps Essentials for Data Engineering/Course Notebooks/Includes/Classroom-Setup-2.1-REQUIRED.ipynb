{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4feb67df-86eb-4ce8-ab1f-cff45c9ed9c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Classroom-Setup-Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e74721b-ab75-4938-9d33-3eaf3650d716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_catalogs(self, catalog_suffix: list):\n",
    "    '''\n",
    "    Tests to see if catalogs are created for the course. \n",
    "\n",
    "    If the catalogs do not exist in the environment it will creates the catalogs based on the user's catalog name + given \n",
    "    suffix and assigns them as attributes to the DA object.\n",
    "\n",
    "    Args:\n",
    "    -------\n",
    "        catalog_suffix (list): A list of strings representing suffixes to append \n",
    "                                to the base user catalog name to form the full catalog name.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        Log information:\n",
    "            - If catalog(s) do not exist, prints information on the catalogs it created.\n",
    "            - If catalog(s) exist, prints information that catalogs exist.\n",
    "\n",
    "    Example:\n",
    "    -------\n",
    "        # Example usage\n",
    "        catalog_suffixes = ['dev_1', 'prod_1', 'test_1']\n",
    "        create_catalogs(catalog_suffixes)\n",
    "\n",
    "        This will create the following catalogs:\n",
    "        - catalog_dev_1\n",
    "        - catalog_prod_1\n",
    "        - catalog_test_1\n",
    "\n",
    "        And the following attributes will be added to the DA object:\n",
    "        - DA.catalog_dev_1 = 'catalog_dev_1'\n",
    "        - DA.catalog_prod_1 = 'catalog_prod_1'\n",
    "        - DA.catalog_test_1 = 'catalog_test_1'\n",
    "    '''\n",
    "\n",
    "    ## Current catalogs\n",
    "    list_of_curr_catalogs = [catalog.name for catalog in spark.catalog.listCatalogs()]\n",
    "\n",
    "    ## Create catalog for the user for the course.\n",
    "    catalog_start = f'{DA.catalog_name}'\n",
    "    create_catalogs = [catalog_start + suffix for suffix in catalog_suffix]\n",
    "\n",
    "    ## Check if the required user catalogs exist in the workspace. If not create catalogs.\n",
    "    if set(create_catalogs).issubset(set(list_of_curr_catalogs)):\n",
    "        print('Required catalogs check. Ready for takeoff.\\n')\n",
    "    else:\n",
    "        ## Create catalog if not already exist and print response\n",
    "        for catalog_name in create_catalogs:\n",
    "            if catalog_name not in list_of_curr_catalogs:\n",
    "                create_catalog = spark.sql(f'CREATE CATALOG IF NOT EXISTS {catalog_name}')\n",
    "                print(f'Created catalog: {catalog_name}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315b1b9b-4c15-4765-9f11-249bcc4cd1c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_volumes(self, in_catalog: str, in_schema: str, vol_names: list):\n",
    "    \"\"\"\n",
    "    Creates the listed volumes in the specified schema within the user's catalog.\n",
    "    \n",
    "    This function will check if the volumes already exists. If they don't exist the volumes will be created and a note returned.\n",
    "    \n",
    "    Args:\n",
    "    -------\n",
    "        - in_catalog (str): The catalog to create the volume in.\n",
    "        - in_schema (str): The schema to create the volumes in.\n",
    "        - vol_names (list): A list of strings representing the volumes to create. If one volume, create a list of a single volume.\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "        Note in the log on the action(s) performed.\n",
    "  \n",
    "    Example:\n",
    "    -------\n",
    "        create_volumes(in_catalog=DA.catalog_dev_1, in_schema='default', vol_names=['health'])\n",
    "    \"\"\"\n",
    "    ## Store current volumes in a list\n",
    "    get_current_volumes = spark.sql(f'SHOW VOLUMES IN {in_catalog}.{in_schema}')\n",
    "    current_volumes = (get_current_volumes\n",
    "                       .select('volume_name')\n",
    "                       .toPandas()['volume_name']\n",
    "                       .tolist())\n",
    "\n",
    "    ## Check to see if the volume(s) are created. If not, test each volume and create.\n",
    "    if set(vol_names).issubset(current_volumes):\n",
    "        print(f'Volume check. Volume {vol_names} already exist in {in_catalog}.{in_schema}. No action taken')\n",
    "\n",
    "    for vol in vol_names:\n",
    "        if vol not in current_volumes:\n",
    "            spark.sql(f'CREATE VOLUME IF NOT EXISTS {in_catalog}.{in_schema}.{vol}')\n",
    "            print(f'Created the volume: {in_catalog}.{in_schema}.{vol}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36f39359-a6a8-41c3-98d5-ff1761083507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_spark_data_frame_from_cdc(self, cdc_csv_file_path):\n",
    "    '''\n",
    "    Create the DataFrame used to create the CSV files for the course.\n",
    "    '''\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import rand, when, lit, to_date, monotonically_increasing_id, col, coalesce\n",
    "    from pyspark.sql.types import StringType\n",
    "    import uuid\n",
    "\n",
    "    ##\n",
    "    ## Generate a column with a unique id (using it as a 'fake' PII column)\n",
    "    ##\n",
    "\n",
    "    # Define a UDF to generate deterministic UUID based on the row index\n",
    "    def generate_deterministic_uuid(index):\n",
    "        # Use UUID5 or UUID3 based on a namespace and the row index as the name\n",
    "        return str(uuid.uuid5(uuid.NAMESPACE_DNS, str(index)))  # You can use uuid3 as well\n",
    "\n",
    "    # Register UDF\n",
    "    generate_deterministic_uuid_udf = udf(generate_deterministic_uuid, StringType())\n",
    "\n",
    "    ##\n",
    "    ## Create spark dataframe with required columns and values to save CSV files.\n",
    "    ##\n",
    "    sdf = (spark\n",
    "        .read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(cdc_csv_file_path)\n",
    "        .repartition(1)\n",
    "        .withColumn(\"ID\", monotonically_increasing_id())\n",
    "        .select(\"ID\",\n",
    "                generate_deterministic_uuid_udf(monotonically_increasing_id()).alias('PII'),\n",
    "                when(col(\"ID\") < 15121, \"2025-01-01\")\n",
    "                    .when(col(\"ID\") < 40351, \"2025-01-02\")\n",
    "                    .otherwise(\"2025-01-03\")\n",
    "                    .alias(\"date\"),\n",
    "                when(col(\"HighChol\") < .2, 0)  # when value is less than .2\n",
    "                    .when((col(\"HighChol\").cast('float') >= .2) & (col(\"HighChol\") < 1.03), 1) # when value is between .2 and 1.03 \n",
    "                    .otherwise(2)              # else when value is greater than or equal to 0.9\n",
    "                    .alias(\"HighCholest\"),\n",
    "                \"HighBP\", \n",
    "                \"BMI\",\n",
    "                \"Age\",\n",
    "                \"Education\",\n",
    "                \"income\"        \n",
    "        )\n",
    "    )\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd44eec8-9848-40cb-9310-3f06e9954964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@DBAcademyHelper.add_method\n",
    "def create_course_csv_files(self,\n",
    "                            dataframe, \n",
    "                            prod_volume_write_path: str, \n",
    "                            stage_volume_write_path: str,\n",
    "                            dev_volume_write_path: str,\n",
    "                            main_volume_write_path: str,\n",
    "                            data_filter_conditions: list, \n",
    "                            data_name_append: str, \n",
    "                            del_files_first: bool = True):\n",
    "    '''\n",
    "    This method will first delete all files (by default) in the specified prod, stage and dev volume paths.\n",
    "\n",
    "    Then it will create the CSV files for the course in the following volumes:\n",
    "        - user user_catalog/volume/health/ 1 csv file with 15% of the rows\n",
    "        - user dev/volume/health/ 1 csv file with 15% of the rows\n",
    "        - user stage volume/volume/health/ 1 csv file with 40% of the rows\n",
    "        - user prod volume/volume/health/ 3 csv files all files\n",
    "    '''\n",
    "    from pyspark.sql.functions import rand, when, lit, to_date, monotonically_increasing_id, col, coalesce\n",
    "\n",
    "    print('\\n----Creating the CSV files in the necessary volumes for the course----\\n')\n",
    "\n",
    "    ##\n",
    "    ## Delete the files in the volume prior to creating the CSV files.\n",
    "    ##\n",
    "    if del_files_first == True:\n",
    "        print('---DELETE FILES IN THE HEALTH VOLUME WITHIN THE MAIN, DEV, STAGE, PROD CATALOGS----')\n",
    "        DA.delete_source_files(source_files = prod_volume_write_path)\n",
    "        DA.delete_source_files(source_files = stage_volume_write_path)\n",
    "        DA.delete_source_files(source_files = dev_volume_write_path)\n",
    "        DA.delete_source_files(source_files = main_volume_write_path)\n",
    "\n",
    "\n",
    "    ## Utility method to delete files that begin with an underscore when creating files in a volume from a spark data frame.\n",
    "    def util_del_files_with_underscore_in_volume(self, volume_path: str):\n",
    "        \"\"\"\n",
    "        Deletes files in the specified volume (directory) whose names start with an underscore ('_').\n",
    "\n",
    "        This method lists all files in the given volume directory, filters out the ones that start \n",
    "        with an underscore, and deletes them using the `dbutils.fs.rm()` function cleaning up a \n",
    "        volume when creating files from a spark data frame.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "            volume_path (str): The path to the volume (directory) where the files are located.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            Output information in the log.\n",
    "        \n",
    "        Example:\n",
    "        --------\n",
    "        >>> del_files_with_underscore_in_volume(\"/Volumes/inquisitive_layer_stage_2/default/health\")\n",
    "        \"\"\" \n",
    "        \n",
    "        ## List files in specified volume\n",
    "        files = dbutils.fs.ls(volume_path)\n",
    "\n",
    "        # Filter files that start with an underscore '_'\n",
    "        files_to_remove = [file.path for file in files if file.name.startswith('_')]\n",
    "        \n",
    "        ## If no files found, pass. Otherwise delete files.\n",
    "        if len(files_to_remove) == 0:\n",
    "            pass\n",
    "        else: \n",
    "            print(f'Deleting files with underscores in: {volume_path}:')\n",
    "            for file_path in files_to_remove:\n",
    "                dbutils.fs.rm(file_path)\n",
    "\n",
    "\n",
    "    ##\n",
    "    ## Utility method to renameCSV files based on filter conditions in the PROD catalog volume\n",
    "    ##\n",
    "    def util_rename_csv_file(self, volume_path: str, file_prefix: str, file_suffix: str):\n",
    "        \"\"\"\n",
    "        Renames CSV files in a specified volume path that start with 'part-' by appending \n",
    "        a date filter condition and a custom name suffix.\n",
    "\n",
    "        `<date_filter_condition>_<data_name_append>.csv`. The renaming is performed using the `os.rename` method.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        volume_path (str): The directory path where the CSV files are located.\n",
    "        date_filter_condition (str): A date or condition to be added at the beginning of the new file name.\n",
    "        data_name_append (str): A suffix to append to the new file name before the `.csv` extension.\n",
    "\n",
    "         Returns:\n",
    "        -------\n",
    "            Output information in the log.\n",
    "\n",
    "        Example:\n",
    "        --------\n",
    "        >>> rename_csv_file('/Volumes/inquisitive_layer_stage_2/default/health', '2025-01-01', 'sales_data')\n",
    "        \"\"\"\n",
    "        import os\n",
    "\n",
    "        output_files = os.listdir(volume_path)\n",
    "\n",
    "        for file in output_files:\n",
    "            if file.startswith('part-'):\n",
    "                file_to_rename = f'{volume_path}/{file}'\n",
    "                new_csv_file_name = f'{file_prefix}_{file_suffix}.csv'\n",
    "                rename_file_to = f'{volume_path}/{new_csv_file_name}'  \n",
    "\n",
    "                print(f'Renaming CSV file: {file_to_rename}')\n",
    "                print(f'New CSV file name: {rename_file_to}')\n",
    "                os.rename(file_to_rename, rename_file_to)\n",
    "                print(f'Created CSV file: {new_csv_file_name}')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "    ###\n",
    "    ### Create the prod csv files based on filter\n",
    "    ###\n",
    "    def create_prod_csv_files(self):\n",
    "        \"\"\"\n",
    "        Creates and writes CSV files for each filter condition in the provided list of filter conditions. All arguments\n",
    "        are obtained from the main outer method.\n",
    "\n",
    "        This function takes a DataFrame, filters it by each date condition in the `data_filter_conditions` list, \n",
    "        writes each filtered DataFrame as a single CSV file to the specified directory (`prod_volume_write_path`).\n",
    "        Each CSV file is written with headers and appended in case the directory already contains data.\n",
    "\n",
    "        Parameters (obtained from outer/main method):\n",
    "        ----------\n",
    "            - data_filter_conditions (list): A list of date filter conditions to filter the DataFrame by.\n",
    "            - dataframe (DataFrame): The Spark DataFrame to be filtered and written to CSV.\n",
    "            - prod_volume_write_path (str): The destination path where the CSV files will be written.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            Prints output in the log.\n",
    "\n",
    "        Example:\n",
    "        >>> create_prod_csv_files()\n",
    "        \"\"\"\n",
    "        print('-------------------------------------------------------')\n",
    "        print(f'--- Creating CSV files in PROD: {prod_volume_write_path}')\n",
    "        print('-------------------------------------------------------')\n",
    "        for filter in data_filter_conditions:\n",
    "            (dataframe\n",
    "             .filter(col(\"date\") == filter)\n",
    "             .repartition(1)   ## One CSV file per date\n",
    "             .write\n",
    "             .option(\"header\", \"true\")\n",
    "             .mode('append')\n",
    "             .csv(prod_volume_write_path)\n",
    "            )\n",
    "\n",
    "            ## Del files with underscores after writing the csv files\n",
    "            util_del_files_with_underscore_in_volume(self, prod_volume_write_path)\n",
    "\n",
    "            ## Rename file with the filter condition + data_name_append\n",
    "            util_rename_csv_file(self, prod_volume_write_path, file_prefix=filter, file_suffix=data_name_append)\n",
    "            \n",
    "\n",
    "        print(f'Finished creating CSV files in PROD: {prod_volume_write_path} \\n')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ###\n",
    "    ### Create the dev csv file\n",
    "    ###\n",
    "    def create_dev_csv_files(self, spec_volume_path):\n",
    "        \"\"\"\n",
    "        Creates and writes a CSV file that is a subset of the orginal with the dev and main catalogs volume.\n",
    "\n",
    "        Parameters (obtained from outer/main method):\n",
    "        ----------\n",
    "            - dataframe (DataFrame): The Spark DataFrame to be filtered and written to CSV.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            Prints output in the log.\n",
    "\n",
    "        Example:\n",
    "        >>> create_dev_csv_files(dataframe)\n",
    "        \"\"\"\n",
    "        print('-------------------------------------------------------')\n",
    "        print(f'--- Creating CSV files in DEV: {spec_volume_path}')\n",
    "        print('-------------------------------------------------------')\n",
    "        (dataframe\n",
    "         .filter(col(\"ID\") <= 7499)\n",
    "         .repartition(1)   ## One CSV file per date\n",
    "         .withColumn(\"PII\", lit(\"********\"))\n",
    "         .write\n",
    "         .option(\"header\", \"true\")\n",
    "         .mode('append')\n",
    "         .csv(spec_volume_path)\n",
    "        )\n",
    "\n",
    "        ## Del files with underscores after writing the csv files\n",
    "        util_del_files_with_underscore_in_volume(self, spec_volume_path)\n",
    "\n",
    "        ## Rename file with the filter condition + data_name_append\n",
    "        ## Rename file with the filter condition + data_name_append\n",
    "        util_rename_csv_file(self, spec_volume_path, file_prefix=\"dev\", file_suffix=data_name_append)\n",
    "\n",
    "        print(f'Finished creating CSV file in DEV: {spec_volume_path} \\n')\n",
    "\n",
    "\n",
    "\n",
    "    def create_stage_csv_files(self):\n",
    "        \"\"\"\n",
    "        Creates and writes a CSV file that is a subset of the orginal with the stage catalog\n",
    "\n",
    "        Parameters (obtained from outer/main method):\n",
    "        ----------\n",
    "            - dataframe (DataFrame): The Spark DataFrame to be filtered and written to CSV.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "            Prints output in the log.\n",
    "\n",
    "        Example:\n",
    "        >>> create_dev_csv_files(dataframe)\n",
    "        \"\"\"\n",
    "        print('-------------------------------------------------------')\n",
    "        print(f'--- Creating CSV files in STAGE: {stage_volume_write_path}')\n",
    "        print('-------------------------------------------------------')\n",
    "        (dataframe\n",
    "         .filter(col(\"ID\") <= 34999)\n",
    "         .repartition(1)   ## One CSV file per date\n",
    "         .write\n",
    "         .option(\"header\", \"true\")\n",
    "         .mode('append')\n",
    "         .csv(stage_volume_write_path)\n",
    "        )\n",
    "\n",
    "        ## Del files with underscores after writing the csv files\n",
    "        util_del_files_with_underscore_in_volume(self, stage_volume_write_path)\n",
    "\n",
    "        ## Rename file with the filter condition + data_name_append\n",
    "        ## Rename file with the filter condition + data_name_append\n",
    "        util_rename_csv_file(self, stage_volume_write_path, file_prefix=\"stage\", file_suffix=data_name_append)\n",
    "\n",
    "        print(f'Finished creating CSV file in STAGE: {stage_volume_write_path} \\n')\n",
    "\n",
    "\n",
    "\n",
    "    ## CREATE PROD CSV FILES FOR PROD DEV STAGE\n",
    "    create_dev_csv_files(self, spec_volume_path = dev_volume_write_path)\n",
    "    create_dev_csv_files(self, spec_volume_path = main_volume_write_path)\n",
    "    create_stage_csv_files(self)\n",
    "    create_prod_csv_files(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9700c04-f90b-47b0-954b-cea54e49e13e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Setup course environment\n",
    "list_of_catalog_suffixes = ['_1_dev', '_2_stage', '_3_prod']\n",
    "\n",
    "## Create user specific catalogs\n",
    "DA.create_catalogs(list_of_catalog_suffixes)\n",
    "\n",
    "## Create the DA keys for the user's catalogs\n",
    "DA.create_DA_keys()\n",
    "\n",
    "## Create the volumes in each catalog\n",
    "DA.create_volumes(in_catalog=DA.catalog_name, in_schema='default', vol_names=['health'])\n",
    "DA.create_volumes(in_catalog=DA.catalog_dev, in_schema='default', vol_names=['health'])\n",
    "DA.create_volumes(in_catalog=DA.catalog_stage, in_schema='default', vol_names=['health'])\n",
    "DA.create_volumes(in_catalog=DA.catalog_prod, in_schema='default', vol_names=['health'])\n",
    "\n",
    "\n",
    "##\n",
    "## Create the CSV files\n",
    "##\n",
    "\n",
    "## Create the Spark dataframe to use\n",
    "sdf = DA.create_spark_data_frame_from_cdc(\"/Volumes/dbacademy_cdc_diabetes/v01/cdc-diabetes/diabetes_binary_5050_raw.csv\")\n",
    "\n",
    "\n",
    "## Create the CSV files for the course.\n",
    "DA.create_course_csv_files(dataframe=sdf, \n",
    "                           prod_volume_write_path = f'/Volumes/{DA.catalog_prod}/default/health', \n",
    "                           stage_volume_write_path = f'/Volumes/{DA.catalog_stage}/default/health',\n",
    "                           dev_volume_write_path = f'/Volumes/{DA.catalog_dev}/default/health',\n",
    "                           main_volume_write_path = f'/Volumes/{DA.catalog_name}/default/health',\n",
    "                           data_filter_conditions = [\"2025-01-01\", \"2025-01-02\",\"2025-01-03\"], \n",
    "                           data_name_append = \"health\", \n",
    "                           del_files_first = True)\n",
    "\n",
    "\n",
    "## Setup Complete\n",
    "print('\\n\\n\\n------------------------------------------------------------------------------')\n",
    "print('COURSE SETUP COMPLETE!')\n",
    "print('------------------------------------------------------------------------------')\n",
    "\n",
    "## Display the course catalog and schema name for the user.\n",
    "DA.display_config_values(\n",
    "  [\n",
    "    ('Main catalog reference: DA.catalog_name', DA.catalog_name)\n",
    "   ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Classroom-Setup-2.1-REQUIRED",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
