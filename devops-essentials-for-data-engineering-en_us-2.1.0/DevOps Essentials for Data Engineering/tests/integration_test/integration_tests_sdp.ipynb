{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e58e287f-92a3-4851-a6b6-b3c8327226c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SDP Integration Tests Using Expectations\n",
    "This Spark Declarative Pipeline is a simple example pipeline that includes a few integration checks using expectations on a simple project to teach the basics. There are additional expectations you can set or unit tests you can build to streamline the project, but we're keeping it simple.\n",
    "\n",
    "Please check out the following resources for more information.\n",
    "\n",
    "- [Manage data quality with pipeline expectations](https://docs.databricks.com/aws/en/ldp/expectations#manage-data-quality-with-pipeline-expectations)\n",
    "\n",
    "- [Expectation recommendations and advanced patterns](https://docs.databricks.com/aws/en/ldp/expectation-patterns#expectation-recommendations-and-advanced-patterns)\n",
    "\n",
    "- [Applying software development & DevOps best practices to Spark Declarative Pipelines](https://www.databricks.com/blog/applying-software-development-devops-best-practices-delta-live-table-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0753bd4-b027-4971-a6b2-82e500c6dfa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Obtain Configuration Variable for the Target Environment\n",
    "This path will use the configuration variable set in the Spark Declarative Pipeline for **development, stage and production**.\n",
    "\n",
    "- If target is **development** or **stage** run all integration tests. \n",
    "- If target is **production**, only run the gold table integration test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60956f61-33d5-45c7-8c6d-ac8b00b263aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as dp\n",
    "\n",
    "## Store the target configuration environment in the variable targert\n",
    "target = spark.conf.get(\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d1292f-4112-4ed2-b0c8-fc7d6902fec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a Dictionary for Integration Test Values\n",
    "\n",
    "Create a dictionary containing the necessary values for integration tests in both **development** and **stage** environments. There are several approaches to achieve this, but this is a straightforward method.\n",
    "\n",
    "For more information, refer to the [Portable and Reusable Expectations](https://docs.databricks.com/en/delta-live-tables/expectation-patterns.html#portable-and-reusable-expectations) documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "496efa8f-99c2-4708-a34a-c6b43630eeff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Based on the deployed target, obtain the specific validation metrics for the tables.\n",
    "target_integration_tests_validation = {\n",
    "    'development': {\n",
    "        'health_bronze': {\n",
    "            'total_rows': 7500\n",
    "        },\n",
    "        'health_silver': {\n",
    "            'total_rows': 7500\n",
    "        }\n",
    "    },\n",
    "    'stage': {\n",
    "        'health_bronze': {\n",
    "            'total_rows': 35000\n",
    "        },\n",
    "        'health_silver': {\n",
    "            'total_rows': 35000\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "## Store the expected values for the total rows in the tables tables in the variables based on the target if in development or stage\n",
    "if target in ('development', 'stage'):\n",
    "    total_expected_bronze = target_integration_tests_validation[target]['health_bronze']['total_rows']\n",
    "    total_expected_silver = target_integration_tests_validation[target]['health_silver']['total_rows']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f623f7d-170a-4d5c-945a-8d2b24df13b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a Function to Count the Total Number of Rows in a Table\n",
    "The `test_count_table_total_rows` function creates a materialized view that counts the total number of rows in the specified table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cd2533c-8b13-497a-b8e0-55f6c9abe3c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_count_table_total_rows(table_name, total_count, target):\n",
    "    '''\n",
    "    Count the number of rows in the specified table and compare with the expected values for development and stage data. \n",
    "    Fail the update if the count does not match the specified values.\n",
    "    '''\n",
    "    @dp.table(\n",
    "        name=f\"TEST_{target}_{table_name}_total_rows_verification\",\n",
    "        comment=f\"Confirms all rows were ingested from the {target} raw data to {table_name}\"\n",
    "    )\n",
    "\n",
    "    @dp.expect_all_or_fail({\"valid count\": f\"total_rows = {total_count}\"}) \n",
    "\n",
    "    def count_table_total_rows():\n",
    "        return spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) AS total_rows FROM LIVE.{table_name}\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcd92c30-3d81-4939-9c16-8ca0ce4e9377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a Function to Confirm the Column Values in the Gold Materialized View\n",
    "The `test_gold_table_columns` function creates a materialized view that checks the values in the columns **Age_Group** and **HighCholest_Group** in **chol_age_agg**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbfa1d8f-b6db-44a4-b8ca-4cf6d5540451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_gold_table_columns():\n",
    "    '''\n",
    "    This function will check unique values in the columns Age_Group and HighCholest_Group in the gold table chol_age_agg.\n",
    "\n",
    "    This confirms that the distinct values for these columns in the gold table are correct.\n",
    "    ''' \n",
    "    ## Set expectations for the columns\n",
    "    check_silver_calc_columns = {\n",
    "        \"valid age group\": \"Age_Group in ('0-9', '10-19', '20-29', '30-39', '40-49', '50+', 'Unknown')\",\n",
    "        \"valid cholest group\": \"HighCholest_Group in ('Normal', 'Above Average', 'High', 'Unknown')\"\n",
    "    }\n",
    "\n",
    "    @dp.table(comment=\"Check age group and high cholest group in the gold table\")\n",
    "\n",
    "    ## Fail if expectations are not met\n",
    "    @dp.expect_all_or_fail(check_silver_calc_columns)\n",
    "\n",
    "    def test_calculated_columns_age_cholesterol():\n",
    "        return (dp\n",
    "                .read(\"chol_age_agg\")\n",
    "                .select(\"Age_Group\", \"HighCholest_Group\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72eb884d-05f5-482d-a222-08f5f20e45d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execute the Specified Integration Tests\n",
    "Execute the specified integration tests based on the target environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c1ef2b9-1c5b-4b8d-a508-1277ac88eb19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Run the specified tests based on the target environment (development, stage or production)\n",
    "\n",
    "if target in ('development','stage'):  ## Dynamic integration test for dev or stage tables\n",
    "    test_count_table_total_rows('health_bronze',  total_expected_bronze, target)\n",
    "    test_count_table_total_rows('health_silver',  total_expected_silver, target)\n",
    "    test_gold_table_columns()\n",
    "elif target == 'production':  ## Only test the gold table in production\n",
    "    test_gold_table_columns()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "integration_tests_sdp",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
