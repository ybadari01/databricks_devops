{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a312f11-c8b9-4183-b15e-f507fbee55b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet -U databricks-sdk==0.46.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb71f175-21f0-4c3e-ba94-7e27f48b2add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors.platform import NotFound\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "class NestedNamespace:\n",
    "\n",
    "    def __init__(self, dictionary: dict = None, prefix=None):\n",
    "        prefix = prefix + '.' if prefix else ''\n",
    "        self.__setattr_direct('dictionary', dictionary or dict())\n",
    "        self.__setattr_direct('prefix', prefix)\n",
    "        self.__setattr_direct('iterator', None)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        name = self.prefix + name\n",
    "        return self.dictionary.get(name, NestedNamespace(dictionary=self.dictionary, prefix=name))\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        name = self.prefix + name\n",
    "        self.dictionary[name] = value\n",
    "\n",
    "        # since we've overwritten the node in the tree, prune branch by deleting any children/ancestors\n",
    "        name += '.'\n",
    "        children = [k for k in filter(lambda x: x.startswith(name), self.dictionary.keys())]\n",
    "        for k in children:\n",
    "            del(self.dictionary[k])\n",
    "\n",
    "    # bypass overridden behaviour to directly set attributes\n",
    "    def __setattr_direct(self, name, value):\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        args = [f\"{key}='{self[key]}'\" for key in self]\n",
    "        return f\"{self.__class__.__name__} ({', '.join(args)})\" if args else \"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.__setattr_direct(\n",
    "            'iterator',\n",
    "            filter(\n",
    "                lambda x: x.startswith(self.prefix),\n",
    "                iter(self.dictionary)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.iterator).removeprefix(self.prefix) if self.iterator else None\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        return self.__getattr__(name)\n",
    "\n",
    "    def __setitem__(self, name, value):\n",
    "        return self.__setattr__(name, value)\n",
    "\n",
    "class DBAcademyHelper(NestedNamespace):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.workspace = WorkspaceClient()\n",
    "\n",
    "        try:\n",
    "            default_catalog = self.workspace.settings.default_namespace.get().namespace.value\n",
    "        except:\n",
    "            default_catalog = 'dbacademy'\n",
    "\n",
    "        meta = f'{default_catalog}.ops.meta'\n",
    "        catalog = None\n",
    "        schema = None\n",
    "\n",
    "        from py4j.protocol import Py4JJavaError\n",
    "        from pyspark.errors import PySparkException\n",
    "\n",
    "        try:\n",
    "            rows = spark.table(meta).collect()\n",
    "        except Py4JJavaError:\n",
    "            raise Exception(f'Error accessing metadata table {meta}; are you using serverless or DBR >= 15.1?')\n",
    "        except PySparkException:\n",
    "            raise Exception(f'Metadata table {meta} not found or accessible; are you running in a properly configured metastore?')\n",
    "\n",
    "        # query the metadata table and populate self with key/values\n",
    "        for row in rows:\n",
    "            setattr(self, row['key'], row['value'])\n",
    "\n",
    "            if row['key'] == 'catalog_name':\n",
    "                catalog = row['value']\n",
    "            elif row['key'] == 'schema_name':\n",
    "                schema = row['value']\n",
    "\n",
    "        # set default catalog and schema according to metadata\n",
    "        if catalog:\n",
    "            spark.sql(f'USE CATALOG {catalog}')\n",
    "\n",
    "            if schema:\n",
    "                spark.sql(f'USE SCHEMA {schema}')\n",
    "\n",
    "    # add an initializer. Initializers can be chained are are all called when DA.init() is called.\n",
    "    # This pattern makes it easier to dynamically augment the class across cells or notebooks.\n",
    "    # There's a couple ways to use this, but using as a function decorator is easiest:\n",
    "    #   @DBAcademyHelper.add_init\n",
    "    #   def init(self)\n",
    "    #       ...\n",
    "    # Alternatively:\n",
    "    #   def init(self):\n",
    "    #       ...\n",
    "    #   DBAcademyHelper.add_init(init)\n",
    "    #\n",
    "    # When DA.init() is called, all initializers are called in the order they were added\n",
    "\n",
    "    @classmethod\n",
    "    def add_init(cls, function_ref):\n",
    "        try:\n",
    "            initializers = getattr(cls, '_initializers')\n",
    "        except AttributeError:\n",
    "            initializers = list()\n",
    "\n",
    "        initializers += [function_ref]\n",
    "        setattr(cls, '_initializers', initializers)\n",
    "        return function_ref\n",
    "\n",
    "    # add a class method (aka \"monkey patch\"). This pattern makes it easier to dynamically augment the class\n",
    "    # across cells or notebooks.\n",
    "    # There's a couple ways to use this, but this is easiest:\n",
    "    #   @DBAcademyHelper.add_method\n",
    "    #   def method(self)\n",
    "    #       ...\n",
    "    # Alternatively:\n",
    "    #   def method(self):\n",
    "    #       ...\n",
    "    #   DBAcademyHelper.add_method(method)\n",
    "    #\n",
    "    # Ultimately, the new method can be called from within notebook code:\n",
    "    #   DA.method()\n",
    "    \n",
    "    @classmethod\n",
    "    def add_method(cls, function_ref):\n",
    "        setattr(cls, function_ref.__name__, function_ref)\n",
    "        return function_ref\n",
    "\n",
    "    def init(self):\n",
    "\n",
    "        for key in self:\n",
    "            value = self[key]\n",
    "\n",
    "            if value and type(value) == str:\n",
    "                try:\n",
    "                    spark.conf.set(f'DA.{key}', value)\n",
    "                    spark.conf.set(f'da.{key}', value)\n",
    "                except:\n",
    "                    # fails on serverless\n",
    "                    pass\n",
    "\n",
    "        try:\n",
    "            for i in getattr(self.__class__, '_initializers'):\n",
    "                i(self)\n",
    "\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def print_copyrights(self):\n",
    "        datasets = self.datasets\n",
    "\n",
    "        for i in datasets:\n",
    "            catalog = datasets[i].split('.')[0]\n",
    "            description = spark.sql(\n",
    "                f'DESCRIBE CATALOG {catalog}'\n",
    "            ).where(\n",
    "                F.col('info_name') == 'Comment'\n",
    "            ).select(\n",
    "                'info_value'\n",
    "            ).collect(\n",
    "            )[0]['info_value']\n",
    "            print(description)\n",
    "    \n",
    "    # Perform common lookups via the SDK. For example to find a structure's ID given a name. Example uses:\n",
    "    # DA.workspace_find(\"catalogs\", \"main\") -> return SDK structure representing catalog named \"main\"\n",
    "    # DA.workspace_find(\"cluster_policies\", \"DBAcademy DLT\") -> return structure representing named policy\n",
    "    # Note: for this to work, SDK must have an API named by \"item_type\" with a \"list\" api, and it assumes you\n",
    "    # want to look up based on a \"name\" element. But in cases all these conditions aren't true, you can use\n",
    "    # \"member\" and \"api\" to tweak behaviour without having to implement your own lookup function. Some examples:\n",
    "    # DA.workspace_find(\"clusters\", \"0913-023811-rzeq07rk\", \"cluster_id\") -> returns cluster structure with\n",
    "    # matching value of \"cluster_id\"\n",
    "    # DA. workspace_find('pipelines', pipeline_name, api='list_pipelines') -> returns structure representing\n",
    "    # the named DLT pipeline\n",
    "    def workspace_find(\n",
    "        self,\n",
    "        item_type: str,\n",
    "        value: str=None,\n",
    "        member: str='name',\n",
    "        api: str='list'\n",
    "    ):\n",
    "        # locate the API (item type), then grab the \"list\" method\n",
    "        method = getattr(getattr(self.workspace, item_type), api)\n",
    "\n",
    "        # iterate over the what the list() returned\n",
    "        for item in method():\n",
    "            if getattr(item, member) == value:\n",
    "                return item\n",
    "\n",
    "    def unique_name(self, sep: str) -> str:\n",
    "        return self.pseudonym.replace(' ', sep)\n",
    "\n",
    "\n",
    "    def display_config_values(self, config_values):\n",
    "        \"\"\"\n",
    "        Displays list of key-value pairs as rows of HTML text and textboxes\n",
    "        \n",
    "        param config_values: \n",
    "            list of (key, value) tuples\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        HTML output displaying the config values\n",
    "        Example\n",
    "        --------\n",
    "        DA.display_config_values([('catalog',DA.catalog_name),('schema',DA.schema_name)])\n",
    "        \"\"\"\n",
    "        html = \"\"\"<table style=\"width:100%\">\"\"\"\n",
    "        for name, value in config_values:\n",
    "            html += f\"\"\"\n",
    "            <tr>\n",
    "                <td style=\"white-space:nowrap; width:1em\">{name}:</td>\n",
    "                <td><input type=\"text\" value=\"{value}\" style=\"width: 100%\"></td></tr>\"\"\"\n",
    "        html += \"</table>\"\n",
    "        displayHTML(html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "_common",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
