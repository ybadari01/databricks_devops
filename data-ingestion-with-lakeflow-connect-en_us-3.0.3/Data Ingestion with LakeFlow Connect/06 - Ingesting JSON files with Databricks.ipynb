{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eb93b73-2ca1-4e5b-b907-e9a5dc0d37e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bf571dc-af5f-430c-9ebe-82c012192894",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6 - Ingesting JSON Files with Databricks\n",
    "\n",
    "In this demonstration, we’ll explore how to ingest JSON files and perform foundational JSON-specific transformations during ingestion, including decoding encoded fields and flattening nested JSON strings. We’ll be working with simulated Kafka event data, sourced from Databricks Marketplace.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Ingest raw JSON data into Unity Catalog using CTAS and `read_files()`.\n",
    "- Apply multiple techniques to flatten JSON string columns with and without converting to a STRUCT type.\n",
    "- Understand the difference between `explode()` and `explode_outer()`.\n",
    "- Introduce the capabilities and use cases of the VARIANT data type (public preview as of Q2-2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8399d71-76b4-48b4-a6e2-c789cf39a7cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default and you have a Shared SQL warehouse.\n",
    "\n",
    "<!-- ![Select Cluster](./Includes/images/selecting_cluster_info.png) -->\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe2977de-f863-411d-b7b2-39735e50e6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this notebook.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course in the lab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c0f2f1-f3cc-46fb-98f7-c6b30f04f881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cca66bac-9bda-48f8-a251-80591637903f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to view your default catalog and schema. Notice that your default catalog is **dbacademy** and your default schema is your unique **labuser** schema.\n",
    "\n",
    "**NOTE:** The default catalog and schema are pre-configured for you to avoid the need to specify the three-level name when writing your tables (i.e., catalog.schema.table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98aa6d6d-defa-45bd-a731-559beee7aad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01dba48b-fd98-4e17-a5ac-5d546d7d4d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Overview of CTAS with `read_files()` for Ingestion of JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b7744f6-e7fa-4002-88a0-57805785368c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B1. Inspect JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "377dc78b-248f-4987-9709-32f9748c8253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the next cell to verify that there are 11 JSON files located at `/Volumes/dbacademy_ecommerce/v01/raw/events-kafka`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bbfa1f-23f6-4c80-a255-2ec1cd034400",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List files in volume"
    }
   },
   "outputs": [],
   "source": [
    "LIST '/Volumes/dbacademy_ecommerce/v01/raw/events-kafka'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fa84850-5c55-46c1-8a1b-a9b8b462dab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the cell below to view the raw JSON data in the output. Note the following:\n",
    "\n",
    "   - Each row contains JSON with 6 key/value pairs.\n",
    "\n",
    "   - The **key** and **value** fields are encoded in base64. Base64 is an encoding scheme that converts binary data into a readable ASCII string.\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "**Example Output Formatted**\n",
    "```\n",
    "{\n",
    "    \"key\": \"VUEwMDAwMDAxMDczOTgwNTQ=\",\n",
    "    \"offset\": 219255030,\n",
    "    \"partition\": 0,\n",
    "    \"timestamp\": 1593880885085,\n",
    "    \"topic\": \"clickstream\",\n",
    "    \"value\": \"eyJkZXZpY2UiOiJBbmRyb2lkIiwiZWNvbW1lcmNlIjp7fSwiZXZlbnRfbmFtZSI6Im1haW4iLCJldmVudF90aW1lc3R\n",
    "    hbXAiOjE1OTM4ODA4ODUwMzYxMjksImdlbyI6eyJjaXR5IjoiTmV3IFlvcmsiLCJzdGF0ZSI6Ik5ZIn0sIml0ZW1zIjp\n",
    "    bXSwidHJhZmZpY19zb3VyY2UiOiJnb29nbGUiLCJ1c2VyX2ZpcnN0X3RvdWNoX3RpbWVzdGFtcCI6MTU5Mzg4MDg4NTA\n",
    "    zNjEyOSwidXNlcl9pZCI6IlVBMDAwMDAwMTA3Mzk4MDU0In0=\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126c241f-e71f-478d-a8fe-545316c9d9d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View JSON files as text"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM text.`/Volumes/dbacademy_ecommerce/v01/raw/events-kafka`\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a96199a-91f7-496d-81d7-b3be337b0334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the cell below to see how to use `read_files()` to read the JSON data. Notice the following:\n",
    "\n",
    "   - The JSON file is cleanly read into a tabular format with 6 columns.\n",
    "\n",
    "   - The **key** and **value** columns are base64-encoded and returned as STRING data type.\n",
    "   \n",
    "   - There are no rows in the **_rescued_data** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5660295d-14d6-414b-8ed1-d959424d409c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View JSON file in tabular form"
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM read_files(\n",
    "  \"/Volumes/dbacademy_ecommerce/v01/raw/events-kafka\",\n",
    "  format => \"json\"\n",
    ")\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d172485-cd91-4502-8d7b-88c675ba8213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Using CTAS and `read_files()` with JSON\n",
    "\n",
    "Ingesting JSON files using `read_files()` is as straightforward as reading CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2908db2-4b2b-47a1-b844-2814cf665d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to store this raw data in the **kafka_events_bronze_raw** table and view the table. When inspecting the results, you'll notice that:\n",
    "\n",
    "   - The **key** and **value** columns are of type STRING and contain data that is **base64-encoded**.\n",
    "\n",
    "   - This means the actual content has been encoded into base64 format and stored as a string. \n",
    "   \n",
    "   - They have not yet been transformed into a readable string in the first bronze table we create.\n",
    "\n",
    "**NOTE:** Base64 encoding is commonly used when ingesting data from sources like message queues or streaming platforms, where preserving formatting and avoiding data corruption is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27851d3-814c-4a97-b9c0-0740af5464e6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the bronze raw from JSON"
    }
   },
   "outputs": [],
   "source": [
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS kafka_events_bronze_raw;\n",
    "\n",
    "\n",
    "-- Create the Delta table\n",
    "CREATE TABLE kafka_events_bronze_raw AS\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "  \"/Volumes/dbacademy_ecommerce/v01/raw/events-kafka\",\n",
    "  format => \"json\"\n",
    ");\n",
    "\n",
    "\n",
    "-- Display the table\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_raw\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff6f818-6571-47cf-87bb-6bca3668ed97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B3. Decoding base64 Strings for the Bronze Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79986f07-a632-489b-8c6a-f4aa0e343b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Let's take a look at decoding the **key** and **value** columns by inspecting their data types after applying the `unbase64()` function. The `unbase64` function returns a decoded base64 string as binary.\n",
    "\n",
    "    - **encoded_key**: The original encoded **key** column as a base64 string.\n",
    "\n",
    "    - **decoded_key**: A new column created by decoding **key** from a base64 string to BINARY.\n",
    "\n",
    "    - **encoded_value**: The original encoded **value** column as a base64 string.\n",
    "\n",
    "    - **decoded_value**: A new column created by decoding **value** from a base64 string to BINARY.\n",
    "\n",
    "    Run the cell and view the results. Notice that the **decoded_key** and **decoded_value** columns are now BINARY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a38a2e8-4973-4510-869e-9c0872277fef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Decode the Base64 string as binary"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  key AS encoded_key,\n",
    "  unbase64(key) AS decoded_key,\n",
    "  value AS encoded_value,\n",
    "  unbase64(value) AS decoded_value\n",
    "FROM kafka_events_bronze_raw\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2856c3f2-9320-439f-ac09-cf7402f2a6e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Run the next cell to convert the BINARY columns to STRING columns using the `CAST` function. Notice the following in the results:\n",
    "\n",
    "    - The **decoded_key** and **decoded_value** columns are now of type STRING and readable.\n",
    "\n",
    "    - The **decoded_value** column is a JSON-formatted string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e677975-2524-4d45-8fa0-e4c72de24aa9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cast the BINARY as a STRING with CAST"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  key AS encoded_key,\n",
    "  cast(unbase64(key) AS STRING) AS decoded_key,\n",
    "  value AS encoded_value,\n",
    "  cast(unbase64(value) AS STRING) AS decoded_value\n",
    "FROM kafka_events_bronze_raw\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28f27100-950a-4d45-b0a8-27b6daf920b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Now, let's put it all together to create another bronze-level table named **kafka_events_bronze_decoded**. This table will store the STRING values for the **key** and **value** columns from the original **kafka_events_bronze_raw** table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58cc84c3-c3e6-44e8-8c9d-bc65a4db4c79",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the bronze_decoded table"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE kafka_events_bronze_decoded AS\n",
    "SELECT\n",
    "  cast(unbase64(key) AS STRING) AS decoded_key,\n",
    "  offset,\n",
    "  partition,\n",
    "  timestamp,\n",
    "  topic,\n",
    "  cast(unbase64(value) AS STRING) AS decoded_value\n",
    "FROM kafka_events_bronze_raw;\n",
    "\n",
    "\n",
    "-- View the new table\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_decoded\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6663297c-7e2e-46b2-ac01-a8be4967f7ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Working with JSON Formatted Strings in a Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0cf4f61-53f1-4a93-b07b-108417131c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Flattening JSON String Columns\n",
    "\n",
    "Next, we will explore how extract a column from a column containing a JSON formatted string. \n",
    "\n",
    "\n",
    "**BENEFITS**\n",
    "- **Simple** - Easy to implement and store JSON as plain text.\n",
    "- **Flexible** - Can hold any JSON structure without schema constraints.\n",
    "\n",
    "**CONSIDERATIONS**\n",
    "- **Performance** - STRING columns are slower when querying and processing complex data.\n",
    "- **No Schema** - The lack of a defined schema for STRING columns can lead to data integrity issues.\n",
    "- **Complex to Query** - Requires additional code to parse and retrieve data, which can be complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76024e8b-ea88-4cb8-85be-9447759e3a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C1.1 Query JSON strings\n",
    "\n",
    "You can extract a column from fields containing JSON strings using the syntax: `<column-name>:<extraction-path>`, where `<column-name>` is the string column name and `<extraction-path>` is the path to the field to extract. The returned results are strings. You can also do this with nested fields by using either `.` or `[]`.\n",
    "\n",
    "This utilizes Spark SQL's built-in functionality to interact directly with nested data stored as JSON strings.\n",
    "\n",
    "[Query JSON strings](https://docs.databricks.com/aws/en/semi-structured/json)\n",
    "\n",
    "\n",
    "Example JSON string pulled from a row in the column **decoded_value**:\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"device\": \"iOS\",\n",
    "    \"ecommerce\": {},\n",
    "    \"event_name\": \"add_item\",\n",
    "    \"event_previous_timestamp\": 1593880300696751,\n",
    "    \"event_timestamp\": 1593880892251310,\n",
    "    \"geo\": {\n",
    "      \"city\": \"Westbrook\", \n",
    "      \"state\": \"ME\"\n",
    "      },\n",
    "    \"items\": [\n",
    "        {\n",
    "            \"item_id\": \"M_STAN_T\",\n",
    "            \"item_name\": \"Standard Twin Mattress\",\n",
    "            \"item_revenue_in_usd\": 595.0,\n",
    "            \"price_in_usd\": 595.0,\n",
    "            \"quantity\": 1,\n",
    "        }\n",
    "    ],\n",
    "    \"traffic_source\": \"google\",\n",
    "    \"user_first_touch_timestamp\": 1593880300696751,\n",
    "    \"user_id\": \"UA000000107392458\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b36b6f3a-51d5-41c0-a774-8463fb7ccc78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. For example, let's extract the following values from the JSON-formatted string:\n",
    "    - `decoded_value:device`\n",
    "    - `decoded_value:traffic_source`\n",
    "    - `decoded_value:geo`\n",
    "    - `decoded_value:items`\n",
    "\n",
    "    Run the cell and view the results. Notice that we have successfully extracted the values from the JSON formatted string.\n",
    "\n",
    "    - **device** is a STRING\n",
    "\n",
    "    - **traffic_source** is a STRING\n",
    "\n",
    "    - **geo** is a STRING containing another JSON formatted string\n",
    "    \n",
    "    - **item** is a STRING contain an array of JSON formatted strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4aac9dc-a493-442e-b724-8c2bbb97f3df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query a JSON string and extract values"
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "  decoded_value,\n",
    "  decoded_value:device,\n",
    "  decoded_value:traffic_source,\n",
    "  decoded_value:geo,       ----- Contains another JSON formatted string\n",
    "  decoded_value:items      ----- Contains a nested-array of JSON formatted strings\n",
    "FROM kafka_events_bronze_decoded\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74e1e05c-8c3a-44d0-b067-266b49889740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. We can then begin to parse out the necessary JSON formatted string values to create another bronze table to flatten the JSON formatted string column for downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2a5c02-ece2-4f57-80b6-a88e554b532f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Flatten the JSON formatted string"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE kafka_events_bronze_string_flattened AS\n",
    "SELECT\n",
    "  decoded_key,\n",
    "  offset,\n",
    "  partition,\n",
    "  timestamp,\n",
    "  topic,\n",
    "  decoded_value:device,\n",
    "  decoded_value:traffic_source,\n",
    "  decoded_value:geo,       ----- Contains another JSON formatted string\n",
    "  decoded_value:items      ----- Contains a nested-array of JSON formatted strings\n",
    "FROM kafka_events_bronze_decoded;\n",
    "\n",
    "\n",
    "-- Display the table\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_string_flattened;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2aec192-c438-4d77-9148-8686239b45ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Flattening JSON Formatting Strings via STRUCT Conversion\n",
    "\n",
    "Similar to the previous section, we will discuss how to flatten our JSON STRING column **decoded_value** using a STRUCT column.\n",
    "\n",
    "#### Benefits and Considerations of STRUCT Columns\n",
    "\n",
    "**Benefits**\n",
    "- **Schema Enforcement** – STRUCT columns define and enforce a schema, helping maintain data integrity.\n",
    "- **Improved Performance** – STRUCTs are generally more efficient for querying and processing than plain strings.\n",
    "\n",
    "**Considerations**\n",
    "- **Schema Enforcement** – Because the schema is enforced, issues can arise if the JSON structure changes over time.\n",
    "- **Reduced Flexibility** – The data must consistently match the defined schema, leaving less room for structural variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6133f0d9-a38b-425b-9b84-d5c2d66d0ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C2.1 Converting a JSON STRING to a STRUCT Column\n",
    "To convert a JSON-formatted STRING column to a STRUCT column, you will need to derive the schema of the JSON-formatted string and then parse each row into a STRUCT type.\n",
    "\n",
    "We can do this in two steps.\n",
    "  1. Get the STRUCT type of the JSON formatted string.\n",
    "  2. Apply the STRUCT to the JSON formatted string column.\n",
    "\n",
    "**NOTE:** We have already copied and pasted the correct values for you as a part of this demonstration. The subsequent cell below is a copy and paste of the output of the single row that appears when running the next cell. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8c7c6fa-c66c-4977-8230-2385807edcf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Determine the derived schema using the [**`schema_of_json()`**](https://docs.databricks.com/en/sql/language-manual/functions/schema_of_json.html) function, which returns the schema inferred from a JSON-formatted string.\n",
    "\n",
    "    Run the cell and view the results. Notice that the output displays the structure of the JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae8f0c04-49cf-497a-a226-8c6df9145b14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Determine the schema of the JSON formatted string"
    }
   },
   "outputs": [],
   "source": [
    "SELECT schema_of_json('{\"device\":\"Linux\",\"ecommerce\":{\"purchase_revenue_in_usd\":1075.5,\"total_item_quantity\":1,\"unique_items\":1},\"event_name\":\"finalize\",\"event_previous_timestamp\":1593879231210816,\"event_timestamp\":1593879335779563,\"geo\":{\"city\":\"Houston\",\"state\":\"TX\"},\"items\":[{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_K\",\"item_name\":\"Standard King Mattress\",\"item_revenue_in_usd\":1075.5,\"price_in_usd\":1195.0,\"quantity\":1}],\"traffic_source\":\"email\",\"user_first_touch_timestamp\":1593454417513109,\"user_id\":\"UA000000106116176\"}')\n",
    "AS schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b22b5cd3-47c3-4b0a-a28d-ae92ff865810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Copy and paste the output from `schema_of_json` into the [**`from_json()`**](https://docs.databricks.com/en/sql/language-manual/functions/from_json.html) function. This function parses a column containing a JSON-formatted string into a STRUCT type using the specified schema, and creates a new table named **kafka_events_bronze_struct**.\n",
    "\n",
    "    Run the cell and view the results. Notice that the **value** column has been transformed into a nested STRUCT that includes scalar fields, nested structs, and an array of structs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4156ccbe-923d-4e9c-a601-84437171c5a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE kafka_events_bronze_struct AS\n",
    "SELECT \n",
    "  * EXCEPT (decoded_value),\n",
    "  from_json(\n",
    "      decoded_value,    -- JSON formatted string column\n",
    "      'STRUCT<device: STRING, ecommerce: STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name: STRING, event_previous_timestamp: BIGINT, event_timestamp: BIGINT, geo: STRUCT<city: STRING, state: STRING>, items: ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source: STRING, user_first_touch_timestamp: BIGINT, user_id: STRING>') AS value\n",
    "FROM kafka_events_bronze_decoded;\n",
    "\n",
    "\n",
    "-- View the new table.\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_struct\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a4f8318-7912-4dcf-9433-6c626db0f832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C2.2 Extract fields, nested fields, and nested arrays from STRUCT columns\n",
    "\n",
    "We can query the STRUCT column using `value.device` or `value.ecommerce` in our SELECT statement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8b930b2-9048-49dd-a05c-c0517a0c50f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Using this syntax, we can obtain values from the **value** struct column. Run the cell and view the results. Notice the following:\n",
    "\n",
    "    - We obtained values from the STRUCT column for **device** and **city**\n",
    "    \n",
    "    - The **items** column contains an ARRAY of STRUCTS. The number of elements in the array varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5741b0ad-6fa9-48c1-8a9d-9d152bd3510f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Obtain values from a STRUCT column"
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "  decoded_key,\n",
    "  value.device as device,  -- <----- Field\n",
    "  value.geo.city as city,  -- <----- Nested-field from geo field\n",
    "  value.items as items,\n",
    "  array_size(items) AS number_elements_in_array -- <----- Count the number of elements in the array column items\n",
    "FROM kafka_events_bronze_struct\n",
    "ORDER BY number_elements_in_array DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccbaec13-8430-4592-b1c5-911c479c894d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### C2.3 Explode Arrays\n",
    "\n",
    "Exploding an array transforms each element of an array column into a separate row, effectively flattening the array. There are a few things to keep in mind when using this function. \n",
    "\n",
    "1. It returns a set of rows composed of the elements of the array or the keys and values of the map.\n",
    "\n",
    "1. If the array is `NULL` no rows are produced. To return a single row with `NULL`s for the array or map values use the [`explode_outer()`](https://docs.databricks.com/gcp/en/sql/language-manual/functions/explode_outer) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c57b9cca-9d28-4a4c-ab04-f3072abdcc6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell to see how the ARRAY of values in the `value.items` explodes the array into one row for each element in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54dfaf76-e636-4842-bed6-4459c493c56e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Explore the array to one row per element"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE bronze_explode_array AS\n",
    "SELECT\n",
    "  decoded_key,\n",
    "  array_size(value.items) AS number_elements_in_array,\n",
    "  explode(value.items) AS item_in_array,\n",
    "  value.items\n",
    "FROM kafka_events_bronze_struct\n",
    "ORDER BY number_elements_in_array DESC;\n",
    "\n",
    "\n",
    "-- Display table\n",
    "SELECT *\n",
    "FROM bronze_explode_array;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c817f7f-f250-4533-a2bb-61348e51ae51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Working with a VARIANT Column (Public Preview)\n",
    "\n",
    "#### VARIANT Column Benefits and Considerations:\n",
    "\n",
    "**BENEFITS**\n",
    "- **Open** - Fully open-sourced, no proprietary data lock-in.\n",
    "- **Flexible** - No strict schema. You can put any type of semi-structured data into VARIANT.\n",
    "- **Performant** - Improved performance over existing methods.\n",
    "\n",
    "**CONSIDERATIONS**\n",
    "- Currently in public preview as of 2025 Q2.\n",
    "- [Variant support in Delta Lake](https://docs.databricks.com/aws/en/delta/variant)\n",
    "\n",
    "**RESOURCES**:\n",
    "- [Introducing the Open Variant Data Type in Delta Lake and Apache Spark](https://www.databricks.com/blog/introducing-open-variant-data-type-delta-lake-and-apache-spark)\n",
    "- [Say goodbye to messy JSON headaches with VARIANT](https://www.youtube.com/watch?v=fWdxF7nL3YI)\n",
    "- [Variant Data Type - Making Semi-Structured Data Fast and Simple](https://www.youtube.com/watch?v=jtjOfggD4YY)\n",
    "\n",
    "\n",
    "**NOTE:** Variant data type will not work on Serverless Version 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0a0d602-43e8-45bd-9bbd-b1101cb605c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. View the **kafka_events_bronze_decoded** table. Confirm the **decoded_value** column contains a JSON formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ec8029b-212f-4080-a320-7e5e54482c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM kafka_events_bronze_decoded\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61f29c61-92d5-4100-b7a3-909bba6ff157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Use the [`parse_json`](https://docs.databricks.com/aws/en/sql/language-manual/functions/parse_json) function to returns a VARIANT value from the JSON formatted string.\n",
    "\n",
    "    Run the cell and view the results. Notice that the **json_variant_value** column is of type VARIANT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b5f5d7-708e-486f-80aa-17a1f03ac30f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a VARIANT column"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE kafka_events_bronze_variant AS\n",
    "SELECT\n",
    "  decoded_key,\n",
    "  offset,\n",
    "  partition,\n",
    "  timestamp,\n",
    "  topic,\n",
    "  parse_json(decoded_value) AS json_variant_value   -- Convert the decoded_value column to a variant data type\n",
    "FROM kafka_events_bronze_decoded;\n",
    "\n",
    "-- View the table\n",
    "SELECT *\n",
    "FROM kafka_events_bronze_variant\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "675e14ec-bfc6-49a9-9933-88088b6d2f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. You can parse the VARIANT data type column using `:` to create your desired table.\n",
    "\n",
    "    [VARIANT type](https://docs.databricks.com/aws/en/sql/language-manual/data-types/variant-type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ee6f41c-4d89-441e-ad9d-2531876b9e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  json_variant_value,\n",
    "  json_variant_value:device :: STRING,  -- Obtain the value of device and cast to a string\n",
    "  json_variant_value:items\n",
    "FROM kafka_events_bronze_variant\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e00350bf-ce60-4a01-852d-d03063783ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7280327450620919,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06 - Ingesting JSON files with Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
