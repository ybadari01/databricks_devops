{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd369c8f-2430-4f16-84c5-a4d938466e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e32dcf47-7356-430c-815c-ccc6c5b80fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4 - Handling CSV Ingestion with the Rescued Data Column\n",
    "\n",
    "In this demonstration, we will focus on ingesting CSV files into Delta Lake using the `CTAS` (`CREATE TABLE AS SELECT`) pattern with the `read_files()` method and exploring the rescued data column. \n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you will be able to:\n",
    "\n",
    "- Ingest CSV files as Delta tables using the `CREATE TABLE AS SELECT` (CTAS) statement with the `read_files()` function.\n",
    "- Define and apply an explicit schema with `read_files()` to ensure consistent and reliable data ingestion.\n",
    "- Handle and inspect rescued data that does not conform to the defined schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5e48d0-3f2a-4420-8331-95f53ac73237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default and you have a Shared SQL warehouse.\n",
    "\n",
    "<!-- ![Select Cluster](./Includes/images/selecting_cluster_info.png) -->\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16c025a4-4a4b-438e-8c14-f5e1f480fd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this notebook.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course in the lab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7878a2ef-d4d2-4b9e-b7bb-b951ed8e8037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "891809e3-55e7-4a22-a5e9-2fa4c185962d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to view your default catalog and schema. Notice that your default catalog is **dbacademy** and your default schema is your unique **labuser** schema.\n",
    "\n",
    "**NOTE:** The default catalog and schema are pre-configured for you to avoid the need to specify the three-level name when writing your tables (i.e., catalog.schema.table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdf533f-a66a-4604-8fde-d2517a83ccf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "664654f5-c82e-46b4-892d-8ff9ebfbbe2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Overview of CTAS with `read_files()` for Ingestion of CSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) files are a simple text-based format for storing data, where each line represents a row and values are separated by commas.\n",
    "\n",
    "In this demonstration, we will use CSV files imported from cloud storage. Let’s explore how to ingest these raw CSV files to Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c58daa33-d37c-43e0-9172-001f13822acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B1. Inspecting CSV files\n",
    "\n",
    "1. List available CSV files from `dbacademy_ecommerce/v01/raw/sales-csv` directory. Confirm that 4 CSV files exist in the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56e19a80-d596-4dfc-b584-07083bcbb969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LIST '/Volumes/dbacademy_ecommerce/v01/raw/sales-csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f7556e2-e86e-4886-a698-417c96ef5906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Query the CSV files by path in the `/Volumes/dbacademy_ecommerce/v01/raw/sales-csv`volume directly and view the results. Notice the following:\n",
    "\n",
    "   - The data files include a header row containing the column names.\n",
    "\n",
    "   - The columns are delimited by the pipe character (`|`). \n",
    "\n",
    "     For example, the first row reads:  \n",
    "     ```order_id|email|transactions_timestamp|total_item_quantity|purchase_revenue_in_usd|unique_items|items```\n",
    "\n",
    "     The pipe (`|`) indicates column separation, meaning there are seven columns:  \n",
    "     **order_id**, **email**, **transactions_timestamp**, **total_item_quantity**, **purchase_revenue_in_usd**, **unique_items**, and **items**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06bbddcf-0d5a-422b-b9a0-0918b53df6fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query raw CSV files"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM csv.`/Volumes/dbacademy_ecommerce/v01/raw/sales-csv`\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e33f52db-1a6c-43c8-802e-7df187d0692f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Run the cell below to query the CSV files using the default options in the `read_files` function.\n",
    "\n",
    "   Review the results. Notice that the CSV files were **not** queried correctly in the table output.\n",
    "\n",
    "   To fix this, we’ll need to provide additional options to the `read_files()` function for proper ingestion of CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa7e4f05-b9e5-4b8e-851f-8190ad4963b6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read CSV files with default options in read_files"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM read_files(\n",
    "        \"/Volumes/dbacademy_ecommerce/v01/raw/sales-csv\",\n",
    "        format => \"csv\"\n",
    "      )\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb23bd25-c3e5-46db-a410-94e3e5329288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B2. Using CSV Options with `read_files()`\n",
    "\n",
    "1. The code in the next cell ingests the CSV files using the `read_files()` function with some additional options.\n",
    "\n",
    "   In this example, we are using the following options with the `read_files()` function:    [CSV options](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files#csv-options)\n",
    "\n",
    "   - The first argument specifies the path to the CSV files.\n",
    "\n",
    "   - `format => \"csv\"` — Indicates that the files are in CSV format.\n",
    "\n",
    "   - `sep => \"|\"` — Specifies that columns are delimited by the pipe (`|`) character.\n",
    "\n",
    "   - `header => true` — Tells the reader to use the first row as column headers.\n",
    "   \n",
    "   - Although we're using CSV files in this demonstration, other file types (like JSON or Parquet) can also be used by specifying different options.\n",
    "\n",
    "   Run the cell and view the results. Notice the CSV files were read correctly, and a new column named **_rescued_data** appeared at the end of the result table.\n",
    "\n",
    "**NOTE:** A **_rescued_data** column is automatically included to capture any data that doesn't match the inferred or provided schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e48e522-4721-4dc9-ac25-3cc4938c4247",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Specify CSV options in read_files"
    }
   },
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM read_files(\n",
    "        \"/Volumes/dbacademy_ecommerce/v01/raw/sales-csv\",\n",
    "        format => \"csv\",\n",
    "        sep => \"|\",\n",
    "        header => true\n",
    "      )\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b824fac-feb3-4e7c-bfee-da9a231b1c88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Now that we’ve successfully queried the CSV files using `read_files()`, let’s use a CTAS (`CREATE TABLE AS SELECT`) statement with the same query to complete the following:\n",
    "    - Create a Delta table named **sales_bronze**. \n",
    "    - Add an ingestion timestamp and ingestion metadata columns to our **sales_bronze** table.\n",
    "\n",
    "        - **Ingestion Timestamp:** To record when the data was ingested, use the [`current_timestamp()`](https://docs.databricks.com/aws/en/sql/language-manual/functions/current_timestamp) function. It returns the current timestamp at the start of query execution and is useful for tracking ingestion time.\n",
    "\n",
    "        - **Metadata Columns:** To include file metadata, use the [`_metadata`](https://docs.databricks.com/en/ingestion/file-metadata-column.html) column, which is available for all input file formats. This hidden column allows access to various metadata attributes from the input files.\n",
    "            - Use `_metadata.file_modification_time` to capture the last modification time of the input file.\n",
    "            - Use `_metadata.file_name` to capture the name of the input file.\n",
    "            - [File metadata column](https://docs.databricks.com/gcp/en/ingestion/file-metadata-column)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Run the cell and review the results. You should see that the **sales_bronze** table was created successfully with the CSV data and additional metadata columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfef1f8a-6026-4e9d-858a-a0ebdd2f065b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a table with read_files from CSV files"
    }
   },
   "outputs": [],
   "source": [
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS sales_bronze;\n",
    "\n",
    "\n",
    "-- Create the Delta table\n",
    "CREATE TABLE sales_bronze AS\n",
    "SELECT \n",
    "  *,\n",
    "  _metadata.file_modification_time AS file_modification_time,\n",
    "  _metadata.file_name AS source_file, \n",
    "  current_timestamp() as ingestion_time \n",
    "FROM read_files(\n",
    "        \"/Volumes/dbacademy_ecommerce/v01/raw/sales-csv\",\n",
    "        format => \"csv\",\n",
    "        sep => \"|\",\n",
    "        header => true\n",
    "      );\n",
    "\n",
    "\n",
    "-- Display the table\n",
    "SELECT *\n",
    "FROM sales_bronze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4693e95-ff40-4f31-aff7-42ab211946f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. View the column data types of the **sales_bronze** table. Notice that the `read_files()` function automatically infers the schema if one is not explicitly provided.\n",
    "\n",
    "      **NOTE:** When the schema is not provided, `read_files()` attempts to infer a unified schema across the discovered files, which requires reading all the files unless a LIMIT statement is used. Even when using a LIMIT query, a larger set of files than required might be read to return a more representative schema of the data.\n",
    "\n",
    "     - [Schema inference](https://docs.databricks.com/aws/en/sql/language-manual/functions/read_files#csv-options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f750de9-4d16-475f-88c3-c2ef382410e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View the inferred schema"
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE TABLE EXTENDED sales_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dec2e307-5944-4c0f-9c00-acef95baafad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### B3. (BONUS) Python Equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecf35cd5-5d61-44f1-8a70-1f913dc26797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "df = (spark\n",
    "      .read \n",
    "      .option(\"header\", True) \n",
    "      .option(\"sep\",\"|\") \n",
    "      .option(\"rescuedDataColumn\", \"_rescued_data\")       # <--------- Add the rescued data column\n",
    "      .csv(\"/Volumes/dbacademy_ecommerce/v01/raw/sales-csv\")\n",
    "    )\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e22c5157-3e6b-4994-986b-eaba8de7ef94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Troubleshooting Common CSV Issues\n",
    "\n",
    "\n",
    "1. To begin, let’s quickly explore your data source raw files volume. Complete the following steps to view your course volume in **dbacademy.ops.labuser**:\n",
    "\n",
    "   a. In the left navigation bar, select the catalog icon ![Catalog Icon](./Includes/images/catalog_icon.png).\n",
    "\n",
    "   b. Expand the **dbacademy** catalog.\n",
    "\n",
    "   c. Expand the **ops** schema.\n",
    "\n",
    "   d. Expand **Volumes**. You should see a volume with your **labuser** name, which contains the source data to ingest.\n",
    "\n",
    "   e. Expand your **labuser** volume. Notice that this volume contains a series of subdirectories. We will be using the **csv_demo_files** directory in your volume.\n",
    "\n",
    "   f. Expand the **csv_demo_files** subdirectory. Notice that it contains the files:\n",
    "      - **malformed_example_1_data.csv**\n",
    "      - **malformed_example_2_data.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7299d3f1-f2be-4126-8800-80c2e44c5795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. In the cell below, view the value of the SQL variable `DA.paths_working_dir`. This variable will reference the path to your **labuser** volume from above, as each user has a different source volume. This variable is created within the classroom setup script to dynamically reference your unique volume.\n",
    "\n",
    "   Run the cell and review the results. You’ll notice that the `DA.paths_working_dir` variable points to your `/Volumes/dbacademy/ops/labuser` volume.\n",
    "\n",
    "**NOTE:** Instead of using the `DA.paths_working_dir` variable, you could also specify the path name directly by right clicking on your volume and selecting **Copy volume path**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25769dc-367b-4034-8d69-192536b9c8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "values(DA.paths_working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82f91c19-146c-4341-8f43-d20ef0794bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. You can concatenate the `DA.paths_working_dir` SQL variable with a string to specify a specific subdirectory in your specific volume.\n",
    "\n",
    "   Run the cell below and review the results. You’ll notice that it returns the path to your **malformed_example_1_data.csv** file. This method will be used when referencing your volume within the `read_files` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28fe788-4dfa-4e58-b448-2c98b45407cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "values(DA.paths_working_dir || '/csv_demo_files/malformed_example_1_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f885ffaa-3b55-420e-9e4b-72456c89f9e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Defining a Schema During Ingestion\n",
    "\n",
    "We want to read the CSV file into the bronze table using a defined schema.\n",
    "\n",
    "**Explicit schemas benefits:**\n",
    "- Reduce the risk of inferred schema inconsistencies, especially with semi-structured data like JSON or CSV.\n",
    "- Enable faster parsing and loading of data, as Spark can immediately apply the correct types and structure without inferring the schema.\n",
    "- Improve performance with large datasets by significantly reducing compute overhead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e198d4a-0680-45b4-b1f8-b9086c9a0aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. The query below will reference the **malformed_example_1_data.csv** file. This will allow you to view the CSV file as text for inspection.\n",
    "\n",
    "   Run the query and review the results. Notice the following:\n",
    "\n",
    "   - The CSV file is **|** delimited.\n",
    "\n",
    "   - The CSV file contains headers.\n",
    "   \n",
    "**NOTE:** The **transactions_timestamp** column contains the string *aaa* in the first row, which will cause issues during ingestion when attempting read the **transactions_timestamp** column as a BIGINT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b34f621-58c4-4f6c-9b52-ac23d669af9f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View the CSV file as text"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark.sql(f'''\n",
    "    SELECT *\n",
    "    FROM text.`{DA.paths.working_dir}/csv_demo_files/malformed_example_1_data.csv`\n",
    "''').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f6a9448-6e66-4407-b6b9-3dc336c5022d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Use the `read_files` function to see how this CSV file is read into the table. Run the cell and view the results. \n",
    "\n",
    "    **IMPORTANT** Notice that the malformed value *aaa* in the **transactions_timestamp** column causes the column to be read as a STRING. However, we want the **transactions_timestamp** column to be read into the bronze table as a BIGINT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7440bec5-ae1a-46b2-a54d-86676299c364",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use read_files without a schema"
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM read_files(\n",
    "        DA.paths_working_dir || '/csv_demo_files/malformed_example_1_data.csv',\n",
    "        format => \"csv\",\n",
    "        sep => \"|\",\n",
    "        header => true\n",
    "      );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b9b525-66ee-4a5c-88cf-7eabe1b2a7d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. You can define a schema for the `read_files()` function to read in the data with a specific structure.\n",
    "\n",
    "   a. Use the `schema` option to define the schema. In this case, we'll read in the following:\n",
    "   - **order_id** as INT  \n",
    "   - **email** as STRING  \n",
    "   - **transactions_timestamp** as BIGINT\n",
    "\n",
    "   b. Use the `rescuedDataColumn` option to collect all data that can’t be parsed due to data type mismatches or schema mismatches into a separate column for review.\n",
    "\n",
    "   Run the cell and review the results. Notice that row 1 (*aaa*) could not be read using the defined schema, so it was placed in the **_rescued_data** column. Keeping rows that don’t conform to the schema allows you to inspect and process them as needed.\n",
    "\n",
    "**NOTE:** Defining a schema when using `read_files` in Databricks improves performance by skipping the expensive schema inference step and ensures consistent, reliable data parsing. It's especially beneficial for large or semi-structured datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba23fd5-1d9c-4e60-9610-0d559c3754d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define a schema with read_files"
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM read_files(\n",
    "        DA.paths_working_dir || '/csv_demo_files/malformed_example_1_data.csv',\n",
    "        format => \"csv\",\n",
    "        sep => \"|\",\n",
    "        header => true,\n",
    "        schema => '''\n",
    "            order_id INT, \n",
    "            email STRING, \n",
    "            transactions_timestamp BIGINT''', \n",
    "        rescueddatacolumn => '_rescued_data'    -- Create the _rescued_data column\n",
    "      );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34c9057e-47cd-40ad-9fad-61196874ccd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Summary: Rescued Data Column\n",
    "\n",
    "The rescued data column ensures that rows that don’t match with the schema are rescued instead of being dropped. The rescued data column contains any data that isn’t parsed for the following reasons:\n",
    "- The column is missing from the schema.\n",
    "- Type mismatches\n",
    "- Case mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e031f027-3ab8-445e-8e3f-9c4d2e4249a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C2. Handling Missing Headers During Ingestion \n",
    "In this example, the CSV file contains a missing header by mistake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e81e252-1e67-4ebc-9e42-7e77b7613a76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to view the **malformed_example_2_data.csv** file. Notice that the first row contains headers, but the first column name is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8349dc37-c827-4e45-8376-d6385a24de8c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View the CSV file as text"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "spark.sql(f'''\n",
    "  SELECT *\n",
    "  FROM text.`{DA.paths.working_dir}/csv_demo_files/malformed_example_2_data.csv`\n",
    "''').display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fecdf41e-50c6-4af3-a8f9-5251d72e3dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Let's try to create a table using the **malformed_example_2_data.csv** file with the `read_files()` function. Run the cell and review the results.\n",
    "\n",
    "    Notice the following:\n",
    "    - The first column of the CSV file was not read into the table as a standard column and was instead placed in the **_rescued_data** column.\n",
    "\n",
    "    - The **_rescued_data** column stores the rescued data as a JSON-formatted string.\n",
    "    \n",
    "    - When inspecting the **_rescued_data** JSON-formatted string, you'll see that the unnamed column from the CSV file is represented with a key of **_c0**, which contains the column value as a string, along with a **_file_path** key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bf8c313-158e-4b7e-bca3-c8f6faba92c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Use read_files to read a malformed CSV file"
    }
   },
   "outputs": [],
   "source": [
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS demo_4_example_2_bronze;\n",
    "\n",
    "-- Create Delta table by ingesting CSV file\n",
    "CREATE OR REPLACE TABLE demo_4_example_2_bronze AS\n",
    "SELECT *\n",
    "FROM read_files(\n",
    "        DA.paths_working_dir || '/csv_demo_files/malformed_example_2_data.csv',\n",
    "        format => \"csv\",\n",
    "        sep => \"|\",\n",
    "        header => true\n",
    "      );\n",
    "\n",
    "\n",
    "-- Display the table\n",
    "SELECT *\n",
    "FROM demo_4_example_2_bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3017386b-e29d-4f0f-b3a2-1896900793c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. The **_rescued_data** column is a JSON-formatted string. We won’t go into detail on how to handle this type of data here, as it will be covered in a later demo and lab.\n",
    "\n",
    "    However, it's important to note that you can extract values from the **_rescued_data** column and add them to your bronze table. To obtain the value from the **_c0** field, you can use the `_rescued_data:_c0` syntax, as shown in the next cell.\n",
    "\n",
    "    **NOTE:** The output from running the next cell returns **order_id** as the rescued column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "484678a9-4ce3-4424-a1cb-8cf6de98458d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix the rescued data column"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  cast(_rescued_data:_c0 AS BIGINT) AS order_id,\n",
    "  *\n",
    "FROM read_files(\n",
    "        DA.paths_working_dir || '/csv_demo_files/malformed_example_2_data.csv',\n",
    "        format => \"csv\",\n",
    "        sep => \"|\",\n",
    "        header => true\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbd442c-79b1-4666-bb7d-0b9acb2f9fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4400282376822369,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04 - Handling CSV Ingestion with the Rescued Data Column",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
