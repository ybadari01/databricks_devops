{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aa2e290-1394-4cf2-baee-9bc0adf69237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e32dcf47-7356-430c-815c-ccc6c5b80fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3 - Adding Metadata Columns During Ingestion\n",
    "\n",
    "In this demonstration, we'll explore how to add metadata columns during data ingestion. \n",
    "\n",
    "This process will include adding metadata, converting Unix timestamps to standard `DATE` format, and row  ingestion times.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "\n",
    "- Modify columns during data ingestion from cloud storage to your bronze table.\n",
    "- Add the current ingestion timestamp to the bronze.\n",
    "- Use the `_metadata` column to extract file-level metadata (e.g., file name, modification time) during ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5e48d0-3f2a-4420-8331-95f53ac73237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default and you have a Shared SQL warehouse.\n",
    "\n",
    "<!-- ![Select Cluster](./Includes/images/selecting_cluster_info.png) -->\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "\n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16c025a4-4a4b-438e-8c14-f5e1f480fd84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this notebook.\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course in the lab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7878a2ef-d4d2-4b9e-b7bb-b951ed8e8037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "891809e3-55e7-4a22-a5e9-2fa4c185962d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the cell below to view your default catalog and schema. Notice that your default catalog is **dbacademy** and your default schema is your unique **labuser** schema.\n",
    "\n",
    "**NOTE:** The default catalog and schema are pre-configured for you to avoid the need to specify the three-level name when writing your tables to your **labuser** schema (i.e., catalog.schema.table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdf533f-a66a-4604-8fde-d2517a83ccf1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View current catalog and schema"
    }
   },
   "outputs": [],
   "source": [
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eca8e4cf-4bb3-454d-9740-3b341b823564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Explore the Data Source Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b04c19-7edc-4754-ace0-84d3fdfe6837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. We'll create a table containing historical user data from Parquet files stored in the volume  \n",
    "   `'/Volumes/dbacademy_ecommerce/v01/raw/users-historical'` within Unity Catalog.\n",
    "\n",
    "   Use the `LIST` statement to view the files in this volume. Run the cell and review the results.\n",
    "\n",
    "   View the values in the **name** column that begin with **part-**. This shows that this volume contains multiple **Parquet** files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b59de9b-a9bf-4ef0-bc9f-bea68f16a736",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List files in a raw/users-historical volume"
    }
   },
   "outputs": [],
   "source": [
    "LIST '/Volumes/dbacademy_ecommerce/v01/raw/users-historical'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2b117a4-4389-4a42-89a3-c072bf36e62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Adding Metadata Columns to the Bronze Table During Ingestion\n",
    "\n",
    "When ingesting data into the Bronze layer, you can apply transformations during ingestion and also retrieve metadata about the input files using the **_metadata** column.\n",
    "\n",
    "The **_metadata** column is a hidden column available for all supported file formats. To include it in the returned data, you must explicitly select it in the read query that specifies the source.\n",
    "\n",
    "\n",
    "### Ingestion Requirements\n",
    "\n",
    "During data ingestion, we'll perform the following actions:\n",
    "\n",
    "1. Convert the parquet Unix timestamp to a `DATE` column.\n",
    "\n",
    "2. Include the **input file name** to indicate the data raw source.\n",
    "\n",
    "3. Include the **last modification** timestamp of the input file.\n",
    "\n",
    "4. Add the **file ingestion time** to the Bronze table.\n",
    "\n",
    "**Note:** The `_metadata` column is available across all supported input file formats.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ae12118-8a04-49ac-9b2a-37437e2199d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to display the parquet data in the `\"/Volumes/dbacademy_ecommerce/v01/raw/users-historical\"` volume and view the results.\n",
    "\n",
    "    Notice that the **user_first_touch_timestamp** column has a Unix timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9fcd15-ac93-494e-9f46-810cc7d146ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM read_files(\n",
    "  \"/Volumes/dbacademy_ecommerce/v01/raw/users-historical\",\n",
    "  format => 'parquet')\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d0b40f6-9453-412c-b192-5f467d3ecc99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### C1. Convert the Unix Time on Ingestion to Bronze\n",
    "\n",
    "The Unix timestamp column **user_first_touch_timestamp** values represent the time in microseconds since the Unix epoch (January 1, 1970).\n",
    "\n",
    "To create a readable date column, use the [`from_unixtime()`](https://docs.databricks.com/en/sql/language-manual/functions/from_unixtime.html) function, converting the **user_first_touch_timestamp** from microseconds to seconds by dividing by 1,000,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c401633-6710-408e-b744-3c5b57152855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the query and review the results. The query generates a new column, **first_touch_date**, by converting the Unix timestamp into a human-readable date column.\n",
    "\n",
    "   Run the cell and view the **first_touch_date** column. Notice the **first_touch_date** column is cast to a data type of **DATE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aae9247-abff-471c-aa9b-59000475f871",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert UNIX timestamp"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  *,\n",
    "  cast(from_unixtime(user_first_touch_timestamp/1000000) AS DATE) AS first_touch_date\n",
    "FROM read_files(\n",
    "  \"/Volumes/dbacademy_ecommerce/v01/raw/users-historical\",\n",
    "  format => 'parquet')\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c4087f6-4026-4a14-9dbd-d4dded8bda41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### C2. Adding Column Metadata on Ingestion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The following metadata can be added to the bronze table:\n",
    "\n",
    "- `_metadata.file_modification_time`: Adds the last modification time of the input file.\n",
    "\n",
    "- `_metadata.file_name`: Adds the input file name.\n",
    "\n",
    "- [`current_timestamp()`](https://docs.databricks.com/aws/en/sql/language-manual/functions/current_timestamp): Returns the current timestamp (`TIMESTAMP` data type) when the query starts, useful for tracking ingestion time.\n",
    "\n",
    "You can read more about the `_metadata` column in the [Databricks documentation](https://docs.databricks.com/en/ingestion/file-metadata-column.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e44be30-450a-426f-a06e-2111ded623c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the query below to add the following columns:\n",
    "\n",
    "   - **file_modification_time** and **file_name**, using the **_metadata** column to capture input file details.  \n",
    "   \n",
    "   - **ingestion_time**, which records the exact time the data was ingested.\n",
    "\n",
    "   Review the results. You should see the new columns **file_modification_time**, **source_file**, and **ingestion_time** added to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d409cb-ebf3-4685-aadf-03609d77f86e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add metadata columns"
    }
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "  *,\n",
    "  cast(from_unixtime(user_first_touch_timestamp / 1000000) AS DATE) AS first_touch_date,\n",
    "  _metadata.file_modification_time AS file_modification_time,      -- Last data source file modification time\n",
    "  _metadata.file_name AS source_file,                              -- Ingest data source file name\n",
    "  current_timestamp() as ingestion_time                            -- Ingestion timestamp\n",
    "FROM read_files(\n",
    "  \"/Volumes/dbacademy_ecommerce/v01/raw/users-historical\",\n",
    "  format => 'parquet')\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2174dc54-3394-4c70-b63e-f23bc68b35e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C3. Creating the Final Bronze Table\n",
    "1. Put it all together with the `CTAS` statement to create the Delta table.\n",
    "\n",
    "    Run the cell to create and view the new table **historical_users_bronze**.\n",
    "    \n",
    "    Confirm that the new columns **first_touch_date**, **file_modification_time**, **source_file** and **ingestion_time** were created successfully in the bronze table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7101c484-e84e-48a0-bfd3-db9ae4cf5e10",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the final bronze table"
    }
   },
   "outputs": [],
   "source": [
    "-- Drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS historical_users_bronze;\n",
    "\n",
    "\n",
    "-- Create an empty table\n",
    "CREATE TABLE historical_users_bronze AS\n",
    "SELECT\n",
    "  *,\n",
    "  cast(from_unixtime(user_first_touch_timestamp / 1000000) AS DATE) AS first_touch_date,\n",
    "  _metadata.file_modification_time AS file_modification_time,      -- Last data source file modification time\n",
    "  _metadata.file_name AS source_file,                              -- Ingest data source file name\n",
    "  current_timestamp() as ingestion_time                            -- Ingestion timestamp\n",
    "FROM read_files(\n",
    "  \"/Volumes/dbacademy_ecommerce/v01/raw/users-historical\",\n",
    "  format => 'parquet');\n",
    "\n",
    "\n",
    "-- View the final bronze table\n",
    "SELECT * \n",
    "FROM historical_users_bronze\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d67ddd9b-b4de-48d0-b438-ec29235453d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C4. Exploring the Final Bronze Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b235a310-6229-4849-9b8c-f7364555d95c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. With the additional metadata columns added to the bronze table, you can explore metadata information from the input files. For example, you can execute a query to see how many rows came from each Parquet file by querying the **source_file** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8258edae-b5a5-4157-b7ec-4fa061d87731",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Count rows by parquet file"
    }
   },
   "outputs": [],
   "source": [
    "SELECT \n",
    "  source_file, \n",
    "  count(*) as total\n",
    "FROM historical_users_bronze\n",
    "GROUP BY source_file\n",
    "ORDER BY source_file;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c1e9902-a211-4fbe-8074-f663896efa9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. (BONUS) Python Equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57be2e4c-6f8a-48a9-8214-2bc1f95e1f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "from pyspark.sql.functions import col, from_unixtime, current_timestamp\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# 1. Read parquet files in cloud storage into a Spark DataFrame\n",
    "df = (spark\n",
    "      .read\n",
    "      .format(\"parquet\")\n",
    "      .load(\"/Volumes/dbacademy_ecommerce/v01/raw/users-historical\")\n",
    "    )\n",
    "\n",
    "\n",
    "# 2. Add metadata columns\n",
    "df_with_metadata = (\n",
    "    df.withColumn(\"first_touch_date\", from_unixtime(col(\"user_first_touch_timestamp\") / 1_000_000).cast(DateType()))\n",
    "      .withColumn(\"file_modification_time\", col(\"_metadata.file_modification_time\"))\n",
    "      .withColumn(\"source_file\", col(\"_metadata.file_name\"))\n",
    "      .withColumn(\"ingestion_time\", current_timestamp())\n",
    ")\n",
    "\n",
    "\n",
    "# 3. Save as a Delta table\n",
    "(df_with_metadata\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(f\"dbacademy.{DA.schema_name}.historical_users_bronze_python_metadata\")\n",
    ")\n",
    "\n",
    "\n",
    "# 4. Read and display the table\n",
    "historical_users_bronze_python_metadata = spark.table(f\"dbacademy.{DA.schema_name}.historical_users_bronze_python_metadata\")\n",
    "\n",
    "display(historical_users_bronze_python_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bede8a72-5a2c-4c67-8ea1-0d7084da4ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2026 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1513883163438400,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - Adding Metadata Columns During Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
